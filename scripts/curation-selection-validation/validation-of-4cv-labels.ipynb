{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef885fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:05.695513Z",
     "start_time": "2021-12-17T09:18:03.125501Z"
    }
   },
   "source": [
    "# Curation, selection and validation of 4CV frame labels\n",
    "\n",
    "**Author(s):** Miguel Xochicale [@mxochicale](https://github.com/mxochicale)  \n",
    "**Contributor(s):** Nhat Phug [@huynhatd13](https://github.com/huynhatd13) \n",
    "\n",
    "March2022; April2022; May 2022\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This notebook presents prototypes to pre-process echocardiography datasets with the use of pytorch features. \n",
    "\n",
    "### Running notebook\n",
    "\n",
    "1. Go to echocardiography repository path: `$HOME/repositories/echocardiography/`\n",
    "2. Open echocardiography repo in pycharm and in the terminal type:\n",
    "    ```\n",
    "    git checkout master # or the branch\n",
    "    git pull # to bring a local branch up-to-date with its remote version\n",
    "    ```\n",
    "3. Launch Notebook server\n",
    "    Go to you repository path: `cd $HOME/repositories/echocardiography/scripts/dataloaders` and type in the pycharm terminal:\n",
    "    ```\n",
    "    conda activate rt-ai-echo-VE \n",
    "    jupyter notebook\n",
    "    ```\n",
    "    which will open your web-browser.\n",
    "    \n",
    "    \n",
    "### References\n",
    "* \"Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback\". https://www.fda.gov/media/122535/download \n",
    "* Gomez A. et al. 2021 https://github.com/vital-ultrasound/lung/blob/main/multiclass_pytorch/datasets/LUSVideoDataset.py \n",
    "* Save animation as gif (if required) or other formats https://holypython.com/how-to-save-matplotlib-animations-the-ultimate-guide/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2aa0",
   "metadata": {},
   "source": [
    "## Jupyter Notebook\n",
    "### Setting imports and datasets paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345efe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T06:51:21.749066Z",
     "start_time": "2022-05-10T06:51:21.709194Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, display #to be used with HTML(animation.ArtistAnimation().to_jshtml())\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from source.dataloaders.EchocardiographicVideoDataset import EchoClassesDataset\n",
    "from source.models.learning_misc import train_loop, test_loop\n",
    "from source.helpers.various import concatenating_YAML_via_tags, plot_dataset_classes, split_train_validate_sets\n",
    "\n",
    "\n",
    "HOME_PATH = os.path.expanduser(f'~')\n",
    "USERNAME = os.path.split(HOME_PATH)[1]\n",
    "CONFIG_FILES_PATH= 'repositories/echocardiography/scripts/config_files/users_paths_files'\n",
    "YML_FILE =  'config_users_paths_files_username_' + USERNAME + '_validation'+ '.yml'\n",
    "FULL_PATH_FOR_YML_FILE = os.path.join(HOME_PATH, CONFIG_FILES_PATH, YML_FILE)\n",
    "PATH_for_temporal_files = os.path.join(HOME_PATH, 'datasets/vital-us/echocardiography/temporal-files')\n",
    "\n",
    "yaml.add_constructor('!join', concatenating_YAML_via_tags)  ## register the tag handler\n",
    "with open(FULL_PATH_FOR_YML_FILE, 'r') as yml:\n",
    "    config = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:08.264310Z",
     "start_time": "2021-12-17T09:18:08.250178Z"
    }
   },
   "source": [
    "### Splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a8fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T06:51:22.955869Z",
     "start_time": "2022-05-10T06:51:22.948344Z"
    }
   },
   "outputs": [],
   "source": [
    "split_train_validate_sets(  \n",
    "                        config['echodataset_path'], \n",
    "                        config['data_list_output_path'], \n",
    "                        config['ntraining'],\n",
    "                        config['randomise_file_list']\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3865c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T09:48:38.180954Z",
     "start_time": "2022-03-31T09:48:38.175487Z"
    }
   },
   "source": [
    "# Verification of participants\n",
    "### Setting up variables for participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd7228",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T06:51:24.219661Z",
     "start_time": "2022-05-10T06:51:24.217566Z"
    }
   },
   "outputs": [],
   "source": [
    "SUBJECT_ID = '048'\n",
    "NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 200\n",
    "PRETRANSFORM_IM_SIZE = [200, 200] #[650, 690] original pixel size for VenueGO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e7c31",
   "metadata": {},
   "source": [
    "### Other relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292478a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T06:51:25.464053Z",
     "start_time": "2022-05-10T06:51:25.458234Z"
    }
   },
   "outputs": [],
   "source": [
    "interval_between_frames_in_milliseconds=33.3 ## 1/30=0.033333\n",
    "frame_per_seconds_for_animated_frames=30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92177c6e",
   "metadata": {},
   "source": [
    "###  Setting up txt list for participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e660103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T06:51:26.693969Z",
     "start_time": "2022-05-10T06:51:26.685042Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #or \"cuda:0\"\n",
    "\n",
    "videolist = config['participant_videos_list_full']\n",
    "annotationlist = config['participant_path_json_list_full']\n",
    "\n",
    "video_filenames = [line.strip() for line in open(videolist)]\n",
    "annotation_filenames = [line.strip() for line in open(annotationlist)]\n",
    "\n",
    "echodataset_path= config['echodataset_path']\n",
    "data_list_output_path = config['data_list_output_path']\n",
    "#print(echodataset_path)\n",
    "#print(data_list_output_path)\n",
    "\n",
    "all_videos_files_to_be_verified = 'video_list_tbv.txt'\n",
    "all_annotation_files_to_be_verified = 'annotation_list_tbv.txt'\n",
    "videolist_tbv_txt = '{}{}'.format(data_list_output_path, all_videos_files_to_be_verified)\n",
    "annotationlist_tbv_txt = '{}{}'.format(data_list_output_path, all_annotation_files_to_be_verified)\n",
    "\n",
    "JSON_FILE_PATTERN = '*'+SUBJECT_ID+'*4[cC][vV].[jJ][sS][oO][nN]'\n",
    "MP4_FILE_PATTERN = '*'+SUBJECT_ID+'*echo*.[mM][pP][4]'\n",
    "\n",
    "result = list(sorted(Path(echodataset_path).rglob(MP4_FILE_PATTERN)))\n",
    "with open(videolist_tbv_txt, 'w') as file_list_txt:\n",
    "    for mp4_filename_i in result:\n",
    "        file_n_nopath = str(mp4_filename_i).replace(echodataset_path, '')\n",
    "        file_list_txt.write(file_n_nopath + '\\n')\n",
    "        \n",
    "\n",
    "result = list(sorted(Path(echodataset_path).rglob(JSON_FILE_PATTERN)))\n",
    "with open(annotationlist_tbv_txt, 'w') as file_list_txt:\n",
    "    for mp4_filename_i in result:\n",
    "        file_n_nopath = str(mp4_filename_i).replace(echodataset_path, '')\n",
    "        file_list_txt.write(file_n_nopath + '\\n')        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e607bf7",
   "metadata": {},
   "source": [
    "### Setting variables and loading datasets using pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T06:52:30.336829Z",
     "start_time": "2022-05-10T06:52:30.325896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining transforms that apply to the entire dataset.\n",
    "# These transforms are not augmentation.\n",
    "if config['use_pretransform_image_size']:\n",
    "    pretransform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=PRETRANSFORM_IM_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    pretransform = None\n",
    "\n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_train_augmentation']:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),  # in degrees\n",
    "        transforms.RandomEqualize(p=0.5),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "    \n",
    "\n",
    "echo_dataset = EchoClassesDataset(\n",
    "    echodataset_path=config['echodataset_path'],\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_tbv'],\n",
    "    participant_path_json_list=config['participant_path_json_list_tbv'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP,\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,\n",
    "    use_tmp_storage=config['use_tmp_storage'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760906",
   "metadata": {},
   "source": [
    "### Plotting Class Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6472a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:03:19.892235Z",
     "start_time": "2022-05-04T11:03:16.129378Z"
    }
   },
   "outputs": [],
   "source": [
    "label_id = ('BKGR', '4CV')\n",
    "number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip'] \n",
    "\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_class_dict = {\n",
    "   'BKGR': 0 ,\n",
    "   \"4CV\": 0\n",
    "    }\n",
    "    \n",
    "    for clip_index_i in range(len(dataset_obj)):\n",
    "        data_idx = dataset_obj[clip_index_i]\n",
    "        label_id_idx = data_idx[1]\n",
    "        label = label_id[label_id_idx]\n",
    "        count_class_dict[label]+= 1\n",
    "        #count_class_dict[label]+= 1* number_of_frames_per_segment_in_a_clip\n",
    "\n",
    "    return count_class_dict\n",
    "        \n",
    "        \n",
    "def plot_from_dict(dict_obj, plot_title, **kwargs):\n",
    "    return sns.barplot(data = pd.DataFrame.from_dict([dict_obj]).melt(), \n",
    "                       x = \"variable\", y=\"value\", hue=\"variable\", **kwargs).set_title(plot_title)\n",
    "\n",
    "print(get_class_distribution(echo_dataset))\n",
    "\n",
    "plot_title_train_label= f'TRAIN dataset of {len(echo_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(18,7))\n",
    "plot_from_dict(get_class_distribution(echo_dataset), plot_title=plot_title_train_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ea2a6",
   "metadata": {},
   "source": [
    "### Animating frames of one clip of the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73552935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:03:20.016413Z",
     "start_time": "2022-05-04T11:03:19.893419Z"
    }
   },
   "outputs": [],
   "source": [
    "#average_HR =\n",
    "#fps = 30\n",
    "# 60 # beats per minute \n",
    "#Beats-per-minute: 60 BPM\n",
    "#Beats-per-second: 1 Hz\n",
    "#Cycle-per-second: 1 (Cycle/s)\n",
    "\n",
    "number_of_clips = len(echo_dataset)\n",
    "\n",
    "\n",
    "print(f'---------------------------------------')\n",
    "### Setting clips\n",
    "clips=[]\n",
    "for clip_index in range( int(number_of_clips)  ):\n",
    "    data_idx = echo_dataset[clip_index]\n",
    "    data_clip_idx = data_idx[0]\n",
    "    label_clip_idx = data_idx[1]\n",
    "    clip_frame_clip_idx = data_idx[2]\n",
    "    n_available_frames_clip_idx = data_idx[3]\n",
    "    print(f' CLIP:{clip_index:02d} of {label_id[label_clip_idx]} label for {data_clip_idx.size()} TOTAL_FRAMES: {n_available_frames_clip_idx} from clip_frame_clip_idx {clip_frame_clip_idx}')    \n",
    "    clips.append([data_clip_idx, label_clip_idx, clip_index, clip_frame_clip_idx, n_available_frames_clip_idx ]) \n",
    "\n",
    "\n",
    "# ### Setting and displaying pair of clips    \n",
    "def pair_clips_labels(clips):\n",
    "    pair_clips_labels_=[]    \n",
    "    number_of_clips=len(clips)\n",
    "    for clip_index_i_A in range( int(number_of_clips/2)  ):\n",
    "        clip_index_i_B=int(number_of_clips/2) + clip_index_i_A \n",
    "        print(f' pair_clips_labels[{clip_index_i_A}]-- BKRG:{clip_index_i_A}, 4CV:{clip_index_i_B}')\n",
    "        data_clip_i_A=clips[clip_index_i_A][0]\n",
    "        label_i_A=clips[clip_index_i_A][1]\n",
    "        clip_i_A=clips[clip_index_i_A][2]\n",
    "        number_of_frames_A=clips[clip_index_i_A][4]\n",
    "        data_clip_i_B=clips[clip_index_i_B][0]\n",
    "        label_i_B=clips[clip_index_i_B][1]\n",
    "        clip_i_B=clips[clip_index_i_B][2]\n",
    "        number_of_frames_B=clips[clip_index_i_B][4]\n",
    "        pair_clips_labels_.append([data_clip_i_A, label_i_A, clip_i_A, number_of_frames_A, data_clip_i_B, label_i_B, clip_i_B, number_of_frames_B])\n",
    "    \n",
    "    return(pair_clips_labels_)\n",
    "\n",
    "\n",
    "print(f'---------------------------------------')\n",
    "pair_clips_labels = pair_clips_labels(clips)\n",
    "\n",
    "\n",
    "\n",
    "### Defining animate clips\n",
    "def animate_clips(pair_clips_labels):\n",
    "    #print(f' CLIP: for {label_id[pair_clips_labels[1]]} ')\n",
    "    fig = plt.figure()    \n",
    "    pair_of_clip_index_i_frames=[]   \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "    data_clip_tensor_A=pair_clips_labels[0]\n",
    "    label_clip_A=pair_clips_labels[1]\n",
    "    clip_i_A=pair_clips_labels[2]\n",
    "    number_of_frames_A=pair_clips_labels[3]\n",
    "    data_clip_tensor_B=pair_clips_labels[4]\n",
    "    label_clip_B=pair_clips_labels[5]\n",
    "    clip_i_B=pair_clips_labels[6]\n",
    "    number_of_frames_B=pair_clips_labels[7]\n",
    "    \n",
    "    \n",
    "    ax1.title.set_text(f' CLIP: {clip_i_A:02d}--{label_id[label_clip_A]} with {number_of_frames_A} of {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} frames [for Subject {SUBJECT_ID}]')\n",
    "    ax2.title.set_text(f' CLIP: {clip_i_B:02d}--{label_id[label_clip_B]} with {number_of_frames_B} of {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} frames [for Subject {SUBJECT_ID}]')\n",
    "    for frames_idx in range(data_clip_tensor_A[:,:,...].size()[1]):\n",
    "        imA = ax1.imshow(data_clip_tensor_A[:,frames_idx,...].cpu().detach().numpy().transpose(1,2,0) , cmap=plt.get_cmap('gray') )  \n",
    "        imB = ax2.imshow(data_clip_tensor_B[:,frames_idx,...].cpu().detach().numpy().transpose(1,2,0) , cmap=plt.get_cmap('gray') )  \n",
    "        pair_of_clip_index_i_frames.append( [imA, imB] )\n",
    "    fig.tight_layout()    \n",
    "    #return fig, pair_of_clip_index_i_frames\n",
    "\n",
    "    anim = animation.ArtistAnimation(fig, pair_of_clip_index_i_frames, interval=interval_between_frames_in_milliseconds, blit=True, repeat_delay=1000)\n",
    "    return anim\n",
    "\n",
    "# plt.show()    \n",
    "###BLURS\n",
    "#\n",
    "#fig, ax = plt.subplots()\n",
    "#clips_axes.append( clip_index_i_frames )\n",
    "# #plt.axes()            \n",
    "\n",
    "# #plt.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d225",
   "metadata": {},
   "source": [
    "### Displayting animated frames in the dataloader\n",
    "**NOTE:** you might need to uncommed the lines where there is no clips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57688b",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[0]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6c3f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:03:50.848631Z",
     "start_time": "2022-05-04T11:03:20.017624Z"
    }
   },
   "outputs": [],
   "source": [
    "PAIR_OF_CLIPS = pair_clips_labels[0]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "##SAVE ANIMATIONS\n",
    "GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e21006",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[1]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bcb91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:04:21.266904Z",
     "start_time": "2022-05-04T11:03:50.850209Z"
    }
   },
   "outputs": [],
   "source": [
    "PAIR_OF_CLIPS = pair_clips_labels[1]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "##SAVE ANIMATIONS\n",
    "GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce17dd4",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[2]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d95b24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:04:51.686494Z",
     "start_time": "2022-05-04T11:04:21.268099Z"
    }
   },
   "outputs": [],
   "source": [
    "PAIR_OF_CLIPS = pair_clips_labels[2]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "##SAVE ANIMATIONS\n",
    "GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c6b51",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[3]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eeb96e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:05:21.683785Z",
     "start_time": "2022-05-04T11:04:51.687696Z"
    }
   },
   "outputs": [],
   "source": [
    "PAIR_OF_CLIPS = pair_clips_labels[3]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "##SAVE ANIMATIONS\n",
    "GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1129f",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[4]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a82233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:05:51.579257Z",
     "start_time": "2022-05-04T11:05:21.684654Z"
    }
   },
   "outputs": [],
   "source": [
    "PAIR_OF_CLIPS = pair_clips_labels[4]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "##SAVE ANIMATIONS\n",
    "GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73496d56",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[5]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d66f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:05:51.582852Z",
     "start_time": "2022-05-04T11:05:51.580497Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[5]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "# ##SAVE ANIMATIONS\n",
    "# GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "# GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "# animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0199ee",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[6]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc1975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:05:51.588479Z",
     "start_time": "2022-05-04T11:05:51.583735Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[6]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "# ##SAVE ANIMATIONS\n",
    "# GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "# GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "# animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82637f3e",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[7]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a717aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:05:51.591186Z",
     "start_time": "2022-05-04T11:05:51.589521Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[7]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "# ##SAVE ANIMATIONS\n",
    "# GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "# GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "# animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1919d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T08:28:28.490581Z",
     "start_time": "2022-04-26T08:28:28.488245Z"
    }
   },
   "source": [
    "## `pair_clips_labels[8]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a5cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:05:51.593789Z",
     "start_time": "2022-05-04T11:05:51.592065Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[8]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "# ##SAVE ANIMATIONS\n",
    "# GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "# GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "# animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f3999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T17:50:33.833772Z",
     "start_time": "2022-03-31T17:50:33.828196Z"
    }
   },
   "source": [
    "# List of files to be verified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48d676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T17:50:41.283701Z",
     "start_time": "2022-03-31T17:50:41.198045Z"
    }
   },
   "source": [
    "**Instructions:** \n",
    "Tick the box once it is verified. If there are comments, please add them at the end of each line.\n",
    "See two examples of patient 40 and 41. \n",
    "\n",
    "* [x] 040 {'BKGR': 5, '4CV': 5}\n",
    "* [x] 040 - T1-03clips; \n",
    "* [x] 040 - T2-03clips; \n",
    "* [x] 040 - T3-00clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 041 {'BKGR': 8, '4CV': 8}\n",
    "* [x] 041 - T1-02clips; \n",
    "* [x] 041 - T2-02clips; \n",
    "* [x] 041 - T3-04clips; \n",
    "* [x] Comments: Clip 13 and 14 seems to have moderate or poor quality of 4CV but this might be subjective as I am not an expert (MX). ADDED: Tue 26 Apr 11:35:01 BST 2022\n",
    "* [ ] Comments: \n",
    "* ---\n",
    "* [x] 042 - T1-01clips; \n",
    "* [x] 042 - T2-01clips; \n",
    "* [x] 042 - T3-03clips; \n",
    "* [ ] Comments: \n",
    "* ---\n",
    "* [x] 043 - T1-01clips; \n",
    "* [x] 043 - T2-02clips; \n",
    "* [x] 043 - T3-02clips; \n",
    "* [ ] Comments: \n",
    "* ---\n",
    "* [x] 044 - T1-00clips; \n",
    "* [x] 044 - T2-03clips; \n",
    "* [x] 044 - T3-01clips; \n",
    "* [ ] Comments: \n",
    "* ---\n",
    "* [x] 045 - T1-02clips; \n",
    "* [x] 045 - T2-01clips; \n",
    "* [x] 045 - T3-03clips; \n",
    "* [ ] Comments: \n",
    "* ---\n",
    "* [x] 046 - T1-02clips; \n",
    "* [x] 046 - T2-02clips; \n",
    "* [x] 046 - T3-02clips; \n",
    "* [ ] Comments: \n",
    "* ---\n",
    "* [x] 047 - T1-03clips; \n",
    "* [x] 047 - T2-02clips; \n",
    "* [x] 047 - T3-02clips; \n",
    "* [ ] Comments: \n",
    "* ---\n",
    "* [x] 048 - T1-02clips; \n",
    "* [x] 048 - T2-01clips; \n",
    "* [x] 048 - T3-02clips; \n",
    "* [ ] Comments: \n",
    "* ---\n",
    "* [x] 049 - T1-??clips; \n",
    "* [x] 049 - T2-??clips; \n",
    "* [x] 049 - T3-??clips; \n",
    "* [x] Comments: No mp4 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d3d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:05:51.731597Z",
     "start_time": "2022-05-04T11:05:51.594756Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE_OF_CLIPS = 4\n",
    "\n",
    "print(f' echo_dataset.__len__() = {echo_dataset.__len__()}')\n",
    "echo_dataloader = torch.utils.data.DataLoader(\n",
    "    echo_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "for clip_batch_idx, sample_batched in enumerate(echo_dataloader):\n",
    "    print(f'====================================================')\n",
    "    sample_batched_images=sample_batched[0]\n",
    "    sample_batched_labels=sample_batched[1]\n",
    "    print(f'BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "    print(f'SAMPLE_IDX_LABELS: {  sample_batched_labels  }')\n",
    "    print(f'SAMPLE_BATCH: {sample_batched_images.size()}')\n",
    "    \n",
    "    sample_batched=sample_batched_images #.squeeze()\n",
    "    print(f'SAMPLE_BATCH.squeeze: {sample_batched.size()}')\n",
    "    \n",
    "    for BATCH_SIZE_IDX, label in enumerate(sample_batched_labels):\n",
    "        print(f'   CLIP_BATCH_SIZE_IDX {BATCH_SIZE_IDX} label: {label}')\n",
    "        sample_batched_idx_image = sample_batched[BATCH_SIZE_IDX,...]\n",
    "        print(f'   Sample_batched_idx_image.size()  {sample_batched_idx_image.size() }'  )\n",
    "        \n",
    "        grid = utils.make_grid(sample_batched_idx_image)\n",
    "        print(f'   Grid size {grid.size()}' )\n",
    "         #plt.figure(figsize =(20,20) )\n",
    "         #plt.imshow( grid.cpu().detach().numpy().transpose(1,2,0) )\n",
    "         #plt.title(f'BATCH_SIZE_IDX {BATCH_SIZE_IDX}; Label: {label_id[label]}')\n",
    "         #plt.axis('off')\n",
    "         #plt.ioff()\n",
    "         #plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96ab6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T17:47:30.053304Z",
     "start_time": "2022-04-13T17:47:30.045971Z"
    }
   },
   "source": [
    "## TO BE VERIFIED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c2552",
   "metadata": {},
   "source": [
    "\n",
    "* =======================\n",
    "* [ ] 050 - T1-??clips; \n",
    "* [ ] 050 - T2-??clips; \n",
    "* [ ] 050 - T3-??clips; \n",
    "* ---\n",
    "* [ ] 051 - T1-??clips; \n",
    "* [ ] 051 - T2-??clips; \n",
    "* [ ] 051 - T3-??clips; \n",
    "* ---\n",
    "* [ ] 052 - T1-??clips; \n",
    "* [ ] 052 - T2-??clips; \n",
    "* [ ] 052 - T3-??clips; \n",
    "* ---\n",
    "* [ ] 053 - T1-??clips; \n",
    "* [ ] 053 - T2-??clips; \n",
    "* [ ] 053 - T3-??clips; \n",
    "* ---\n",
    "* [ ] 054 - T1-??clips; \n",
    "* [ ] 054 - T2-??clips; \n",
    "* [ ] 054 - T3-??clips; \n",
    "* ---\n",
    "* [ ] 055 - T1-??clips; \n",
    "* [ ] 055 - T2-??clips; \n",
    "* [ ] 055 - T3-??clips; \n",
    "* ---\n",
    "* [ ] 056 - T1-??clips; \n",
    "* [ ] 056 - T2-??clips; \n",
    "* [ ] 056 - T3-??clips; \n",
    "* ---\n",
    "* [ ] 057 - T1-??clips; \n",
    "* [ ] 057 - T2-??clips; \n",
    "* [ ] 057 - T3-??clips; \n",
    "* ---\n",
    "* [ ] 058 - T1-??clips; \n",
    "* [ ] 058 - T2-??clips; \n",
    "* [ ] 058 - T3-??clips; \n",
    "* ---\n",
    "* [ ] 059 - T1-??clips; \n",
    "* [ ] 059 - T2-??clips; \n",
    "* [ ] 059 - T3-??clips; \n",
    "* =======================\n",
    "* [ ] 060 - T1-01clips; \n",
    "* [ ] 060 - T2-00clips; \n",
    "* [ ] 060 - T3-00clips\n",
    "* ---\n",
    "* [ ] 061 - T1-00clips; \n",
    "* [ ] 061 - T2-00clips; \n",
    "* [ ] 061 - T3-01clips\n",
    "* ---\n",
    "* [ ] 061 - T1-02clips; \n",
    "* [ ] 061 - T2-02clips; \n",
    "* [ ] 061 - T3-01clips\n",
    "* ---\n",
    "* [ ] 064 - T1-02clips; \n",
    "* [ ] 064 - T2-03clips; \n",
    "* [ ] 064 - T3-00clips\n",
    "* ---\n",
    "* [ ] 065 - T1-02clips; \n",
    "* [ ] 065 - T2-04clips; \n",
    "* [ ] 065 - T3-05clips\n",
    "* ---\n",
    "* [ ] 066 - T1-01clips; \n",
    "* [ ] 066 - T2-02clips; \n",
    "* [ ] 066 - T3-00clips\n",
    "* ---\n",
    "* [ ] 067 - T1-00clips; \n",
    "* [ ] 067 - T2-00clips; \n",
    "* [ ] 067 - T3-00clips\n",
    "* ---\n",
    "* [ ] 068 - T1-02clips; \n",
    "* [ ] 068 - T2-00clips; \n",
    "* [ ] 068 - T3-02clips\n",
    "* ---\n",
    "* [ ] 069 - T1-02clips; \n",
    "* [ ] 069 - T2-03clips; \n",
    "* [ ] 069 - T3-02clips\n",
    "* =======================\n",
    "* [ ] 070 - T1-04clips; \n",
    "* [ ] 070 - T2-04clips; \n",
    "* [ ] 070 - T3-01clips \n",
    "* ---\n",
    "* [ ] 071 - T1-00clips; \n",
    "* [ ] 071 - T2-02clips; \n",
    "* [ ] 071 - T3-01clips\n",
    "* ---\n",
    "* [ ] 072 - T1-01clips; \n",
    "* [ ] 072 - T2-03clips; \n",
    "* [ ] 072 - T3-02clips\n",
    "* ---\n",
    "* [ ] 073 - T1-02clips; \n",
    "* [ ] 073 - T2-01clips; \n",
    "* [ ] 073 - T3-00clips\n",
    "* ---\n",
    "* [ ] 074 - T1-02clips; \n",
    "* [ ] 074 - T2-02clips; \n",
    "* [ ] 074 - T3-00clips\n",
    "* ---\n",
    "* [ ] 075 - T1-01clips; \n",
    "* [ ] 075 - T2-02clips; \n",
    "* [ ] 075 - T3-02clips\n",
    "* ---\n",
    "* [ ] 076 - T1-01clips; \n",
    "* [ ] 076 - T2-01clips; \n",
    "* [ ] 076 - T3-02clips\n",
    "* ---\n",
    "* [ ] 077 - T1-00clips; \n",
    "* [ ] 077 - T2-00clips; \n",
    "* [ ] 077 - T3-00clips\n",
    "* ---\n",
    "* [ ] 078 - T1-01clips; \n",
    "* [ ] 078 - T2-01clips; \n",
    "* [ ] 078 - T3-01clips\n",
    "* ---\n",
    "* [ ] 079 - T1-01clips; \n",
    "* [ ] 079 - T2-02clips; \n",
    "* [ ] 079 - T3-01clips\n",
    "* ---\n",
    "* [ ] 080 - T1-00clips; \n",
    "* [ ] 080 - T2-00clips; \n",
    "* [ ] 080 - T3-00clips\n",
    "* =======================\n",
    "* [ ] 081 - T1-00clips; \n",
    "* [ ] 081 - T2-00clips; \n",
    "* [ ] 081 - T3-00clips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47021a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:05:51.734542Z",
     "start_time": "2022-05-04T11:05:51.732622Z"
    }
   },
   "outputs": [],
   "source": [
    "## TODO: For better quality control of subject verifcation\n",
    "# subjectsCSV_PATH = 'datasets/vital-us/echocardiography/subjectsCSV/anonimised'\n",
    "# CSV_FILE = 'validation_anonymised_april2022.csv'\n",
    "# FULL_PATH_FOR_CSV_FILE = os.path.join(HOME_PATH, subjectsCSV_PATH, CSV_FILE)\n",
    "# datatablea_validation_anonymised = pd.read_csv(FULL_PATH_FOR_CSV_FILE)\n",
    "\n",
    "# ## Filtering columns\n",
    "# basic_demographics=datatablea_validation_anonymised.filter(items=[ 'SUBJID', 'LABELLED', 'CLIPS_DAY01', 'CLIPS_DAY02', 'CLIPS_DAY02'])\n",
    "# basic_demographics\n",
    "\n",
    "# print(f'=================== LABELLED =======================')\n",
    "# basic_demographics['LABELLED'].value_counts().plot.pie(autopct='%.1f %%', ylabel='TOTAL', legend=True)\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "###SAVE ANIMATIONS\n",
    "# ### Save animation as gif (if required) or other formats https://holypython.com/how-to-save-matplotlib-animations-the-ultimate-guide/\n",
    "# f = r\"/home/mx19/repositories/echocardiography/scripts/learning-pipeline/animation.gif\" \n",
    "# writergif = animation.PillowWriter(fps=30) \n",
    "# writergif2='imagemagick'\n",
    "# anim.save(f, dpi=80, writer=writergif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf219c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a060e041",
   "metadata": {},
   "source": [
    "## [**!WARNING!**] Cleanup temporal data directory \n",
    "Remove directory if a temporary was used.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "       Make sure you know which path you will remove as you do not like to remove important files.\n",
    "       shutil.rmtree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ae88d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-04T11:05:51.736932Z",
     "start_time": "2022-05-04T11:05:51.735565Z"
    }
   },
   "outputs": [],
   "source": [
    "# temporal_files_path = config['temporal_data_path']\n",
    "\n",
    "# shutil.rmtree(temporal_files_path)\n",
    "# print(f' {temporal_files_path} is empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5420c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
