{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef885fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:05.695513Z",
     "start_time": "2021-12-17T09:18:03.125501Z"
    }
   },
   "source": [
    "# Curation, selection and validation of 4CV frame labels\n",
    "\n",
    "**Author(s):** Miguel Xochicale [@mxochicale](https://github.com/mxochicale)  \n",
    "**Contributor(s):** Nhat Phug [@huynhatd13](https://github.com/huynhatd13) \n",
    "\n",
    "March2022; April2022; May 2022\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This notebook presents prototypes to pre-process echocardiography datasets with the use of pytorch features. \n",
    "\n",
    "### Running notebook\n",
    "\n",
    "1. Go to echocardiography repository path: `$HOME/repositories/echocardiography/`\n",
    "2. Open echocardiography repo in pycharm and in the terminal type:\n",
    "    ```\n",
    "    git checkout master # or the branch\n",
    "    git pull # to bring a local branch up-to-date with its remote version\n",
    "    ```\n",
    "3. Launch Notebook server\n",
    "    Go to you repository path: `cd $HOME/repositories/echocardiography/scripts/dataloaders` and type in the pycharm terminal:\n",
    "    ```\n",
    "    conda activate rt-ai-echo-VE \n",
    "    jupyter notebook\n",
    "    ```\n",
    "    which will open your web-browser.\n",
    "    \n",
    "    \n",
    "### References\n",
    "* \"Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback\". https://www.fda.gov/media/122535/download \n",
    "* Gomez A. et al. 2021 https://github.com/vital-ultrasound/lung/blob/main/multiclass_pytorch/datasets/LUSVideoDataset.py \n",
    "* Save animation as gif (if required) or other formats https://holypython.com/how-to-save-matplotlib-animations-the-ultimate-guide/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2aa0",
   "metadata": {},
   "source": [
    "## Jupyter Notebook\n",
    "### Setting imports and datasets paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345efe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:48.126096Z",
     "start_time": "2022-05-12T14:26:47.335361Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, display #to be used with HTML(animation.ArtistAnimation().to_jshtml())\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from source.dataloaders.EchocardiographicVideoDataset import EchoClassesDataset\n",
    "from source.models.learning_misc import train_loop, test_loop\n",
    "from source.helpers.various import concatenating_YAML_via_tags, plot_dataset_classes, split_train_validate_sets\n",
    "\n",
    "\n",
    "HOME_PATH = os.path.expanduser(f'~')\n",
    "USERNAME = os.path.split(HOME_PATH)[1]\n",
    "CONFIG_FILES_PATH= 'repositories/echocardiography/scripts/config_files/users_paths_files'\n",
    "YML_FILE =  'config_users_paths_files_username_' + USERNAME + '.yml'\n",
    "FULL_PATH_FOR_YML_FILE = os.path.join(HOME_PATH, CONFIG_FILES_PATH, YML_FILE)\n",
    "PATH_for_temporal_files = os.path.join(HOME_PATH, 'datasets/vital-us/echocardiography/temporal-files')\n",
    "\n",
    "yaml.add_constructor('!join', concatenating_YAML_via_tags)  ## register the tag handler\n",
    "with open(FULL_PATH_FOR_YML_FILE, 'r') as yml:\n",
    "    config = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:08.264310Z",
     "start_time": "2021-12-17T09:18:08.250178Z"
    }
   },
   "source": [
    "### Splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a8fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:48.146087Z",
     "start_time": "2022-05-12T14:26:48.127487Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Setting ECHODATASET_PATH; \n",
    "#ECHODATASET_PATH = config['echodataset_path'] # Default\n",
    "ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-ALL'\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-in-verification40-49'\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-in-verification50-59'\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-in-verification60-69'\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-in-verification70-79'\n",
    "\n",
    "split_train_validate_sets(  \n",
    "                        ECHODATASET_PATH, #config['echodataset_path']\n",
    "                        config['data_list_output_path'], \n",
    "                        config['ntraining'],\n",
    "                        config['randomise_file_list']\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3865c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T09:48:38.180954Z",
     "start_time": "2022-03-31T09:48:38.175487Z"
    }
   },
   "source": [
    "# Verification of participants\n",
    "### Setting up variables for participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd7228",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:48.150748Z",
     "start_time": "2022-05-12T14:26:48.148801Z"
    }
   },
   "outputs": [],
   "source": [
    "SUBJECT_ID = '053'\n",
    "NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 500\n",
    "PRETRANSFORM_IM_SIZE = [200, 200] #[650, 690] original pixel size for VenueGO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e7c31",
   "metadata": {},
   "source": [
    "### Other relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292478a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:48.153438Z",
     "start_time": "2022-05-12T14:26:48.151796Z"
    }
   },
   "outputs": [],
   "source": [
    "interval_between_frames_in_milliseconds=33.3 ## 1/30=0.033333\n",
    "frame_per_seconds_for_animated_frames=30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92177c6e",
   "metadata": {},
   "source": [
    "###  Setting up txt list for participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e660103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:48.208897Z",
     "start_time": "2022-05-12T14:26:48.154263Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #or \"cuda:0\"\n",
    "\n",
    "videolist = config['participant_videos_list_full']\n",
    "annotationlist = config['participant_path_json_list_full']\n",
    "\n",
    "video_filenames = [line.strip() for line in open(videolist)]\n",
    "annotation_filenames = [line.strip() for line in open(annotationlist)]\n",
    "\n",
    "data_list_output_path = config['data_list_output_path']\n",
    "#print(data_list_output_path)\n",
    "\n",
    "all_videos_files_to_be_verified = 'video_list_tbv.txt'\n",
    "all_annotation_files_to_be_verified = 'annotation_list_tbv.txt'\n",
    "videolist_tbv_txt = '{}{}'.format(data_list_output_path, all_videos_files_to_be_verified)\n",
    "annotationlist_tbv_txt = '{}{}'.format(data_list_output_path, all_annotation_files_to_be_verified)\n",
    "\n",
    "JSON_FILE_PATTERN = '*'+SUBJECT_ID+'*4[cC][vV].[jJ][sS][oO][nN]'\n",
    "MP4_FILE_PATTERN = '*'+SUBJECT_ID+'*echo*.[mM][pP][4]'\n",
    "\n",
    "result = list(sorted(Path(ECHODATASET_PATH).rglob(MP4_FILE_PATTERN)))\n",
    "with open(videolist_tbv_txt, 'w') as file_list_txt:\n",
    "    for mp4_filename_i in result:\n",
    "        file_n_nopath = str(mp4_filename_i).replace(ECHODATASET_PATH, '')\n",
    "        file_list_txt.write(file_n_nopath + '\\n')\n",
    "        \n",
    "\n",
    "result = list(sorted(Path(ECHODATASET_PATH).rglob(JSON_FILE_PATTERN)))\n",
    "with open(annotationlist_tbv_txt, 'w') as file_list_txt:\n",
    "    for mp4_filename_i in result:\n",
    "        file_n_nopath = str(mp4_filename_i).replace(ECHODATASET_PATH, '')\n",
    "        file_list_txt.write(file_n_nopath + '\\n')        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e607bf7",
   "metadata": {},
   "source": [
    "### Setting variables and loading datasets using pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:48.240081Z",
     "start_time": "2022-05-12T14:26:48.210727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining transforms that apply to the entire dataset.\n",
    "# These transforms are not augmentation.\n",
    "if config['use_pretransform_image_size']:\n",
    "    pretransform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=PRETRANSFORM_IM_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    pretransform = None\n",
    "\n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_train_augmentation']:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),  # in degrees\n",
    "        transforms.RandomEqualize(p=0.5),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "    \n",
    "\n",
    "echo_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_tbv'],\n",
    "    participant_path_json_list=config['participant_path_json_list_tbv'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP,\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,\n",
    "    use_tmp_storage=config['use_tmp_storage'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760906",
   "metadata": {},
   "source": [
    "### Plotting Class Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6472a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.188360Z",
     "start_time": "2022-05-12T14:26:48.240942Z"
    }
   },
   "outputs": [],
   "source": [
    "label_id = ('BKGR', '4CV')\n",
    "number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip'] \n",
    "\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_class_dict = {\n",
    "   'BKGR': 0 ,\n",
    "   \"4CV\": 0\n",
    "    }\n",
    "    \n",
    "    for clip_index_i in range(len(dataset_obj)):\n",
    "        data_idx = dataset_obj[clip_index_i]\n",
    "        label_id_idx = data_idx[1]\n",
    "        label = label_id[label_id_idx]\n",
    "        count_class_dict[label]+= 1\n",
    "        #count_class_dict[label]+= 1* number_of_frames_per_segment_in_a_clip\n",
    "\n",
    "    return count_class_dict\n",
    "        \n",
    "        \n",
    "def plot_from_dict(dict_obj, plot_title, **kwargs):\n",
    "    return sns.barplot(data = pd.DataFrame.from_dict([dict_obj]).melt(), \n",
    "                       x = \"variable\", y=\"value\", hue=\"variable\", **kwargs).set_title(plot_title)\n",
    "\n",
    "print(get_class_distribution(echo_dataset))\n",
    "\n",
    "plot_title_train_label= f'TRAIN dataset of {len(echo_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "#fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(18,7))\n",
    "plot_from_dict(get_class_distribution(echo_dataset), plot_title=plot_title_train_label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ea2a6",
   "metadata": {},
   "source": [
    "### Animating frames of one clip of the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73552935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.189660Z",
     "start_time": "2022-05-12T14:26:53.189651Z"
    }
   },
   "outputs": [],
   "source": [
    "#average_HR =\n",
    "#fps = 30\n",
    "# 60 # beats per minute \n",
    "#Beats-per-minute: 60 BPM\n",
    "#Beats-per-second: 1 Hz\n",
    "#Cycle-per-second: 1 (Cycle/s)\n",
    "\n",
    "number_of_clips = len(echo_dataset)\n",
    "\n",
    "\n",
    "print(f'---------------------------------------')\n",
    "### Setting clips\n",
    "clips=[]\n",
    "for clip_index in range( int(number_of_clips)  ):\n",
    "    data_idx = echo_dataset[clip_index]\n",
    "    data_clip_idx = data_idx[0]\n",
    "    label_clip_idx = data_idx[1]\n",
    "    clip_frame_clip_idx = data_idx[2]\n",
    "    n_available_frames_clip_idx = data_idx[3]\n",
    "    print(f' CLIP:{clip_index:02d} of {label_id[label_clip_idx]} label for {data_clip_idx.size()} TOTAL_FRAMES: {n_available_frames_clip_idx} from clip_frame_clip_idx {clip_frame_clip_idx}')    \n",
    "    clips.append([data_clip_idx, label_clip_idx, clip_index, clip_frame_clip_idx, n_available_frames_clip_idx ]) \n",
    "\n",
    "\n",
    "# ### Setting and displaying pair of clips    \n",
    "def pair_clips_labels(clips):\n",
    "    pair_clips_labels_=[]    \n",
    "    number_of_clips=len(clips)\n",
    "    for clip_index_i_A in range( int(number_of_clips/2)  ):\n",
    "        clip_index_i_B=int(number_of_clips/2) + clip_index_i_A \n",
    "        print(f' pair_clips_labels[{clip_index_i_A}]-- BKRG:{clip_index_i_A}, 4CV:{clip_index_i_B}')\n",
    "        data_clip_i_A=clips[clip_index_i_A][0]\n",
    "        label_i_A=clips[clip_index_i_A][1]\n",
    "        clip_i_A=clips[clip_index_i_A][2]\n",
    "        number_of_frames_A=clips[clip_index_i_A][4]\n",
    "        data_clip_i_B=clips[clip_index_i_B][0]\n",
    "        label_i_B=clips[clip_index_i_B][1]\n",
    "        clip_i_B=clips[clip_index_i_B][2]\n",
    "        number_of_frames_B=clips[clip_index_i_B][4]\n",
    "        pair_clips_labels_.append([data_clip_i_A, label_i_A, clip_i_A, number_of_frames_A, data_clip_i_B, label_i_B, clip_i_B, number_of_frames_B])\n",
    "    \n",
    "    return(pair_clips_labels_)\n",
    "\n",
    "\n",
    "print(f'---------------------------------------')\n",
    "pair_clips_labels = pair_clips_labels(clips)\n",
    "\n",
    "### Defining animate clips\n",
    "def animate_clips(pair_clips_labels):\n",
    "    #print(f' CLIP: for {label_id[pair_clips_labels[1]]} ')\n",
    "    fig = plt.figure()    \n",
    "    pair_of_clip_index_i_frames=[]   \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "    data_clip_tensor_A=pair_clips_labels[0]\n",
    "    label_clip_A=pair_clips_labels[1]\n",
    "    clip_i_A=pair_clips_labels[2]\n",
    "    number_of_frames_A=pair_clips_labels[3]\n",
    "    data_clip_tensor_B=pair_clips_labels[4]\n",
    "    label_clip_B=pair_clips_labels[5]\n",
    "    clip_i_B=pair_clips_labels[6]\n",
    "    number_of_frames_B=pair_clips_labels[7]\n",
    "    \n",
    "    \n",
    "    ax1.title.set_text(f' CLIP: {clip_i_A:02d}--{label_id[label_clip_A]} with {number_of_frames_A} of {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} frames [for Subject {SUBJECT_ID}]')\n",
    "    ax2.title.set_text(f' CLIP: {clip_i_B:02d}--{label_id[label_clip_B]} with {number_of_frames_B} of {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} frames [for Subject {SUBJECT_ID}]')\n",
    "    for frames_idx in range(data_clip_tensor_A[:,:,...].size()[1]):\n",
    "        imA = ax1.imshow(data_clip_tensor_A[:,frames_idx,...].cpu().detach().numpy().transpose(1,2,0) , cmap=plt.get_cmap('gray') )  \n",
    "        imB = ax2.imshow(data_clip_tensor_B[:,frames_idx,...].cpu().detach().numpy().transpose(1,2,0) , cmap=plt.get_cmap('gray') )  \n",
    "        pair_of_clip_index_i_frames.append( [imA, imB] )\n",
    "    fig.tight_layout()    \n",
    "    #return fig, pair_of_clip_index_i_frames\n",
    "\n",
    "    anim = animation.ArtistAnimation(fig, pair_of_clip_index_i_frames, interval=interval_between_frames_in_milliseconds, blit=True, repeat_delay=1000)\n",
    "    return anim\n",
    "\n",
    "# plt.show()    \n",
    "###BLURS\n",
    "#\n",
    "#fig, ax = plt.subplots()\n",
    "#clips_axes.append( clip_index_i_frames )\n",
    "# #plt.axes()\n",
    "# #plt.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b104687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.190474Z",
     "start_time": "2022-05-12T14:26:53.190464Z"
    }
   },
   "outputs": [],
   "source": [
    "##SAVE ANIMATIONS\n",
    "for idx in range(0,len(pair_clips_labels)):\n",
    "    PAIR_OF_CLIPS = pair_clips_labels[idx]\n",
    "    print( f' pair_clips_labels {str(PAIR_OF_CLIPS[2])} {str(PAIR_OF_CLIPS[6])}')\n",
    "\n",
    "    animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "    HTML(animated_frames.to_jshtml())   \n",
    "\n",
    "    GIF_FILE='subject'+SUBJECT_ID+'_clips_'+str(PAIR_OF_CLIPS[2])+'-'+str(PAIR_OF_CLIPS[6])+'.gif'    \n",
    "    GIF_FILENAME_FULLPATH = os.path.join(PATH_for_temporal_files, GIF_FILE)\n",
    "    animated_frames.save(GIF_FILENAME_FULLPATH, writer=animation.PillowWriter(fps=frame_per_seconds_for_animated_frames))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d225",
   "metadata": {},
   "source": [
    "### Displayting animated frames in the dataloader\n",
    "**NOTE:** you might need to uncommed the lines where there is no clips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57688b",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[0]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6c3f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.191204Z",
     "start_time": "2022-05-12T14:26:53.191194Z"
    }
   },
   "outputs": [],
   "source": [
    "PAIR_OF_CLIPS = pair_clips_labels[0]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "HTML(animated_frames.to_jshtml())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e21006",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[1]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bcb91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.192069Z",
     "start_time": "2022-05-12T14:26:53.192057Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[1]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce17dd4",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[2]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d95b24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.192912Z",
     "start_time": "2022-05-12T14:26:53.192901Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[2]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c6b51",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[3]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eeb96e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.193733Z",
     "start_time": "2022-05-12T14:26:53.193721Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[3]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce1129f",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[4]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a82233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.194443Z",
     "start_time": "2022-05-12T14:26:53.194431Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[4]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73496d56",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[5]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d66f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.195219Z",
     "start_time": "2022-05-12T14:26:53.195207Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[5]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0199ee",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[6]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc1975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.195993Z",
     "start_time": "2022-05-12T14:26:53.195981Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[6]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82637f3e",
   "metadata": {},
   "source": [
    "## `pair_clips_labels[7]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a717aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.196732Z",
     "start_time": "2022-05-12T14:26:53.196721Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[7]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1919d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T08:28:28.490581Z",
     "start_time": "2022-04-26T08:28:28.488245Z"
    }
   },
   "source": [
    "## `pair_clips_labels[8]`\n",
    "**NOTE:** _you might need to comment this line as won't be available for_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a5cc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.197552Z",
     "start_time": "2022-05-12T14:26:53.197541Z"
    }
   },
   "outputs": [],
   "source": [
    "# PAIR_OF_CLIPS = pair_clips_labels[8]\n",
    "\n",
    "# animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "# HTML(animated_frames.to_jshtml())   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f3999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T17:50:33.833772Z",
     "start_time": "2022-03-31T17:50:33.828196Z"
    }
   },
   "source": [
    "# List of verified files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48d676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-31T17:50:41.283701Z",
     "start_time": "2022-03-31T17:50:41.198045Z"
    }
   },
   "source": [
    "**Instructions:** \n",
    "Tick the box once it is verified. If there are comments, please add them at the end of each line.\n",
    "See two examples of patient 40 and 41. \n",
    "\n",
    "\n",
    "* =====================================================================\n",
    "* =====================================================================\n",
    "* =====================================================================\n",
    "* [x] 040 - T1-03clips; \n",
    "* [x] 040 - T2-02clips; \n",
    "* [x] 040 - T3-00clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 041 - T1-02clips; \n",
    "* [x] 041 - T2-02clips; \n",
    "* [x] 041 - T3-04clips; \n",
    "* [x] Comments:\n",
    "* ---\n",
    "* [x] 042 - T1-01clips; \n",
    "* [x] 042 - T2-01clips; \n",
    "* [x] 042 - T3-03clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 043 - T1-01clips; \n",
    "* [x] 043 - T2-02clips; \n",
    "* [x] 043 - T3-02clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 044 - T1-00clips; \n",
    "* [x] 044 - T2-03clips; \n",
    "* [x] 044 - T3-01clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 045 - T1-02clips; \n",
    "* [x] 045 - T2-01clips; \n",
    "* [x] 045 - T3-03clips; \n",
    "* [x] Comments: 4CVs don't look good: subject045_clips_2-8.gif; subject045_clips_3-9.gif; subject045_clips_4-10.gif\n",
    "* [x] Comments: Renamed filenames\n",
    "* ---\n",
    "* [x] 046 - T1-02clips; \n",
    "* [x] 046 - T2-02clips; \n",
    "* [x] 046 - T3-02clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 047 - T1-03clips; \n",
    "* [x] 047 - T2-02clips; \n",
    "* [x] 047 - T3-02clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 048 - T1-02clips; \n",
    "* [x] 048 - T2-01clips; \n",
    "* [x] 048 - T3-02clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 049 - T1-??clips; \n",
    "* [x] 049 - T2-??clips; \n",
    "* [x] 049 - T3-??clips; \n",
    "* [x] Comments: NO_VIDEOS\n",
    "* [x] Comments: TODO: No annotated!\n",
    "* =====================================================================\n",
    "* =====================================================================\n",
    "* =====================================================================\n",
    "* [x] 050 - T1-02clips; \n",
    "* [x] 050 - T2-02clips; \n",
    "* [x] 050 - T3-02clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 051 - T1-02clips; \n",
    "* [x] 051 - T2-02clips; \n",
    "* [x] 051 - T3-01clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 052 - T1-02clips; \n",
    "* [x] 052 - T2-01clips; \n",
    "* [x] 052 - T3-00clips; \n",
    "* [x] Comments: MX: 4CV doesn't look good to me! subject052_clips_0-3.gif; subject052_clips_2-5.gif\n",
    "* [x] Comments: TODO: Nhat will re-annotate labels\n",
    "* ---\n",
    "* [x] 053 - T1-04clips; \n",
    "* [x] 053 - T2-02clips; \n",
    "* [x] 053 - T3-03clips; \n",
    "* [x] Comments: 4CV don't look good: subject053_clips_6-15.gif; subject053_clips_7-16.gif; subject053_clips_8-17.gif\n",
    "* [x] Comments: json files were duplicated and therefore delete one and delete one mp4!\n",
    "* ---\n",
    "* [x] 054 - T1-02clips; \n",
    "* [x] 054 - T2-00clips; \n",
    "* [x] 054 - T3-02clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 055 - T1-??clips; \n",
    "* [x] 055 - T2-??clips; \n",
    "* [x] 055 - T3-??clips; \n",
    "* [x] Comments: NO_VIDEOS\n",
    "* [x] Comments: Confirmed NO VIDEOS\n",
    "* ---\n",
    "* [x] 056 - T1-00clips; \n",
    "* [x] 056 - T2-01clips; \n",
    "* [x] 056 - T3-01clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 057 - T1-00clips; \n",
    "* [x] 057 - T2-01clips; \n",
    "* [x] 057 - T3-02clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 058 - T1-01clips; \n",
    "* [x] 058 - T2-02clips; \n",
    "* [x] 058 - T3-03clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 059 - T1-??clips; \n",
    "* [x] 059 - T2-??clips; \n",
    "* [x] 059 - T3-??clips; \n",
    "* [x] Comments: NO_VIDEOS\n",
    "* [x] Comments: Confirmed NO_VIDEOS\n",
    "* =====================================================================\n",
    "* =====================================================================\n",
    "* =====================================================================\n",
    "* [x] 060 - T1-01clips; \n",
    "* [x] 060 - T2-00clips; \n",
    "* [x] 060 - T3-00clips;\n",
    "* [x] Comments: 4CVs don't look good: subject060_clips_0-1.gif\n",
    "* [x] Comments: 4CVs is OKAY as there are many factors that are considered in the quality\n",
    "* ---\n",
    "* [x] 061 - T1-00clips; \n",
    "* [x] 061 - T2-00clips; \n",
    "* [x] 061 - T3-01clips;\n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 062 - T1-??clips; \n",
    "* [x] 062 - T2-??clips; \n",
    "* [x] 062 - T3-??clips;\n",
    "* [x] Comments: NO_VIDEOS\n",
    "* [x] Comments: Confirmed NO_VIDEOS\n",
    "* ---\n",
    "* [x] 063 - T1-02clips; \n",
    "* [x] 063 - T2-02clips; \n",
    "* [x] 063 - T3-01clips;\n",
    "* [x] Comments:  4CVs don't look good: subject063_clips_2-7.gif\n",
    "* [x] Comments: It's fine but has lung artifacts\n",
    "* ---\n",
    "* [x] 064 - T1-02clips; \n",
    "* [x] 064 - T2-03clips; \n",
    "* [x] 064 - T3-00clips;\n",
    "* [x] Comments: 4CVs don't look good to me: subject064_clips_0-5.gif; subject064_clips_1-6.gif; subject064_clips_2-7.gif; subject064_clips_3-8.gif; subject064_clips_4-9.gif\n",
    "* [x] Comments: 4CVs contain lung artifacts which is fine\n",
    "* ---\n",
    "* [x] 065 - T1-03clips; \n",
    "* [x] 065 - T2-04clips; \n",
    "* [x] 065 - T3-05clips;\n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 066 - T1-01clips; \n",
    "* [x] 066 - T2-02clips; \n",
    "* [x] 066 - T3-00clips;\n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 067 - T1-??clips; \n",
    "* [x] 067 - T2-??clips; \n",
    "* [x] 067 - T3-??clips;\n",
    "* [x] Comments: NO_VIDEOS\n",
    "* [x] Comments: Confirmed NO_VIDEOS\n",
    "* ---\n",
    "* [x] 068 - T1-02clips; \n",
    "* [x] 068 - T2-00clips; \n",
    "* [x] 068 - T3-02clips;\n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 069 - T1-02clips; \n",
    "* [x] 069 - T2-02clips; \n",
    "* [x] 069 - T3-02clips;\n",
    "* [x] Comments: \n",
    "* =====================================================================\n",
    "* =====================================================================\n",
    "* =====================================================================\n",
    "* [x] 070 - T1-04clips; \n",
    "* [x] 070 - T2-04clips; \n",
    "* [x] 070 - T3-01clips; \n",
    "* [x] Comments: 4CVs don't look good to me: subject070_clips_4-13.gif; subject070_clips_5-14.gif; subject070_clips_6-15.gif; subject070_clips_7-16.gif\n",
    "* [x] Comments: TODO: Annotarions are wrong! It needs to be re-annotated!\n",
    "* ---\n",
    "* [x] 071 - T1-01clips; \n",
    "* [x] 071 - T2-02clips; \n",
    "* [x] 071 - T3-01clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 072 - T1-01clips; \n",
    "* [x] 072 - T2-03clips; \n",
    "* [x] 072 - T3-02clips\n",
    "* [x] Comments: 4CVs don't look good to me: subject072_clips_4-10.gif; subject072_clips_5-11.gif\n",
    "* [x] Comments: No 4CV for clips in T3\n",
    "* ---\n",
    "* [x] 073 - T1-02clips; \n",
    "* [x] 073 - T2-01clips; \n",
    "* [x] 073 - T3-00clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 074 - T1-02clips; \n",
    "* [x] 074 - T2-02clips; \n",
    "* [x] 074 - T3-00clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 075 - T1-01clips; \n",
    "* [x] 075 - T2-02clips; \n",
    "* [x] 075 - T3-02clips; \n",
    "* [x] Comments: Duplicated 4CVs clips: subject075_clips_1-6.gif; subject075_clips_2-7.gif\n",
    "* [x] Comments: It is not duplicated! So, clips are good!\n",
    "* ---\n",
    "* [x] 076 - T1-01clips; \n",
    "* [x] 076 - T2-01clips; \n",
    "* [x] 076 - T3-02clips; \n",
    "* [x] Comments: \n",
    "* ---\n",
    "* [x] 077 - T1-00clips; \n",
    "* [x] 077 - T2-00clips; \n",
    "* [x] 077 - T3-00clips; \n",
    "* [x] Comments: VIDEOS BUT NO 4CV CLIPS\n",
    "* ---\n",
    "* [x] 078 - T1-01clips; \n",
    "* [x] 078 - T2-01clips; \n",
    "* [x] 078 - T3-01clips; \n",
    "* [x] Comments: 4CVs don't look good to me: subject078_clips_0-3.gif\n",
    "* [x] Comments: 4CVs looks good but perhaps not at the best quality!\n",
    "* ---\n",
    "* [x] 079 - T1-01clips; \n",
    "* [x] 079 - T2-02clips; \n",
    "* [x] 079 - T3-02clips; \n",
    "* [x] Comments: \n",
    "* ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d3d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.198330Z",
     "start_time": "2022-05-12T14:26:53.198318Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE_OF_CLIPS = 4\n",
    "\n",
    "print(f' echo_dataset.__len__() = {echo_dataset.__len__()}')\n",
    "echo_dataloader = torch.utils.data.DataLoader(\n",
    "    echo_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "for clip_batch_idx, sample_batched in enumerate(echo_dataloader):\n",
    "    print(f'====================================================')\n",
    "    sample_batched_images=sample_batched[0]\n",
    "    sample_batched_labels=sample_batched[1]\n",
    "    print(f'BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "    print(f'SAMPLE_IDX_LABELS: {  sample_batched_labels  }')\n",
    "    print(f'SAMPLE_BATCH: {sample_batched_images.size()}')\n",
    "    \n",
    "    sample_batched=sample_batched_images #.squeeze()\n",
    "    print(f'SAMPLE_BATCH.squeeze: {sample_batched.size()}')\n",
    "    \n",
    "    for BATCH_SIZE_IDX, label in enumerate(sample_batched_labels):\n",
    "        print(f'   CLIP_BATCH_SIZE_IDX {BATCH_SIZE_IDX} label: {label}')\n",
    "        sample_batched_idx_image = sample_batched[BATCH_SIZE_IDX,...]\n",
    "        print(f'   Sample_batched_idx_image.size()  {sample_batched_idx_image.size() }'  )\n",
    "        \n",
    "        grid = utils.make_grid(sample_batched_idx_image)\n",
    "        print(f'   Grid size {grid.size()}' )\n",
    "         #plt.figure(figsize =(20,20) )\n",
    "         #plt.imshow( grid.cpu().detach().numpy().transpose(1,2,0) )\n",
    "         #plt.title(f'BATCH_SIZE_IDX {BATCH_SIZE_IDX}; Label: {label_id[label]}')\n",
    "         #plt.axis('off')\n",
    "         #plt.ioff()\n",
    "         #plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96ab6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T17:47:30.053304Z",
     "start_time": "2022-04-13T17:47:30.045971Z"
    }
   },
   "source": [
    "## TO BE VERIFIED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c2552",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "* [ ] 080 - T1-00clips; \n",
    "* [ ] 080 - T2-00clips; \n",
    "* [ ] 080 - T3-00clips\n",
    "* =======================\n",
    "* [ ] 081 - T1-00clips; \n",
    "* [ ] 081 - T2-00clips; \n",
    "* [ ] 081 - T3-00clips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47021a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.199039Z",
     "start_time": "2022-05-12T14:26:53.199028Z"
    }
   },
   "outputs": [],
   "source": [
    "## TODO: For better quality control of subject verifcation\n",
    "# subjectsCSV_PATH = 'datasets/vital-us/echocardiography/subjectsCSV/anonimised'\n",
    "# CSV_FILE = 'validation_anonymised_april2022.csv'\n",
    "# FULL_PATH_FOR_CSV_FILE = os.path.join(HOME_PATH, subjectsCSV_PATH, CSV_FILE)\n",
    "# datatablea_validation_anonymised = pd.read_csv(FULL_PATH_FOR_CSV_FILE)\n",
    "\n",
    "# ## Filtering columns\n",
    "# basic_demographics=datatablea_validation_anonymised.filter(items=[ 'SUBJID', 'LABELLED', 'CLIPS_DAY01', 'CLIPS_DAY02', 'CLIPS_DAY02'])\n",
    "# basic_demographics\n",
    "\n",
    "# print(f'=================== LABELLED =======================')\n",
    "# basic_demographics['LABELLED'].value_counts().plot.pie(autopct='%.1f %%', ylabel='TOTAL', legend=True)\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "###SAVE ANIMATIONS\n",
    "# ### Save animation as gif (if required) or other formats https://holypython.com/how-to-save-matplotlib-animations-the-ultimate-guide/\n",
    "# f = r\"/home/mx19/repositories/echocardiography/scripts/learning-pipeline/animation.gif\" \n",
    "# writergif = animation.PillowWriter(fps=30) \n",
    "# writergif2='imagemagick'\n",
    "# anim.save(f, dpi=80, writer=writergif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf219c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a060e041",
   "metadata": {},
   "source": [
    "## [**!WARNING!**] Cleanup temporal data directory \n",
    "Remove directory if a temporary was used.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "       Make sure you know which path you will remove as you do not like to remove important files.\n",
    "       shutil.rmtree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ae88d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T14:26:53.200135Z",
     "start_time": "2022-05-12T14:26:53.200123Z"
    }
   },
   "outputs": [],
   "source": [
    "# temporal_files_path = config['temporal_data_path']\n",
    "\n",
    "# shutil.rmtree(temporal_files_path)\n",
    "# print(f' {temporal_files_path} is empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea5420c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
