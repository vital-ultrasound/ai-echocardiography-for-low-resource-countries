{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef885fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:05.695513Z",
     "start_time": "2021-12-17T09:18:03.125501Z"
    }
   },
   "source": [
    "# Evaluation of learning pipeline\n",
    "\n",
    "**Author**: Miguel Xochicale [@mxochicale](https://github.com/mxochicale)     \n",
    "**Contributors**: Nhat Phung Tran Huy [@huynhatd13](https://github.com/huynhatd13); Hamideh Kerdegari [@hamidehkerdegari](https://github.com/hamidehkerdegari);  Alberto Gomez [@gomezalberto](https://github.com/)  \n",
    "\n",
    "Feb2022; March2022 \n",
    "\n",
    "\n",
    "## Summary\n",
    "This notebook presents a learning pipeline to classify 4 chamber view from echocardiography datasets.\n",
    "\n",
    "### How to run the notebook\n",
    "\n",
    "1. Go to echocardiography repository path: `$HOME/repositories/echocardiography/`\n",
    "2. Open echocardiography repo in pycharm and in the terminal type:\n",
    "    ```\n",
    "    git checkout master # or the branch\n",
    "    git pull # to bring a local branch up-to-date with its remote version\n",
    "    ```\n",
    "3. Launch Notebook server  \n",
    "    Go to you repository path: `cd $HOME/repositories/echocardiography/scripts/dataloaders` and type in the pycharm terminal:\n",
    "    ```\n",
    "    conda activate rt-ai-echo-VE \n",
    "    jupyter notebook\n",
    "    ```\n",
    "    which will open your web-browser.\n",
    "    \n",
    "    \n",
    "### References\n",
    "* \"Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback\". https://www.fda.gov/media/122535/download \n",
    "* Gomez A. et al. 2021 https://github.com/vital-ultrasound/lung/blob/main/multiclass_pytorch/datasets/LUSVideoDataset.py \n",
    "* Kerdegari H. et al. 2021 https://github.com/vital-ultrasound/lung/tree/main/multiclass_tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2aa0",
   "metadata": {},
   "source": [
    "# Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d3efa",
   "metadata": {},
   "source": [
    "## 1. Setting imports and datasets paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f345efe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T15:52:35.791032Z",
     "start_time": "2022-03-29T15:52:35.020079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.9.0\n",
      "Torchvision Version: 0.10.0a0\n",
      "FULL_PATH_FOR_YML_FILE: /home/mx19/repositories/echocardiography/scripts/config_files/users_paths_files/config_users_paths_files_username_mx19.yml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML #to be used with HTML(animation.ArtistAnimation().to_jshtml())\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from source.dataloaders.EchocardiographicVideoDataset import EchoClassesDataset\n",
    "#from source.models.learning_misc import train_loop, test_loop, basicVGGNet\n",
    "from source.helpers.various import concatenating_YAML_via_tags, plot_dataset_classes, split_train_validate_sets\n",
    "\n",
    "HOME_PATH = os.path.expanduser(f'~')\n",
    "USERNAME = os.path.split(HOME_PATH)[1]\n",
    "REPOSITORY_PATH='repositories/echocardiography'\n",
    "FULL_REPO_PATH = HOME_PATH+'/'+REPOSITORY_PATH\n",
    "FULL_REPO_MODEL_PATH = HOME_PATH +'/' + REPOSITORY_PATH + '/models'\n",
    "\n",
    "CONFIG_FILES_PATH= REPOSITORY_PATH + '/scripts/config_files/users_paths_files'\n",
    "YML_FILE =  'config_users_paths_files_username_' + USERNAME + '.yml'\n",
    "FULL_PATH_FOR_YML_FILE = os.path.join(HOME_PATH, CONFIG_FILES_PATH, YML_FILE)\n",
    "\n",
    "yaml.add_constructor('!join', concatenating_YAML_via_tags)  ## register the tag handler\n",
    "with open(FULL_PATH_FOR_YML_FILE, 'r') as yml:\n",
    "    config = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "    \n",
    "    \n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')    \n",
    "print(f'FULL_PATH_FOR_YML_FILE: {FULL_PATH_FOR_YML_FILE}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:08.264310Z",
     "start_time": "2021-12-17T09:18:08.250178Z"
    }
   },
   "source": [
    "## 2. Setting variables and loading datasets using pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5109aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T15:52:36.231446Z",
     "start_time": "2022-03-29T15:52:35.792515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "[ERROR] [EchoClassesDataset.__init__()] Error reading /home/mx19/datasets/vital-us/echocardiography/videos-echo-annotated/01NVb-003-074/T3/01NVb-003-074-3-4CV.json (empty). Removing from list\n",
      "[ERROR] [EchoClassesDataset.__init__()] Error reading /home/mx19/datasets/vital-us/echocardiography/videos-echo-annotated/01NVb-003-077/T1/01NVb-003-077-1-4CV.json (empty). Removing from list\n",
      "[ERROR] [EchoClassesDataset.__init__()] Error reading /home/mx19/datasets/vital-us/echocardiography/videos-echo-annotated/01NVb-003-077/T2/01NVb-003-077-2-4CV.json (empty). Removing from list\n",
      "[ERROR] [EchoClassesDataset.__init__()] Error reading /home/mx19/datasets/vital-us/echocardiography/videos-echo-annotated/01NVb-003-077/T3/01NVb-003-077-3-4CV.json (empty). Removing from list\n",
      "169 19 188\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # \"cuda:NN\" can also be used\n",
    "print(f'Device: {device}')\n",
    "\n",
    "pretransform_im_size = config['pretransform_im_size']\n",
    "\n",
    "# Defining transforms that apply to the entire dataset.\n",
    "# These transforms are not augmentation.\n",
    "if config['use_pretransform_image_size']:\n",
    "    pretransform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=pretransform_im_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    pretransform = None\n",
    "\n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_train_augmentation']:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),  # in degrees\n",
    "        transforms.RandomEqualize(p=0.5),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_validation_augmentation']:\n",
    "    val_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=5),  # in degrees\n",
    "    #transforms.RandomEqualize(p=0.5),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "\n",
    "\n",
    "train_dataset = EchoClassesDataset(\n",
    "    main_data_path=config['main_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_train'],\n",
    "    participant_path_json_list=config['participant_path_json_list_train'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    number_of_frames_per_segment_in_a_clip=config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=device,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    pretransform=pretransform,\n",
    "    transform=train_transform,\n",
    "    use_tmp_storage=True,\n",
    "    )\n",
    "\n",
    "validation_dataset = EchoClassesDataset(\n",
    "    main_data_path=config['main_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_validation'],\n",
    "    participant_path_json_list=config['participant_path_json_list_validation'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    number_of_frames_per_segment_in_a_clip=config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=device,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    pretransform=pretransform,\n",
    "    transform=val_transform,\n",
    "    use_tmp_storage=True,\n",
    "    )\n",
    "\n",
    "\n",
    "## Spliting train_dataset into train_set and test_set\n",
    "Ntdt = train_dataset.__len__()\n",
    "ntraining = 0.9\n",
    "\n",
    "Ntrain=round(Ntdt*ntraining)\n",
    "Ntest = round(Ntdt - (Ntdt*ntraining))\n",
    "print(Ntrain, Ntest, Ntrain+Ntest)\n",
    "train_set, test_set = torch.utils.data.random_split(train_dataset, [Ntrain, Ntest])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760906",
   "metadata": {},
   "source": [
    "## 3. Plotting Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6472a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T15:52:44.657685Z",
     "start_time": "2022-03-29T15:52:36.232576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BKGR': 86, '4CV': 83}\n",
      "{'BKGR': 13, '4CV': 13}\n",
      "{'BKGR': 8, '4CV': 11}\n",
      "Number of frames for training datasets 10140\n",
      "Number of frames for testing datasets 1140\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDYAAAG5CAYAAAB1DbTmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABIuElEQVR4nO3deZhcZZX48e/JwhIgEJIG2QOIgAGHJSqIgxHEBRf4KQooi46K4zICw4jguOAKLqMoikxEBWQJGFEQWWUdFdAEUMCAKGsgkCYCSdgS4Pz+eG+HSqe3JF1dfbu/n+fpp6vuem7Ve0+9deoukZlIkiRJkiTV0YhWByBJkiRJkrSiLGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKw0UcRkRHx0lbH0Z8i4qMR8UhELIyI8a2OpysRcW9EvKF6/JmIOLVFcbwvIi7vYfyUiJg9kDGtrIjYLSLuqt7/fVsdT910fs8j4vaImNK6iIYHc3HTYjguIs6sHm9axTKyRbEsjIgtehi/5HOhDiJi9Yj4dUQ8ERE/b3U8dTRY+gJqnoh4f0T8rtVx9KfBsu8Plv3HvrQ66+++dEsKG9Wb3/H3QkQ83fD8fVUHa3H1/PGI+ENE7NrFck6LiOciYsNOw5d00KrnGRG3RsSIhmFfiYjTmrBtE6v1jervZffneiJiNPBt4I2ZuWZmzutimqkRcWf1Hr2/i/FbRMRFEbEgIh6NiG80jNs2Iq6qkvnfI+L/rUicjTLza5n5oZVdzgqu+6zMfGPH8/74chURb4iImyLiyYh4ICLe0zBuh4iYGRFPVf93WJl1deNLwPer9/9XTVj+sJKZkzLzmlbH0R+WM0d3/D3eMP8+EXFLRMyvcsOVVc46pWH6RZ2WcUk/b8OwyMURsWpEfCciHoqIxyLi5GqZKywz769ieX5llrMS618zM++GJZ/zX1mZ5UVEW0ScXfUnHouIsxrGrRoRP6na6sMR8Z8rG38X9gPWB8Zn5rubsPxhpZV9gZ5ExGUR8aUuhu9Tta1R1fMpVc44utN03eaSbvq1T1a5c16VY/fvYr6IiLsj4q8Nwy5pyLuLq1zc8fyU6OLLZUS8LSL+WK1zXkScFREbN4x/fxXTpzrNNzuaUPDv/Ho0Sz+sp8d9PyIOrfp486vX6hud3/+IOCAiZlWv/T8i4l9XIh770v3PvnQ/Wtm+dEsKG9Wbv2ZmrgncD7y9YVhHh+PcavwE4GpgqUpnRKwBvAt4AnhfH1a7IXBAv21E/a0PrAbc3sM0fwY+BtzUeURErAJcAVwFvATYGOj4tW8UcAFwEbAucBhwZkS8rB/jr7WIeDlwNvDfwNrADsDMatwqlNfvTGAccDpwQTW8P21GN+9/1RkaFkd0Dadt7avlydENf+sAVJ2UM4CjKG17c+Bk4IXM/PeG5X6t0zLeMtDbOUisVC4GjgEmA9sBLwN2Aj7bzzHW3fnAw5Sctx7wrYZxxwFbVeNeDxwdEW/u5/VvBvwtM5/ramSzi2+DyRDf1tOAgyMiOg0/GDir4f0/FPhn9X9l/EuVS7eu1v39iPhCp2l2p7T5LSLilQCZ+ZaGPHwW8I2GPPzvnVcSEftR+ivfpfTJJwHPAr+LiHENk/4T+HREjF3J7RpKetz3gTHAEZTX9dXAnsB/dYyMiL2ArwMfANaivJ93NzHeWrEvPXgMmm3NzJb+AfcCb+g07DjgzIbnLwcSaGsYdgjwAHA4cFsv8yfwaeAuYFQ17CvAaT3E9SlgDvAQ8G/VMl5ajXsrcDMwv4rhuIb57q+mXVj97QpsSSkAzAMepXyQrNMwz6eBB4EFwJ3AntXwEZRO6z+qec8D1u1uPV1sw6rAidU2PFQ9XpXS+X2yYf6renmPfge8v9Oww4D/62b67arlRsOwy4Ev97CODwOzqtfgr8BOndtH4/sKTKziP6zatjnAUQ3LexUwo3qPHgG+3c16rwXeVT1+bbXMvavnbwBuqR6/H/hd9fi6aronq+3cH5gCzKZ8mZtbxfOBHrb37O5eD+CNVXtofP3uB97czfSnAT8AflO9fjcCW/bynv4DeAF4utqGVYFrgK8Cv6+Gv5TyYdrxvtwNfKRhGR3bfHTDNu8L7A38jdLJ+UzD9D2159UoHz7zgMeBPwHr97IN1wDHA3+kFDgv6FheNX4X4A/V8v4MTOk071Lb2sN61gV+WrWzx4BfNW5/V7mM0lanA+dWr91NlE5oy3Pu8v7Rhxzdadx+VPtNL8vtdhmdpjMXv7icrnLxDODdDc/fCzzQwzImUYrS/6Tkxs90fj94Mb92fF5eQzf7Gn3cdym55NcNz/8OnNfw/AFgh+pxUvLPYcBiYFH1+vy6oU3+F/CXKp5zgdW62d43VtOP7Gb8g5SjZTqefxmY1s20U1iOPF/N88Uq/sXVNnyQ8nnye+A71fvwFXpvm/dS9oW/VG3mx5Si2CWU9vpbYFzD9D3lv/dT8vkC4B7gfb1sQ0e8J1Wv9x1U+0Y1fu0qnjnV6/mVjte7q23tZV392hcYyD9g9er12b1h2DjgGar8T/kiu4DyQ9siYHLDtB3bMqqLZS/Z5sZ9pNM0+1XrGt8w7CdVWzqf8qty5+We1vk9oeGzDQjgPuDoTtOMAG4DvtTwPv8O+DXwhYbpZje2vU7LGA9cSMnff6Tse79rGP9dSl6YT/my+q/V8Dez9D7154Yc011/ZQLlx7bHq3b4f8CIatyGwC+Adsr+8Mme1tPFdmxLyZGPU77gvqO7fb8Pbeg/WTpP/qEv8zVr/8G+tH3pmvWlBzzxd7GhSzagYdhxvLjTrQKcQPmgH9UwzZXANygf7M9R7byd56+eJ+UXmZnAh6ph3RY2KMnsEcoX9DUoO05jZ3oKsH3VuF5RTbtvpyTRGOtLgb0oDb6NsjOfWI3bmpK4N2yYf8vq8RHADZSjIVYF/hc4p7v1dLEdX6rmX69a7x+oEkBf5m9YTled6Z8AP6N0qh6tGvf21bjtWbawcQXwy26W/25K8nkl5UP0pcBm3TTwzsn4nOo92p7yodQx7fXAwdXjNYFdeniNTqoef4aSKL7eMO671eP3s/QH7lKdiqpNPFfNM5qSkJ6ioaPZab13Uz7Eb6UksTN5MTEdCVzSafqL6KazRknG/6R8AI2idGK67Jj3tO9V7+H9lC8+o6rteCulwx3A66pt2qnTNn++mvbD1XtwNuWXhUmUTtYWfWjPH6F0iMYAI4GdgbG9xH9N1W469tNfNLSPjSiJfW/KfrpX9bytu23tYT2/oSTVcdV2vq5h+3tKxospHc3RlC9h9/S0nsH617mddN4Xu5h+i+p9/w7lF/A1u5mu22U0TGMuXno5XeXimcB7Gp6/r1re2l3MvxZVx5XSAVoLeHXn96NzTPS8r/Vp363axePVe7UB5cvSgw3jHuPFLxqN7/FpLPvF615KJ2xDSmdpFvDv3bxmnwcu48XO3p94cR8eV61r/Ybp9wNu7WZZU1iOPN9dW6d8njwH/Acl/6xOD22zYZtvoPR5NqJ0gG8CdqzmuYrqCyU95L/q/ZsPbF1NuwEwqZf4O+I9stru/Skd4I7PrF9R9ok1KG38j1Qd9662tYf19HtfYKD/gB8BpzY8/wgNhV7K0RtzKPvKr4HvNYzr2JYVLWyMrl7rt1TPx1Tv9d6UI5wfBVbpNM9p9FzY2KZa1+ZdxPRF4PqG9/l3lF/MH29oGz0VNqZRvpStQcktD7J0P+sgSvFjFCVnPUxVwOz8elTDeuqvHA+cUr1Go4F/raYbQcmhn6d839iC0j97U3fr6eI1/zul/7gKsAflC9jWfZm/i+X9CjihejySUhg5plrHbOD7dLMPYV/avrR96UF98dD3RDln+2nKm7xfVodyRcSmlA7z2Zn5CKXIcWgvy0vgc8DnI2LV3tYN/DQzb8vMJykv7IsLyrwmM2/NzBcy8y+UhPC6blec+ffMvCIzn83Mdsr51B3TP09pmC+PiNGZeW9m/qMa9xHgvzNzdmY+W8Wx33Icyvk+SjV9brXeL1I+VPvDxpRfHL5H6Vz+hhcP8bqD0un6VESMjog3UrZ3TDfL+hDlUMg/ZfH3zLyvj3F8MTOfzMxbKZXAA6vhi4GXRsSEzFyYmTd0M/+1vPhe7E758Ot4/rpqfF8tprzeizPzYkpxZ+tupt2Y8l68i1J0W53yaxiUD48nOk3/BCXBdef8zPxjtY+cRelcrIjTMvP2zHyu2o7fZOY/qvflWsqRN43ndy4GvpqZiymdlAmUD7AFmXk75deLV1TT9tSeF1M6MC/NzOczc2Zmzu9DvD9r2E8/R8kbIykdoosz8+JqP72C8qvD3t1ta1cLj4gNgLdQvjQ9Vr0mfW0TMzNzerXsb1O+SO7Sx3nr4D1RrlnQ8Xc1QJZrI0yhfCCeBzwa5ToJa67IOjAX9+YS4PAo15F4CfDJanhX+fZtwMOZ+T+Z+Uy1n97Yx/V0t6/1ad+t2sUCSm56HaXY8GBEbFM9/7/MfKHPW12+ED6Umf+kdOR26Ga6jSm/3F1NOW3yfyifVRMouRaWzre95drlyfM9eSgzT6ryz9O9tM0OJ2XmI5n5IOUX5xsz8+aqTf6SUuSA3vPfC8B2EbF6Zs6pcnVv5lIKLYsz81zKEU1vjYj1KTnyiOqzeC6lqNl46u9S29rDOprRFxhopwPvjojVq+eHVMM6HEo5Be95yheXA2Mlr4nTofqseZRS7AN4J+WUkcspX+hGUb5gLY8J1f85XYyb0zC+I4ZbqvV9uqeFVrnjXcDnq/ftNpZ+ncjMMzNzXtVu/oeSn7vd13rpryymFPE2q9rw/2VmUooAbZn5pcxcVOWpH9H3U9d3oeSRE6r5r6K81svd/iLiA5TTCjtOlVuf8mVuv2o7dqDs492damhf2r70sO9LD+bCxnlZztlen3K4284N4w4GZlUJFErje29vHw7VDnI/5ZCrnmxI+eWuw1KJISJeHRFXR0R7RDwB/Dudknun6deLiGkR8WBEzKdUFCdUMf2dUn07DphbTddxMdTNgF92fHGg/Cr1POU16YsNO8V+XzWsPzxNqbpekpmLKIl4PLBt1fj2pXyAPkyptJ9HqTZ3ZRNKdXdFdH6fOrbvg5TDvO+IiD9FxNu6mf964GVV52wHyrUBNqk6va+i/GrWV/Ny6fMon+LFjnNnT1O+sP0tMxdSrjfQkSgWAp3PUR1L+VLQnYf7uN7eNL6eRMRbIuKGiPhn1Qb3Zum2Pi9fvMBgR4f1kYbxTzfE0lN7/hnli860KBdB/EYfO3ud3//RVXybUTqXjzes77WUjk2X29qNTYB/ZuZjfZi229iqL2yz6b/9bzA4LzPXafh7fceIzLwhM9+TmW2UD+/dKefALi9zce++Sjkd5xbKkSC/onRu5nYxbX/m2o59bXn23WspRa/dq8fXUDq9y9vxhb7nvKeBezPzx1Vnalq1LbtRci0snW97y7XLk+d70jnXdts2G3TOrT3l2i7zX9Vx3Z+yr8yJiN9UxaXePFh9EezQ0YY3o7SFOQ3r+l/KkRtdbmsPmtEXGFCZ+TvKr637RLmzzyspBQwiYhPKj3Id1ym6gNJJX95iQ5eq/a6N8qszlCLKedUXjmcpp6P09iNgZ49W/zfoYtwGDeMbfR74aFVo7U4bpdDSU34/KspFM5+o2tXa9Jzfe+qvfJNy1MPlUS6mekw1fDNgw077ymdYvtz+QC5dlL2PUtjvsyh30ziBcrRNx2va0ac6qSpAPkr5Yrd3F4sA+9L2pe1LD+rCBgDVjvwR4Liq4gOlAr5FlKtMP0zZ0SdQqkG9+Sylg93d0QNQqtCbNDzftNP4synnBW6SmWtTDm/ruFhUsqzjq+GvyMyxlArYkotLZebZmflaSgNKyoWCoLyZb+n05WG1LL/WdLWezh6qltm4HQ/1Yb6++EtPMWTmXzLzdZk5PjPfRDm874/dTP4A5RCtFdH5fXqoWv9dmXkgpXP1dWB6lAvOdo7zKcphiIdTrtWyiPLl4D+BfzR8wPS3nl6/24FXRCx1AbJX0PPFBfvLkpiiHNn0C0rRav2q0HgxDW13OXXbnqsvHF/MzJcDr6H8snxIH5bZ+f3v+MXqAUoFunFda2TmCV1tay8xrxsR6/RlA7uLLcoFlTam//a/2sjMP1E61NutwOzm4l5k+bX/E5m5UWZuQTlMdGZ2fUeT/sy1i4FHl3Pf7Shs/Gv1uONXvp4KG315fXvSba6tOllzgH9pGPwvDHCurfTYNpdTj/kvMy/LzL0ondM7KL9Q92ajTp9JHW34AcpRARMa1jU2Myc1TNvX97Df+wItcgZlHzgYuDzLkcVUz0cAv676rndTCht9+azri30oh7T/McodS/YADmroK+8H7F192eyrOylfJN7dOLD6THsX5YjppWTmHZSc/5kelttexdplfo9y549PU47aG1f1P56gm/zeW38lyy/fR1U58u3Af0bEnpQ2d0+nfWWtzNy7q/V04SHKF/jG71ObUg7t75MoFyv+EeUi3bd2DK/y0+w+xNDBvvTS7EsPw770oC9swJIkeRnlauUdF4B7FaUquAOlw3w2fahEZ7mFzK29THse8P6IeHlEjAG+0Gn8WpTK0zMR8SrKxdo6tFMO89yi0/QLgccjYiPKBcAAiIitI2KPquE/Q6nKdXRITwG+GhGbVdO2RcQ+Payns3OAz1bzTaBU0c/sYfqlRMQqEbEaZecbHRGrNSTvM4FdotxmaSTll85HKZVDIuIV1fRjIuK/KB2o07pZ1anAf0XEzlG8tGOb++Bz1TomUS7Oc261/oMioq2q7j1eTdvdrQuvBT7Bix3razo978oj9Pza9+anwAei3DJ3DOUD/KKG9T8PfDLKrQg/UQ2/aiXWtyJWoRz62Q48FxFvoRzSvaK6bc8R8fqI2L5qS/MpSbUvt5o8qGE//RIwvfpCdybw9oh4U0SMrNrilGi4PV1fZOYcyqH+J0fEuCinVu3ex9l3joh3Rjk88AhK57+7wziHjIh4bUR8OCLWq55vA7yDFdt2czE95+KI2CgiNqxy5y6Uw0g7v04dLgJeEhFHVLllrYh4dR/D6HJfW85991rKL9arZ+ZsyukUb6Yc7XdzN/OsbK79JTAuym0VR0a5w8NGlIudQfkS+tlq/96GcurraSuxvhXVbdtcAd3mv4hYPyLeUX05ebZaZ19y7XqUz6TREfFuygUTL65y5OXA/0TE2IgYERFbRkS3p4T1oN/7Ai1yBuWCiR9m6dMrDqGchrZDw9+7KKf0jG+YbtXqPev467GvHhHrRsT7KBc+/HqWW0YfTLnw4NYN63oZ5Ytyn0+TqI7S+S/KPvLeiFg9ypEYp1J+/f5ON7N+kfI+rNPNcp+nFD+Oq963l7N0v3wtSuGjHRgVEZ9n6V/fHwEmNrw2PfZXotyu9qUREZQ89Xz190dgfkR8utq2kRGxXVR3kOliPZ3dSLn45dHVvjGFUjiZ1s30S4mIPShH8LwrM7v68e+nwH9EOaJrHKUvcVEX04F9afvS9qXrUdiofJNyCsmHgQuynFf9cMcf5erJb4uIdXtcSvFZXjwHcRmZeQnlqvVXUQ5d67wTfAz4UkQsoHRQz2uY9ymqK8RGOWxnF0qC34lSbf4NJZl3WJUXL476MKXz0FHl/i7l18jLq3XdQLkdVHfr6ewrlHOh/kIp5txUDeuryymd+9cAU6vHu1frv5Pyi9IplIu+7UO5EvSiat6OC2TNpdy+aq8sh0IuIzN/Xm3L2ZRDxH5FD+9PJ9dS3qMrgW9l5uXV8DcDt0fEQsrreEBmPtPDMtbixUPlOj/vynHA6dVr/54+xrpEZv6E0vm5kXLY17NU58ZXr+G+lE7Q45Q7Qezb8NoOiMxcUMV0HuU9fi+lPa6obtsz5dz36ZREPIvyHvTli9/PKF9CHqb88tXxGj5AaZOfoXyYPED5orAiOe9gyodDx7VjjujjfBdQDvl+rFrGO7Ob8w9rav+IWNjpbz1Km30HcGu1/11K+XL5jeVdgbl4iW5zMaXQ/wdK5/p04JiGPLiUap/ei9Lxfphyp7DXdzVtF7rc11iOfTcz/0b5Iv1/1fP5lF+tf9/NESZQ7rbx8uq1/VUfY21c5z8p7fG/KO/7McA+Db8gfoFy+PZ9VezfzMxLl3c9/aCntrlcesl/Iyinhz5EOWXhdZT9qDc3Us5hf5TS3vervkBD+axahXIXhsco7aGrUxd6i7sZfYEBl5n3UvbJNag+M6u8MBH4QWPfNTMvpMTdWGxYSNnHO/726GZVf65y7N8p11c4MjM/X407FDi507oepvTZlut0lCzXVDmYcjHGRynv8+rAbg1toPM891ByxjK/7jf4BOXw+ocpueWnDeMuo3wR+htl33yGpQ95/3n1f15E3NSH/spWlDsHLaScNnFylms0PU/JhztQLkr4KKVAsHZX6+liOxdR8stbqnlPBg6pfpDti89V67q44XP0kobxX6Zc8PhvlPx6M2UfWYZ9afvS2Jcud62Q6iYiJvLilXGf62VyDUERcQ3lys2ntjqWziLiOMrFmw5qdSzSyhrM+5qaLyLeT7mj3GtbHUtn9gWkFef+o8H8+b4ifek6HbEhSZIkSZK0FAsb0hAVEf/axakCC6tDCmuhu/ijXFisduuRNDRFxO3d5JD3tTq2voiIU7qJ/5Q6rkeS+oN96cG3nh5j8FQUSZIkSZJUVx6xIUmSJEmSamtUqwPoiwkTJuTEiRNbHYYkLWXmzJmPZmZbq+MYCOZhSYOVuViSWmsw5OFaFDYmTpzIjBkzWh2GJC0lIu5rdQwDxTwsabAyF0tSaw2GPOypKJIkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmqrFtfYkNR6ixcvZvbs2TzzzDOtDmXArbbaamy88caMHj261aEMKrYJ24QkScON/Z/B2f+xsCGpT2bPns1aa63FxIkTiYhWhzNgMpN58+Yxe/ZsNt9881aHM6jYJmwTkiQNN/Z/Bmf/x1NRJPXJM888w/jx44dVAgeICMaPHz8sq/K9sU3YJiRJGm7s/wzO/o+FDUl9NtwSeIfhut19MVxfm+G63ZIkafj2AwbzdlvYkCRJkiRJtWVhQ9KgtPfee/P444/3OM2aa67Z5fD3v//9TJ8+vQlRqdVsF5IkaTix79M3XjxU0qCSmWQmF198catD0SBiu5AkScOJfZ/l4xEbkpri05/+NCeffPKS58cddxxf/OIX2XPPPdlpp53YfvvtueCCCwC499572XbbbfnYxz7GTjvtxAMPPMDEiRN59NFHAdh3333ZeeedmTRpElOnTl1qPUcddRQ77bQTe+65J+3t7cvEMXPmTF73utex884786Y3vYk5c+Y0cavVG9uFJEkaTuz7DJCOStBg/tt5551TUmv99a9/Xa7pb7rpptx9992XPN92223zvvvuyyeeeCIzM9vb23PLLbfMF154Ie+5556MiLz++uuXTL/ZZptle3t7ZmbOmzcvMzOfeuqpnDRpUj766KOZmQnkmWeemZmZX/ziF/PjH/94ZmYeeuih+fOf/zwXLVqUu+66a86dOzczM6dNm5Yf+MAHVmTzu9x+YEYOghw5EH9d5eHlbROZQ6tdrMj2S+p/wz0XSxpY9okHZ5/YU1EkNcWOO+7I3Llzeeihh2hvb2fcuHFssMEGHHnkkVx33XWMGDGCBx98kEceeQSAzTbbjF122aXLZX3ve9/jl7/8JQAPPPAAd911F+PHj2fEiBHsv//+ABx00EG8853vXGq+O++8k9tuu4299toLgOeff54NNtigWZusPrBdSJKk4cS+z8CwsCGpafbbbz+mT5/Oww8/zAEHHMBZZ51Fe3s7M2fOZPTo0UycOHHJvbDXWGONLpdxzTXX8Nvf/pbrr7+eMWPGMGXKlG7vn935FlSZyaRJk7j++uv7d8O0UmwXkiRpOLHv03xeY0NS0xxwwAFMmzaN6dOns99++/HEE0+w3nrrMXr0aK6++mruu+++XpfxxBNPMG7cOMaMGcMdd9zBDTfcsGTcCy+8sORKz2effTavfe1rl5p36623pr29fUkSX7x4Mbfffns/bqFWhO1CkiQNJ/Z9ms8jNiQ1zaRJk1iwYAEbbbQRG2ywAe973/t4+9vfzuTJk9lhhx3YZpttel3Gm9/8Zk455RRe8YpXsPXWWy91aN4aa6zB7bffzs4778zaa6/Nueeeu9S8q6yyCtOnT+eTn/wkTzzxBM899xxHHHEEkyZN6vdtVd/ZLiRJ0nBi36f5olzrY3CbPHlyzpgxY4Xm3flTZ/RzNIPXzG8e0uoQNITNmjWLbbfdttVhtExX2x8RMzNzcotCGlBd5WHbxPDefjXf/V/avtUhDJhNP3/rCs873HNxX9knlpZfV3n4ib1O5GWbvaQF0TTfqhv2XugYrH1iT0WRJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1daoVgcgqZ76+7Zxfbk128iRI9l+++3JTEaOHMn3v/99XvOa13Dvvffytre9jdtuuw2AH/3oR/zwhz/kyiuvZNy4cXz7299m6tSpjB49mhEjRrDnnnvy9a9/ndGjRzNx4kTWWmstIoJx48ZxxhlnsNlmm/Xrtg0XtglJkjTcvOY7M/t1eX84cudepxmzySvYbputlvR/vvOVz7DrK3fk3gce5J2HfpybrvoVAD8+azo/OuNcLjn3VMatszbf/d/T+fFZ0xk9ehQjInj9a3fhq/99JKNHj+Zlr34jY9dZt7b9H4/YkFQbq6++Orfccgt//vOfOf744zn22GOXmeZnP/sZJ510Epdffjnjxo3jlFNO4fLLL+eGG27g1ltv5U9/+hPrrbceTz/99JJ5rr76av7yl78wZcoUvvKVrwzkJmkl2SYkSdJws/pqq/LHK37Bn357Pl8+9gg+d8J3l5nmrOkX8sOfnsVF50xl3Dpr86MzzuW31/2B6359FjOv/CW/v/hc2iasy9PPPLtknjr3fyxsSKql+fPnM27cuKWGnXfeeZxwwglcfvnlTJgwAYCvfvWr/PCHP2SdddYBYJVVVuGYY45h7Nixyyxz11135cEHH2x67GoO24QkSRpu5i9YyLi1l+7DTL/wUr71gx9z0dk/YsK6pW90wvem8r3jP8c61bSrrDKaT33iQ4xda81lllnH/o+nokiqjaeffpoddtiBZ555hjlz5nDVVVctGXfffffxiU98gptvvpmXvOQlACxYsICFCxey+eab92n5l156Kfvuu28zQleT2CYkSdJw8/Qzz/Kqvd7FM88u4uG57Vx63o+XjLt/9kMc+dmvccNlP+cl65UfdRYsfJInn3qazTfduE/Lr2P/xyM2JNVGx2kHd9xxB5deeimHHHIImQlAW1sbm266Keedd96S6TOTiFjy/LLLLmOHHXZg4sSJ/OEPf1gy/PWvfz3rrbcev/3tb3nve987cBuklWabkCRJw03HqSh/ue7XXHjmKXzw8M8s6f9MGL8um2z0En7x68uWTF/6Py/Of8U1v+dVe72Ll736jVz/p5uXDK9z/8fChqRa2nXXXXn00Udpb28HYMyYMVxyySWccsopnHXWWQCMHTuWNdZYg3vuuQeAN73pTdxyyy1st912LFq0aMmyrr76au677z4mTZrE5z//+YHfGPUL24QkSRpudpm8A/P++Rjt8/4JwJjVV+OCM0/hRz87j3POvwiAsWutyZjVV+ee+2cDsNeU3fjjFb9g0tYvZdHixUuWVef+j6eiSOrS/V/afqnnz+11Is8+9ELT1vfsQ7f3PlG+sGS6O/9+N88vXsSazz7MY488TD73LGMXz+WC07/HG/f7AGvHk+w1ZTc+9dH385F/O5ifnfxN1ll7LJnJU0/MY9Gj9/DsQ23k8yWZr7766px44olsv/32fPazn2Xddddt2raqOe644w6ef/55xo8fz1NPPQWUozYuvfRSpkyZwoQJE3jTm97Esccey0c/+lGmTZvGOuusQ2byzDPPLLM824QkSRrs7vz73Tz//AuMH7cOTz39MABt49flwrNO4Y37fYAJ645jrym7cfQnPswnj/3yUn3iZ55dtMzy6tr/sbAhaYX05VZU/a3jfEIoh9SdeuJXGTly5FLTbL7pxvzipyex7yEfY9qPTuSwQ/fnqWee5l/f9l5WXXU0a44Zw66v3JEdttt2meVvsMEGHHjggfzgBz/gc5/73IBs01DSl9uz9reOa2xAaROnn376sm1i88258MIL2XvvvTn//PP56Ec/ylNPPcWrX/1qVl11VdZcc0122203dtxxx2WWb5uQJEk9sU88OETHuTiD2eTJk3PGjBkrNO/Onzqjn6MZvFrxpUJDV+cjNp7Y60RettlLWhRNc6264aRep5k1axbbbrt04o+ImZk5uVlxDSZd5eGuXpPhZLhvv5qvcx4eyjb9/K0rPO9wz8V9ZZ9YWn5d5WH7xIOzT+w1NiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1NarVAUiqp0dOPaBfl7f+h6b1edrnn3+e17xlfzZ8yXr88oyTAfjOKT/lp2efz6hRIxk5YgSHf+RQ7rl/Ns8uWsRXjj1yybx/vu0ODvn4p/jztb/u1/jV/7emXJ7bPz7//PNMnjyZjTbaiIsuugiAb33rW5x66qmMGjWKkSNHctRRR3H33Xfz7LPPcvzxxy+Z95ZbbuHAAw9k1qxZ/Rq/JEka+uwTDw5NPWIjIo6MiNsj4raIOCciVouIdSPiioi4q/o/rpkxSBp6vn/qmWy91RZLnv/ojHO58rrr+d1vzuGmq37Fb88/nUzYf5+9mX7hZUvN+/MLL2H/fd860CGryb773e8udU/1U045hSuuuII//vGP3HbbbVx33XVkJgceeCDnnnvuUvNOmzaN9773vQMdsiRJ0kqxT/yiphU2ImIj4JPA5MzcDhgJHAAcA1yZmVsBV1bPJalPZj/0MJdceR0fOPBdS4Z9/aQf8d2vfZaxa60JwNpj1+Lg9+zDy166OeuMXYs/3vSXJdNO//VlvGeftwx43Gqe2bNn85vf/IYPfehDS4Z97Wtf4+STT2bs2LEArL322hx66KFsvfXWrLPOOtx4441Lpj3vvPM44ID+/bVFkiSpmewTL63Z19gYBaweEaOAMcBDwD7A6dX404F9mxyDpCHkU1/4Ol/77H8yYkQAsGDhkyx88im2nLhpl9O/Z9+38PMLLgHgxpl/Zvy4tXnpFpsNWLxqviOOOIJvfOMbjBhRPtIWLFjAggUL2HLLLbuc/sADD2TatHKY5w033MD48ePZaqutBixeScNLRPwkIuZGxG0Nw74ZEXdExF8i4pcRsU4LQ5RUQ/aJl9a0wkZmPgh8C7gfmAM8kZmXA+tn5pxqmjnAel3NHxGHRcSMiJjR3t7erDAl1cjFV1xD24R12ekVk5YMy0wiup/n3e94C+f/5nJeeOEFfn7BJbxnn70HIFINlIsuuoj11luPnXfeecmw0ia6bxQHHHAA06dP54UXXmDatGkceOCBAxGqpOHrNODNnYZdAWyXma8A/gYcO9BBSaov+8TLatrFQ6trZ+wDbA48Dvw8Ig7q6/yZORWYCjB58uRsRoyS6uUPM27mN5dfw6VX/R/PPvss8xc8ySc/82XGrL46d9/3AFtstsky82yy0QZstslGXHf9DH558RVce+FZLYhczfL73/+eCy+8kIsvvphnnnmG+fPn87GPfYw11liDu+++my222GKZeTbZZBMmTpzItddeyy9+8Quuv/76FkQuabjIzOsiYmKnYZc3PL0B2G9Ag5JUa/aJl9XMU1HeANyTme2ZuRg4H3gN8EhEbABQ/Z/bxBgkDSFfOfZI/jHzSv524+WccfI3mbLbqzjtpK9z9Cc+zBH//VXmL1gIwPwFCzn1zJ8vmW//ffbm6OO+zhYTN2HjDV/SqvDVBMcffzyzZ8/m3nvvZdq0aeyxxx6ceeaZHHvssXz84x9n/vz5AMyfP5+pU6cume/AAw/kyCOPZMstt2TjjTduVfiSBPBvwCXdjfQoZkmd2SdeVjNv93o/sEtEjAGeBvYEZgBPAocCJ1T/L2hiDJKaZHluRdVshx26Pwufeord9j6A0aNHMXrUKA7/yKFLxr/z7W/kqC+cwHe+7JG+zbQ8t2dtto9+9KMsXLiQV77ylYwePZrRo0dz1FFHLRn/7ne/m8MPP5yTTjqphVFKGu4i4r+B54Bufzr1KGZpcLNPPDg0rbCRmTdGxHTgJkrCvpmSlNcEzouID1KKH+9uVgyShq7XveZVvO41rwIgIjjqY//GUR/7ty6nbRu/Lgvvu2UAoxs8IuInwNuAudUdqoiIdYFzgYnAvcB7MvOxVsXYX6ZMmcKUKVOA0iaOPvpojj766C6nbWtrY/HixQMYnSQtLSIOpeTnPTPTgoWkFWKfuGjqXVEy8wuZuU1mbpeZB2fms5k5LzP3zMytqv//bGYMkjTMncayF63zttuS1EIR8Wbg08A7MvOpVscjSXXX7Nu9SpJaKDOvAzoXkL3ttiQNkIg4B7ge2DoiZldHLX8fWAu4IiJuiYhTWhqkJNVcM6+xIWlIyV5vozlUDcEjhJe67XZEdHvbbeAwgE037fqe6LYJSepZZnZ1T+kfD3ggkvqJfeLByCM2JPXJyPkP8PiTiwZ1QmuGzGTevHmsttpqrQ5lwGXm1MycnJmT29ralhm/2mqrMW/ePNuEJEkaNuwTD87+j0dsSOqTMTf/iH/yYdrHbgIMrQr1qCd6rvGuttpqQ+2WoI9ExAbV0RorfNvtjTfemNmzZzMcbz84BNuEJEnqA/vEg7P/Y2FDUp+MWLSANW/8dqvDaIrBdJvSAXIh/XDb7dGjR7P55pv3Z1ySJEmDmn3iwclTUSRpCOvmonUnAHtFxF3AXtVzSZIkqZY8YkOShrBuLloHsOeABiJJkiQ1iUdsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZLUJBHxk4iYGxG3NQxbNyKuiIi7qv/jWhmjJNWdhQ1JkiSpeU4D3txp2DHAlZm5FXBl9VyStIIsbEiSJElNkpnXAf/sNHgf4PTq8enAvgMZkyQNNRY2JEmSpIG1fmbOAaj+r9fdhBFxWETMiIgZ7e3tAxagJNWJhQ1JkiRpkMrMqZk5OTMnt7W1tTocSRqULGxIkiRJA+uRiNgAoPo/t8XxSFKtWdiQJEmSBtaFwKHV40OBC1oYiyTVnoUNSZIkqUki4hzgemDriJgdER8ETgD2ioi7gL2q55KkFTSq1QFIkiRJQ1VmHtjNqD0HNBBJGsI8YkOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JGmYiogjI+L2iLgtIs6JiNVaHZMkSZK0vCxsSNIwFBEbAZ8EJmfmdsBI4IDWRiVJkiQtPwsbkjR8jQJWj4hRwBjgoRbHI0mSJC23Ua0OQJI08DLzwYj4FnA/8DRweWZe3jhNRBwGHAaw6aabrvC6dv7UGSsRab3M/OYhrQ5BkiRp2PGIDUkahiJiHLAPsDmwIbBGRBzUOE1mTs3MyZk5ua2trRVhSpIkSb2ysCFJw9MbgHsysz0zFwPnA69pcUySJEnScrOwIUnD0/3ALhExJiIC2BOY1eKYJEmSpOVmYUOShqHMvBGYDtwE3Er5PJja0qAkSZKkFeDFQyVpmMrMLwBfaHUckiRJ0srwiA1JkiRJklRbFjYkSZIkSVJtNbWwERHrRMT0iLgjImZFxK4RsW5EXBERd1X/xzUzBkmSJEmSNHQ1+4iN7wKXZuY2wL9Qrrh/DHBlZm4FXFk9lyRJkiRJWm5NK2xExFhgd+DHAJm5KDMfB/YBTq8mOx3Yt1kxSJIkSZKkoa2ZR2xsAbQDP42ImyPi1IhYA1g/M+cAVP/X62rmiDgsImZExIz29vYmhilJkiRJkuqqmYWNUcBOwA8zc0fgSZbjtJPMnJqZkzNzcltbW7NilCRJkiRJNdbMwsZsYHZm3lg9n04pdDwSERsAVP/nNjEGSZIkSZI0hDWtsJGZDwMPRMTW1aA9gb8CFwKHVsMOBS5oVgySJEmSJGloG9Xk5f8HcFZErALcDXyAUkw5LyI+CNwPvLvJMUiSJEmSpCGqqYWNzLwFmNzFqD2buV5JkiRJkjQ8NPMaG5IkSZIkSU1lYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSWqBiDgyIm6PiNsi4pyIWK3VMUlSHVnYkCRJkgZYRGwEfBKYnJnbASOBA1oblSTVk4UNSZIkqTVGAatHxChgDPBQi+ORpFqysCFJkiQNsMx8EPgWcD8wB3giMy/vPF1EHBYRMyJiRnt7+0CHKUm1YGFDkiRJGmARMQ7YB9gc2BBYIyIO6jxdZk7NzMmZObmtrW2gw5SkWrCwIUmSJA28NwD3ZGZ7Zi4Gzgde0+KYJKmWLGxIkiRJA+9+YJeIGBMRAewJzGpxTJJUSxY2JEmSpAGWmTcC04GbgFsp/fKpLQ1KkmpqVKsDkCRJkoajzPwC8IVWxyFJdecRG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaqtXgsbEbF+RPw4Ii6pnr88Ij7Y/NAkSR2akYsjYp2ImB4Rd0TErIjYtX+ilaShxz6xJA1efTli4zTgMmDD6vnfgCOaFI8kqWun0f+5+LvApZm5DfAvwKyVXJ4kDWWnYZ9YkgalvhQ2JmTmecALAJn5HPB8U6OSJHXWr7k4IsYCuwM/rpa3KDMf74c4JWmosk8sSYNUXwobT0bEeCABImIX4ImmRiVJ6qy/c/EWQDvw04i4OSJOjYg1GieIiMMiYkZEzGhvb1+JVUnSkGCfWJIGqb4UNv4TuBDYMiJ+D5wB/EdTo5IkddbfuXgUsBPww8zcEXgSOKZxgsycmpmTM3NyW1vbSqxKkoYE+8SSNEiN6m2CzLwpIl4HbA0EcGdmLm56ZJKkJZqQi2cDszPzxur5dDoVNiRJL7JPLEmDV6+FjYg4pNOgnSKCzDyjSTFJkjrp71ycmQ9HxAMRsXVm3gnsCfx1pQOVpCHKPrEkDV69FjaAVzY8Xo3S+b2JcvidJGlgNCMX/wdwVkSsAtwNfGAlliVJQ519YkkapPpyKspS5w5GxNrAz5oWkSRpGc3IxZl5CzB5ZZYhScOFfWJJGrz6cvHQzp4CturvQCRJy8VcLEmtZR6WpEGiL9fY+DXVba0ohZCXA+c1MyhJ0tLMxZLUWuZhSRq8+nKNjW81PH4OuC8zZzcpHklS18zFktRa5mFJGqT6co2NawciEElS98zFktRa5mFJGry6LWxExAJePNxuqVFAZubYpkUlSQLMxZLUauZhSRr8ui1sZOZaAxmIJGlZ5mJJai3zsCQNfn25xgYAEbEe5Z7dAGTm/U2JSJLULXOxJLWWeViSBp9eb/caEe+IiLuAe4BrgXuBS5oclySpgblYklrLPCxJg1evhQ3gy8AuwN8yc3NgT+D3TY1KktSZuViSWss8LEmDVF8KG4szcx4wIiJGZObVwA7NDUuS1Im5WJJayzwsSYNUX66x8XhErAn8H3BWRMyl3LtbkjRwzMWS1FrmYUkapPpyxMZ1wDrA4cClwD+AtzcxJknSsszFktRa5mFJGqT6UtgI4DLgGmBN4NzqMDxJ0sAxF0tSa5mHJWmQ6rWwkZlfzMxJwMeBDYFrI+K3TY9MkrSEuViSWss8LEmDV1+O2OgwF3gYmAes15xwJEm9MBdLUmuZhyVpkOm1sBERH42Ia4ArgQnAhzPzFc0OTJL0InOxJLWWeViSBq++3BVlM+CIzLylybFIkrpnLpak1jIPS9Ig1WthIzOPGYhAJEndMxdLUmuZhyVp8Fqea2xIkiRJkiQNKhY2JEmSJElSbTW9sBERIyPi5oi4qHq+bkRcERF3Vf/HNTsGSZIkSZI0NA3EERuHA7Manh8DXJmZW1GuKu35ipIkSRp2ImKdiJgeEXdExKyI2LXVMUlSHTW1sBERGwNvBU5tGLwPcHr1+HRg32bGIEmSJA1S3wUuzcxtgH9h6R8DJUl91OwjNk4EjgZeaBi2fmbOAaj+r9fVjBFxWETMiIgZ7e3tTQ5TkiRJGjgRMRbYHfgxQGYuyszHWxqUJNVU0wobEfE2YG5mzlyR+TNzamZOzszJbW1t/RydJEmS1FJbAO3AT6vr0Z0aEWu0OihJqqNmHrGxG/COiLgXmAbsERFnAo9ExAYA1f+5TYxBkiRJGoxGATsBP8zMHYEn6eLacx7FLEm9a1phIzOPzcyNM3MicABwVWYeBFwIHFpNdihwQbNikCRJkgap2cDszLyxej6dUuhYikcxS1LvBuKuKJ2dAOwVEXcBe1XPJUmSpGEjMx8GHoiIratBewJ/bWFIklRbowZiJZl5DXBN9XgeJXFLkiRJw9l/AGdFxCrA3cAHWhyPJNXSgBQ2JEmSJC0tM28BJrc6Dkmqu1aciiJJkiRJktQvLGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJA1jETEyIm6OiItaHYskSZK0IixsSNLwdjgwq9VBSJIkSSvKwoYkDVMRsTHwVuDUVsciSZIkrSgLG5I0fJ0IHA280NXIiDgsImZExIz29vYBDUySJEnqKwsbkjQMRcTbgLmZObO7aTJzamZOzszJbW1tAxidJEmS1HcWNiRpeNoNeEdE3AtMA/aIiDNbG5IkSZK0/CxsSNIwlJnHZubGmTkROAC4KjMPanFYkiRJ0nKzsCFJkiRJkmprVKsDkCS1VmZeA1zT4jAkSZKkFeIRG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJktQiETEyIm6OiItaHYsk1ZWFDUmSJKl1DgdmtToISaozCxuSJElSC0TExsBbgVNbHYsk1ZmFDUmSJKk1TgSOBl7oboKIOCwiZkTEjPb29gELTJLqxMKGJEmSNMAi4m3A3Myc2dN0mTk1Mydn5uS2trYBik6S6sXChiRJkjTwdgPeERH3AtOAPSLizNaGJEn1ZGFDkiRJGmCZeWxmbpyZE4EDgKsy86AWhyVJtWRhQ5IkSZIk1daoVgcgSZIkDWeZeQ1wTYvDkKTa8ogNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm01rbAREZtExNURMSsibo+Iw6vh60bEFRFxV/V/XLNikCRJkiRJQ1szj9h4DjgqM7cFdgE+HhEvB44BrszMrYArq+eSJEmSJEnLrWmFjcyck5k3VY8XALOAjYB9gNOryU4H9m1WDJIkSZIkaWgbkGtsRMREYEfgRmD9zJwDpfgBrDcQMUiSJEmSpKGn6YWNiFgT+AVwRGbOX475DouIGRExo729vXkBSpIkSZKk2mpqYSMiRlOKGmdl5vnV4EciYoNq/AbA3K7mzcypmTk5Mye3tbU1M0xJkiRJklRTzbwrSgA/BmZl5rcbRl0IHFo9PhS4oFkxSJIkSZKkoW1UE5e9G3AwcGtE3FIN+wxwAnBeRHwQuB94dxNjkCRJkiRJQ1jTChuZ+Tsguhm9Z7PWK0mSJEmSho8BuSuKJEmSJElSM1jYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkoahiNgkIq6OiFkRcXtEHN7qmCRJkqQVMarVAUiSWuI54KjMvCki1gJmRsQVmfnXVgcmSZIkLQ+P2JCkYSgz52TmTdXjBcAsYKPWRiVJkiQtPwsbkjTMRcREYEfgxk7DD4uIGRExo729vSWxSZIkSb2xsCFJw1hErAn8AjgiM+c3jsvMqZk5OTMnt7W1tSZASZIkqRcWNiRpmIqI0ZSixlmZeX6r45EkSZJWhIUNSRqGIiKAHwOzMvPbrY5HkiRJWlEWNiRpeNoNOBjYIyJuqf72bnVQkiRJ0vLydq+SNAxl5u+AaHUckiRJ0sryiA1JkiRJklRbFjYkSZKkARYRm0TE1RExKyJuj4jDWx2TJNWVp6JIkiRJA+854KjMvCki1gJmRsQVmfnXVgcmSXXjERuSJEnSAMvMOZl5U/V4ATAL2Ki1UUlSPVnYkCRJklooIiYCOwI3djHusIiYEREz2tvbBzw2SaoDCxuSJElSi0TEmsAvgCMyc37n8Zk5NTMnZ+bktra2gQ9QkmrAwoYkSZLUAhExmlLUOCszz291PJJUVxY2JEmSpAEWEQH8GJiVmd9udTySVGcWNiRJkqSBtxtwMLBHRNxS/e3d6qAkqY683askSZI0wDLzd0C0Og5JGgo8YkOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm21pLAREW+OiDsj4u8RcUwrYpCk4c5cLEmtZR6WpP4x4IWNiBgJ/AB4C/By4MCIePlAxyFJw5m5WJJayzwsSf2nFUdsvAr4e2benZmLgGnAPi2IQ5KGM3OxJLWWeViS+smoFqxzI+CBhuezgVd3nigiDgMOq54ujIg7ByC2WotvHToBeLTVcWjIGD7t6QuxonNu1p9hDLBec7F5ePmZh9XPhk97WvE8DPXNxfaJm8RcrH40vNpSjfvErShsdPVq5TIDMqcCU5sfztARETMyc3Kr49DQYHsa8nrNxebh5ed+o/5kexry7BM3ifuO+ottqT5acSrKbGCThucbAw+1IA5JGs7MxZLUWuZhSeonrShs/AnYKiI2j4hVgAOAC1sQhyQNZ+ZiSWot87Ak9ZMBPxUlM5+LiE8AlwEjgZ9k5u0DHccQ5WGK6k+2pyHMXNw07jfqT7anIcw83FTuO+ovtqWaiMxlTuWTJEmSJEmqhVaciiJJkiRJktQvLGxIkiRJkqTasrAxSEXE8xFxS0T8OSJuiojXVMMnRsRtDdN9uBo/rnr+nxFxR0TcWs377YgYXY27txr+l4i4NiJafr9htU5EjIyImyPiooZh/1W1n9uq9nNIRBwXEcd3mneHiJg18FFLA8c8rGYzD0u9Mxer2czFQ4OFjcHr6czcITP/BTgWOL7zBBFxMPAfwBsz87GI+HfgjcAumbk98EpgLrB6w2yvz8xXANcAn23yNmhwOxxYkoir9rMX8KrM3A7YHQjgHGD/TvMeAJw9QHFKrWIeVrOZh6XemYvVbObiIcDCRj2MBR5rHBAR7wGOoSTwR6vB/w18NDMfB8jMRZl5QmbO72KZ1wMbNS9kDWYRsTHwVuDUhsGfAT7W0V4y84nMPD0z7wQej4hXN0z7HmDagAUstZ55WP3KPCytEHOx+pW5eOgY8Nu9qs9Wj4hbgNWADYA9GsZtBnwf2DEzHwaIiLWANTPznj4u/83Ar/otWtXNicDRwFqwpP2slZn/6Gb6cygV6RsjYhdgXmbeNRCBSi1kHlYznYh5WOoLc7Ga6UTMxUOCR2wMXh2H3W1DSbhnRERU49qB+ykVwg4BLLl3b0S8qTof8d6OcxErV0fEXOANeNjUsBQRbwPmZubMxsE0tJ8uTAP2i4gRlGR+ThNDlAYL87CawjwsLRdzsZrCXDy0WNiogcy8HpgAtFWDngLeAvx7RLyvmmY+8GREbF49vywzdwBuA1ZpWNzrKdXt24EvDcgGaLDZDXhHRNxLSc57ACdT2s8WXc2QmQ8A9wKvA94FnDcgkUqDhHlY/cw8LK0Ac7H6mbl4CLGwUQMRsQ0wEpjXMSwz2ylV669FxJuqwccDP4yIdar5gnLY3lIy82ngCOCQiFi3qcFr0MnMYzNz48ycSKk0X5WZB1Hazw8iYixARIyNiMMaZj0H+A7wj8ycPdBxS61kHlZ/Mg9LK8ZcrP5kLh5avMbG4NVxPiGUQ6IOzcznXzzyDjLznoh4B3BxRLwT+CEwhnLO17PAQuD3wM2dF56ZcyLiHODjwJebuiWqix8CawJ/iojFwGLgfxrG/xz4LuWq49JwYB7WQDMPS8syF2ugmYtrKDJ7OoVIkiRJkiRp8PJUFEmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjY05EXExR33Me9hmoXdDD8tIvZrSmCSNEyYhyWp9czFGspGtToAqVmi3OA8MnPvVsciScOReViSWs9crOHAIzY06EXE1yPiYw3Pj4uIL0TElRFxU0TcGhH7VOMmRsSsiDgZuAnYJCLujYgJ1fhfRcTMiLg9Ig7rtJ7/qZZ3ZUS0dRHHzhFxbTX/ZRGxQXO3XJIGB/OwJLWeuVjqnoUN1cE0YP+G5+8Bfgr8v8zcCXg98D9VNRpga+CMzNwxM+/rtKx/y8ydgcnAJyNifDV8DeCmannXAl9onCkiRgMnAftV8/8E+Gq/baEkDW7mYUlqPXOx1A1PRdGgl5k3R8R6EbEh0AY8BswBvhMRuwMvABsB61ez3JeZN3SzuE9GxP+rHm8CbAXMq5ZxbjX8TOD8TvNtDWwHXFF9VoysYpCkIc88LEmtZy6WumdhQ3UxHdgPeAmlWv0+SkLfOTMXR8S9wGrVtE92tYCImAK8Adg1M5+KiGsa5uksO88O3J6Zu674JkhSrZmHJan1zMVSFzwVRXUxDTiAksinA2sDc6sE/npgsz4sY23gsSqBbwPs0jBuRLVsgPcCv+s0751AW0TsCuUwvIiYtMJbI0n1Yx6WpNYzF0td8IgN1UJm3h4RawEPZuaciDgL+HVEzABuAe7ow2IuBf49Iv5CScqNh+Y9CUyKiJnAEyx9/iKZuSjKLa6+FxFrU/adE4HbV27LJKkezMOS1HrmYqlrkdn56CJJkiRJkqR68FQUSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbX1/wFnANKTKzGCyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_id = ('BKGR', '4CV')\n",
    "number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip'] \n",
    "\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_class_dict = {\n",
    "   'BKGR': 0 ,\n",
    "   \"4CV\": 0\n",
    "    }\n",
    "    \n",
    "    for clip_index_i in range(len(dataset_obj)):\n",
    "        data_idx = dataset_obj[clip_index_i]\n",
    "        label_id_idx = data_idx[1]\n",
    "        label = label_id[label_id_idx]\n",
    "        count_class_dict[label]+= 1\n",
    "        #count_class_dict[label]+= 1* number_of_frames_per_segment_in_a_clip\n",
    "\n",
    "    return count_class_dict\n",
    "        \n",
    "        \n",
    "def plot_from_dict(dict_obj, plot_title, **kwargs):\n",
    "    return sns.barplot(data = pd.DataFrame.from_dict([dict_obj]).melt(), \n",
    "                       x = \"variable\", y=\"value\", hue=\"variable\", **kwargs).set_title(plot_title)\n",
    "\n",
    "\n",
    "print(get_class_distribution(train_set))\n",
    "print(get_class_distribution(validation_dataset))\n",
    "print(get_class_distribution(test_set))\n",
    "    \n",
    "number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip']    \n",
    "print(f'Number of frames for training datasets {Ntrain*number_of_frames_per_segment_in_a_clip}')\n",
    "print(f'Number of frames for testing datasets {Ntest*number_of_frames_per_segment_in_a_clip}')\n",
    "#print(f'Number of frames for training datasets {Ntrain*number_of_frames_per_segment_in_a_clip}')\n",
    "\n",
    "plot_title_train_label= f'TRAIN dataset of {len(train_set)} clips with {number_of_frames_per_segment_in_a_clip} n_frames_per_clip'\n",
    "plot_title_test_label= f'TEST dataset of {len(test_set)} clips with {number_of_frames_per_segment_in_a_clip} n_frames_per_clip'\n",
    "plot_title_val_label= f'VALIDATION dataset of {len(validation_dataset)} clips with {number_of_frames_per_segment_in_a_clip} n_frames_per_clip'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,7))\n",
    "plot_from_dict(get_class_distribution(train_set), plot_title=plot_title_train_label, ax=axes[0])\n",
    "plot_from_dict(get_class_distribution(test_set), plot_title=plot_title_test_label, ax=axes[1])\n",
    "plot_from_dict(get_class_distribution(validation_dataset), plot_title=plot_title_val_label, ax=axes[2])\n",
    "plt.show()\n",
    "\n",
    "###########\n",
    "## NOTES ##\n",
    "# HamidehK on Thu 23 Mar 14:00:00 GMT 2022\n",
    "    # 65 clips with 60 frames each create 3900 frames which is a low number for traininig data.\n",
    "    # Hamideh recomends to increase the training data or perhaps reduce the convs to one!\n",
    "\n",
    "# MiguelX on Mon 28 Mar 11:35:55 BST 2022\n",
    "    # Make use of 169 clips with 60 frames which results in 10140 frames for training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d225",
   "metadata": {},
   "source": [
    "## 5. Displayting frames in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d8805e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T15:52:48.045191Z",
     "start_time": "2022-03-29T15:52:44.659671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train_dataset.__len__() = 169\n",
      " test_dataset.__len__() = 19\n",
      " validation_dataset.__len__() = 26\n",
      "====================================================\n",
      "len(train_dataloader): 43\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 0 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 1 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 2 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 3 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 4 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 5 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 6 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 7 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 8 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 9 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 10 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 11 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 12 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 13 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 14 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 15 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 16 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 17 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 18 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 19 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 20 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 21 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 22 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 23 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 24 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 25 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 26 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 27 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 28 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 29 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 30 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 31 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 32 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 33 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 34 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 35 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 36 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 37 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 38 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 39 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 40 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 41 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 42 / 42\n",
      "    sample_batched_labels.size(): torch.Size([1])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([])\n",
      "    sample_batched_images.size(): torch.Size([1, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "##### Setting up BATCH_SIZE_OF_CLIPS\n",
    "BATCH_SIZE_OF_CLIPS = 4\n",
    "##############################\n",
    "##############################\n",
    "\n",
    "print(f' train_dataset.__len__() = {train_set.__len__()}')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "print(f' test_dataset.__len__() = {test_set.__len__()}')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f' validation_dataset.__len__() = {validation_dataset.__len__()}')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f'len(train_dataloader): {len(train_dataloader)}')\n",
    "for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "    print(f'  ====================================================')\n",
    "    sample_batched_images=sample_batched[0]\n",
    "    sample_batched_labels=sample_batched[1]\n",
    "    print(f'    BATCH_OF_CLIPS_INDEX : {clip_batch_idx} / {len(train_dataloader) - 1}')\n",
    "    print(f'    sample_batched_labels.size(): {  sample_batched_labels.size()  }')\n",
    "    print(f'    sample_batched_labels.squeeze().size(): {  sample_batched_labels.squeeze().size()  }')\n",
    "    print(f'    sample_batched_images.size(): {sample_batched_images.size()}')\n",
    "\n",
    "    for BATCH_SIZE_IDX, label in enumerate(sample_batched_labels):\n",
    "        print(f'        BATCH_SIZE_IDX {BATCH_SIZE_IDX} label: {label}')\n",
    "        sample_batched_idx_image = sample_batched_images[BATCH_SIZE_IDX,...]\n",
    "        print(f'        Sample_batched_idx_image.size()  {sample_batched_idx_image.size() }'  )\n",
    "\n",
    "        grid = utils.make_grid(sample_batched_idx_image)\n",
    "        print(f'        Grid size {grid.size()}' )\n",
    "#         plt.figure(figsize =(20,20) )\n",
    "#         plt.imshow( grid.cpu().detach().numpy().transpose(1, 2, 0) )\n",
    "#         plt.title(f'BATCH_SIZE_IDX {BATCH_SIZE_IDX}; Label: {label_id[label]}')\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e92336",
   "metadata": {},
   "source": [
    "## 6. Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e369a416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T15:52:48.060037Z",
     "start_time": "2022-03-29T15:52:48.046766Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "################################\n",
    "##### Define VGG00 architecture\n",
    "class VGG00(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, n_classes=2):\n",
    "        \"\"\"\n",
    "        Simple Video classifier to classify into two classes:\n",
    "        Args:\n",
    "            input_size:  shape of the input image. Should be a 2 element vector for a 2D video (width, height) [e.g. 128, 128].\n",
    "            n_classes: number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        super(VGG00, self).__init__()\n",
    "        self.name = 'VGG00'\n",
    "        self.input_size = input_size\n",
    "        self.n_classes = n_classes\n",
    "        self.n_frames_per_clip = config['number_of_frames_per_segment_in_a_clip']\n",
    "        self.n_features = np.prod(self.input_size)*self.n_frames_per_clip\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.conv0 = nn.Conv3d(in_channels=1, out_channels=64,\n",
    "                               kernel_size = (1, 3, 3),  ## (-depth, -height, -width)\n",
    "                               stride =      (4, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "                               padding =     (0, 0, 0)\n",
    "                               )\n",
    "        #NOTES\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html\n",
    "        #IN: [N,Cin,D,H,W]; OUT: (N,Cout,Dout,Hout,Wout)\n",
    "        #[batch_size, channels, depth, height, width].\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=64, out_channels=128,\n",
    "                               kernel_size = (1, 3, 3),  # (-depth, -height, -width)\n",
    "                               stride =      (4, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "                               padding =     (0, 0, 0)\n",
    "                               )\n",
    "        \n",
    "#         self.conv2 = nn.Conv3d(in_channels=128, out_channels=256,\n",
    "#                                kernel_size =  (1, 3, 3),  # (-depth, -height, -width)\n",
    "#                                stride =       (1, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding =      (0, 0, 0)\n",
    "#                                )\n",
    "        \n",
    "        \n",
    "#         self.conv3 = nn.Conv3d(in_channels=256, out_channels=512,\n",
    "#                                kernel_size=(2, 2, 2),  # (-depth, -height, -width)\n",
    "#                                stride=(2, 2, 2), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding = (0, 0, 0)\n",
    "#                                )\n",
    "        \n",
    "        \n",
    "#         self.pool0 = nn.MaxPool3d(\n",
    "#                                 kernel_size = (1, 3, 3),  # (-depth, -height, -width)\n",
    "#                                 stride =      (1, 1, 1), \n",
    "#                                 padding =     (0, 0, 0), \n",
    "#                                 dilation =    (1, 1, 1)\n",
    "#                                 )\n",
    "#         #NOTES\n",
    "#         #Keeps the training to 50% after 100 epochs\n",
    "\n",
    "\n",
    "        self.fc0 = nn.Linear(in_features=100352, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=self.n_classes)\n",
    "        #self.fc1 = nn.Linear(in_features=2048, out_features=self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f'x.shape(): {x.size()}') ##[batch_size, channels, depth, height, width]\n",
    "        \n",
    "        x = F.relu( self.conv0(x) )\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape(): x.shape(): torch.Size([2, 64, 60, 128, 128]) with kernel_size=(1, 1, 1)\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape():torch.Size([2, 64, 51, 29, 29]) with kernel_size=(10, 100, 100)\n",
    "        #print(f'conv0.size(): {x.size()}')\n",
    "        \n",
    "        x = F.relu( self.conv1(x) )\n",
    "        #print(f'x.shape(): {x.size()}') with kernel_size=(1, 10, 10) #x.shape(): torch.Size([2, 32, 60, 20, 20])\n",
    "        #print(f'conv1.size(): {x.size()}')\n",
    "        \n",
    "        #x = F.relu( self.conv2(x) )\n",
    "        #x = F.relu( self.conv3(x) )\n",
    "        \n",
    "        #x = self.pool0(x)\n",
    "        #print(f'x.pool0..shape(): {x.size()}') \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        #print(f'self.flatten(x) size() {x.size()}') #x.shape(): torch.Size([4, 983040])\n",
    "        x = self.fc0(x)\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape(): torch.Size([4, 32])\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=0.5) #dropout was included to combat overfitting\n",
    "        \n",
    "        #print(f'x.shape(): {x.size()}') # x.shape(): torch.Size([4, 2])\n",
    "        #x = self.sigmoid(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        #print(f'x.shape(): {x.size()}')  #x.shape(): torch.Size([4, 2])\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "################################\n",
    "##### Define VGG architecture\n",
    "class basicVGGNet(nn.Module):\n",
    "\n",
    "    def __init__(self, tensor_shape_size, n_classes=2, cnn_channels=(1, 16, 32)):\n",
    "        \"\"\"\n",
    "        Simple Visual Geometry Group Network (VGGNet) to classify two US image classes (background and 4CV).\n",
    "\n",
    "        Args:\n",
    "            tensor_shape_size: [Batch_clips, Depth, Channels, Height, Depth]\n",
    "\n",
    "        \"\"\"\n",
    "        super(basicVGGNet, self).__init__()\n",
    "        self.name = 'basicVGGNet'\n",
    "\n",
    "        self.tensor_shape_size = tensor_shape_size\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # define the CNN\n",
    "        self.n_output_channels = cnn_channels ##  self.n_output_channels::: (1, 16, 32)\n",
    "        self.kernel_size = (3, ) * (len(cnn_channels) -1) ## self.kernel_size::: (3, 3)\n",
    "\n",
    "        self.n_batch_size_of_clip_numbers = self.tensor_shape_size[0]\n",
    "        self.n_frames_per_clip = self.tensor_shape_size[1]\n",
    "        self.n_number_of_image_channels = self.tensor_shape_size[2]\n",
    "        self.input_shape_tensor = self.n_batch_size_of_clip_numbers * self.n_frames_per_clip * self.n_number_of_image_channels\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=self.n_number_of_image_channels, out_channels=64,\n",
    "                               kernel_size=(1, 1, 1), stride=(1, 1, 1), padding = (0, 0, 0)\n",
    "                               )\n",
    "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=2,\n",
    "                               kernel_size=(1, 1, 1), stride=(1, 1, 1), padding = (0, 0, 0)\n",
    "                               )\n",
    "                    #IN: [N,Cin,D,H,W]; OUT: (N,Cout,Dout,Hout,Wout)\n",
    "                    #[batch_size, channels, depth, height, width].\n",
    "\n",
    "        self.pool0 = nn.MaxPool3d(kernel_size=(60, 128, 128), stride=(1, 1, 1), padding=(0, 0, 0), dilation=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 0, 0))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(3, 3, 3), padding=(0, 0, 0))\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(16, 16, 16), stride=(16, 16, 16), padding=(0, 0, 0))\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=(32, 32, 32), stride=(32, 32, 32), padding=(0, 0, 0))\n",
    "        self.pool6 = nn.MaxPool3d(kernel_size=(30, 30, 30), stride=(30, 30, 30), padding=(0, 0, 0))\n",
    "        self.pool7 = nn.MaxPool3d(kernel_size= 60, stride= 60, padding=0)\n",
    "        self.pool8 = nn.MaxPool3d(kernel_size=(64, 64, 64), stride=(64, 64, 64), padding=(0, 0, 0))\n",
    "        self.pool9 = nn.MaxPool3d(kernel_size=(128, 128, 128), stride=(128, 128, 128), padding=(0, 0, 0))\n",
    "\n",
    "        self.bn1 = nn.BatchNorm3d(num_features = 64)\n",
    "        self.bn2 = nn.BatchNorm3d(num_features = 12)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc0 = nn.Linear(in_features=10, out_features=self.n_classes)\n",
    "        self.fc1 = nn.Linear(in_features=62914560, out_features=self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f'x.shape(): {x.size()}') ##[batch_size, channels, depth, height, width]\n",
    "        \n",
    "        #x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(f'x.shape(): {x.size()}')\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(f'x.shape():: {x.size()}')\n",
    "        x = self.pool0(x)\n",
    "        #print(f'x.shape()::: {x.size()}')\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"After flattening, x.shape: {x.shape}\")\n",
    "\n",
    "        # x = x.reshape(x.shape[0], -1)\n",
    "        # x = F.dropout(x, p=0.5) #dropout was included to combat overfitting\n",
    "        #x = self.fc0(x)\n",
    "        #print(f\"After fc1, x.shape: {x.shape}\")\n",
    "\n",
    "        return x    \n",
    "\n",
    "    \n",
    "    \n",
    "###########\n",
    "## NOTES ##\n",
    "# Miguel on Mon 28 Mar 15:47:26 BST 2022\n",
    "    #adding more conv3d might create reduced number of pixel size\n",
    "    #RuntimeError: Given input size: (256x60x4x4). Calculated output size: (256x60x-5x-5). Output size is too small\n",
    "    \n",
    "\n",
    "# Miguel on Thu 24 Mar 09:30:09 GMT 2022\n",
    "    # Implementations of conv3D are nearly finalised. I will then move on to conv2D with indivial frames to create \n",
    "    # sketchs comparison metrics \n",
    "        \n",
    "# Alberto on Thu 22 Mar 09:00:00 GMT 2022\n",
    "    # The use of conv3D might consume more resources vs conv2D with indivial frames (MX: maybe there is a Trade-off here)\n",
    "    # For features per frame, AG prototyped the following: \n",
    "    #     def forward(self, data):\n",
    "\n",
    "    #         n_frames = data.shape[2]\n",
    "    #         features = []\n",
    "    #         for f in range(n_frames):\n",
    "    #             feat_i = self.frame_features(data[:, :, f, ...])\n",
    "    #             features.append(feat_i)\n",
    "\n",
    "    #         feature_vector = torch.stack(features, dim=1)\n",
    "\n",
    "    #         out = self.classifier(feature_vector)\n",
    "    #         return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aafb41",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3791b35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T15:52:51.022939Z",
     "start_time": "2022-03-29T15:52:48.061291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "VGG00(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (conv0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(4, 3, 3))\n",
      "  (conv1): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(4, 3, 3))\n",
      "  (fc0): Linear(in_features=100352, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=2, bias=True)\n",
      ")\n",
      " /home/mx19/repositories/echocardiography/models\n",
      "Saved metric model\n",
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 0 \n",
      "   X_train_batch.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45475/3550200339.py:99: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 1 \n",
      "   X_train_batch.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([4])\n",
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 2 \n",
      "   X_train_batch.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([4])\n",
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 3 \n",
      "   X_train_batch.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([4])\n",
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 4 \n",
      "   X_train_batch.size(): torch.Size([3, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([3])\n",
      "==================================================\n",
      "==================================================\n",
      "{'BKGR': 8, '4CV': 11}\n",
      "y_pred_list[1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "y_true_list[1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "model = VGG00(config['pretransform_im_size']) #print(config['pretransform_im_size']) #(128, 128)\n",
    "## PRINT MODEL\n",
    "print(f'====================================================')\n",
    "print(model)\n",
    "\n",
    "print(f' {FULL_REPO_MODEL_PATH}')\n",
    "torch.save(model.state_dict(), os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\"))\n",
    "print(\"Saved metric model\")\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    #os.path.join(model_path, \"VGG00v00_metric_model.pth\")\n",
    "    #os.path.join(model_path, \"VGG00v01_metric_model.pth\")\n",
    "    os.path.join(FULL_REPO_MODEL_PATH, \"VGG00v02_metric_model.pth\")\n",
    "    ))\n",
    "\n",
    "model.to(device) # Place model on GPU\n",
    "\n",
    "model.eval()\n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clip_batch_idx, sample_batched in enumerate(test_dataloader):\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(device), sample_batched[1].to(device)\n",
    "        print(f'==================================================')\n",
    "        print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        y_test_pred = model(X_train_batch)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)        \n",
    "        \n",
    "        for i in range(len(y_test_pred)):\n",
    "            y_true_list.append(y_train_batch[i].cpu().item())\n",
    "            y_pred_list.append(y_pred_tag[i].cpu().item())\n",
    "            \n",
    "        \n",
    "print(f'==================================================')        \n",
    "print(f'==================================================')        \n",
    "print(get_class_distribution(test_set))\n",
    "print(f'y_pred_list{y_pred_list}')\n",
    "print(f'y_true_list{y_true_list}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "781a2101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T15:52:51.130658Z",
     "start_time": "2022-03-29T15:52:51.023864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.38      0.40         8\n",
      "           1       0.58      0.64      0.61        11\n",
      "\n",
      "    accuracy                           0.53        19\n",
      "   macro avg       0.51      0.51      0.50        19\n",
      "weighted avg       0.52      0.53      0.52        19\n",
      "\n",
      "[[3 5]\n",
      " [4 7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Predicted'), Text(0, 0.5, 'True')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(y_true_list, y_pred_list))\n",
    "print(confusion_matrix(y_true_list, y_pred_list))\n",
    "\n",
    "cm=confusion_matrix(y_true_list, y_pred_list)\n",
    "#cm=confusion_matrix(y_true_list, y_pred_list, normalize='all')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['BGR','4CV'])\n",
    "cmd.plot()\n",
    "cmd.ax_.set(xlabel='Predicted', ylabel='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607176b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:44:50.478014Z",
     "start_time": "2022-03-28T16:44:50.470241Z"
    }
   },
   "source": [
    "\n",
    "## File size of models: \n",
    "```\n",
    ":~/repositories/echocardiography/models$ tree -s\n",
    ".\n",
    " [  201010447]  metric_model.pth\n",
    " [  351538511]  VGG00v00_metric_model.pth\n",
    " [  752946511]  VGG00v01_metric_model.pth\n",
    " [  201010511]  VGG00v02_metric_model.pth\n",
    "\n",
    "0 directories, 4 files\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9403d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72faf859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
