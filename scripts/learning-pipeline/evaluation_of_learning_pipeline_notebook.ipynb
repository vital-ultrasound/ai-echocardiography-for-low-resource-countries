{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef885fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:05.695513Z",
     "start_time": "2021-12-17T09:18:03.125501Z"
    }
   },
   "source": [
    "# Evaluation of models\n",
    "\n",
    "**Author**: Miguel Xochicale [@mxochicale](https://github.com/mxochicale)     \n",
    "**Contributors**: Nhat Phung Tran Huy [@huynhatd13](https://github.com/huynhatd13); Hamideh Kerdegari [@hamidehkerdegari](https://github.com/hamidehkerdegari);  Alberto Gomez [@gomezalberto](https://github.com/)  \n",
    "\n",
    "* Feb2022; March2022 \n",
    "* Aug2022; tidies notebook\n",
    "\n",
    "\n",
    "## Summary\n",
    "This notebook presents a learning pipeline to classify 4 chamber view from echocardiography datasets.\n",
    "\n",
    "### How to run the notebook\n",
    "\n",
    "1. Go to echocardiography repository path: `$HOME/repositories/echocardiography/`\n",
    "2. Open echocardiography repo in pycharm and in the terminal type:\n",
    "    ```\n",
    "    git checkout master # or the branch\n",
    "    git pull # to bring a local branch up-to-date with its remote version\n",
    "    ```\n",
    "3. Launch Notebook server  \n",
    "    Go to you repository path: `cd $HOME/repositories/echocardiography/scripts/dataloaders` and type in the pycharm terminal:\n",
    "    ```\n",
    "    conda activate rt-ai-echo-VE \n",
    "    jupyter notebook\n",
    "    ```\n",
    "    which will open your web-browser.\n",
    "    \n",
    "    \n",
    "### References\n",
    "* \"Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback\". https://www.fda.gov/media/122535/download \n",
    "* Gomez A. et al. 2021 https://github.com/vital-ultrasound/lung/blob/main/multiclass_pytorch/datasets/LUSVideoDataset.py \n",
    "* Kerdegari H. et al. 2021 https://github.com/vital-ultrasound/lung/tree/main/multiclass_tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2aa0",
   "metadata": {},
   "source": [
    "# Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d3efa",
   "metadata": {},
   "source": [
    "## 1. Setting imports and datasets paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f345efe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-04T16:38:50.353378Z",
     "start_time": "2022-08-04T16:38:49.520014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.9.0\n",
      "Torchvision Version: 0.10.0a0\n",
      "FULL_PATH_FOR_YML_FILE: /home/mx19/repositories/echocardiography/scripts/config_files/users_paths_files/config_users_paths_files_username_mx19.yml\n",
      "FULL_REPO_MODEL_PATH: /home/mx19/repositories/echocardiography/data/models\n",
      "TRAINING_CURVES_PATH: /home/mx19/repositories/echocardiography/scripts/learning-pipeline/results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "#import matplotlib.animation as animation\n",
    "from IPython.display import HTML #to be used with HTML(animation.ArtistAnimation().to_jshtml())\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, \\\n",
    "                            ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from source.dataloaders.EchocardiographicVideoDataset import EchoClassesDataset\n",
    "from source.helpers.various import concatenating_YAML_via_tags, \\\n",
    "                                    plot_dataset_classes, \\\n",
    "                                    split_train_validate_sets\n",
    "from source.helpers.learning_pipeline import get_class_distribution, \\\n",
    "                                            plot_from_dict, \\\n",
    "                                            creating_pair_of_clips, \\\n",
    "                                            pair_clips_labels, \\\n",
    "                                            animate_clips\n",
    "from source.models.learning_misc import train_loop, \\\n",
    "                                        test_loop\n",
    "\n",
    "from source.models.architectures import basicVGG, TrompNetV1, \\\n",
    "        LeNet5_source00, LeNet5_source01, LeNet5_source02, \\\n",
    "        AlexNet_source00, AlexNet_source01, AlexNet_source02, AlexNet_source03, \\\n",
    "        MobileNetV1, MobileNetV2, \\\n",
    "        SqueezeNet_source0, SqueezeNet_source1, SqueezeNet_source2\n",
    "        #ShuffleNetV1, ShuffleNetV2 (require 3 channels of input images)\n",
    "\n",
    "HOME_PATH = os.path.expanduser(f'~')\n",
    "USERNAME = os.path.split(HOME_PATH)[1]\n",
    "\n",
    "REPOSITORY_PATH='repositories/echocardiography'\n",
    "FULL_REPO_PATH = HOME_PATH+'/'+REPOSITORY_PATH\n",
    "FULL_REPO_MODEL_PATH = HOME_PATH +'/' + REPOSITORY_PATH + '/data/models'\n",
    "CONFIG_FILES_PATH= REPOSITORY_PATH + '/scripts/config_files/users_paths_files'\n",
    "YML_FILE =  'config_users_paths_files_username_' + USERNAME + '.yml'\n",
    "FULL_PATH_FOR_YML_FILE = os.path.join(HOME_PATH, CONFIG_FILES_PATH, YML_FILE)\n",
    "PATH_for_temporal_files = os.path.join(HOME_PATH, 'datasets/vital-us/echocardiography/temporal-files')\n",
    "\n",
    "## Setting TRAINING_CURVES_PATH\n",
    "#CURRENT_PATH=os.path.abspath(os.getcwd())\n",
    "RESULTS_PATH='scripts/learning-pipeline/results'\n",
    "TRAINING_CURVES_PATH = os.path.join(FULL_REPO_PATH, RESULTS_PATH)\n",
    "\n",
    "## Setting FULL_PATH_FOR_YML_FILE\n",
    "yaml.add_constructor('!join', concatenating_YAML_via_tags)  ## register the tag handler\n",
    "with open(FULL_PATH_FOR_YML_FILE, 'r') as yml:\n",
    "    config = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "\n",
    "## Printing Versions and paths\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')    \n",
    "print(f'FULL_PATH_FOR_YML_FILE: {FULL_PATH_FOR_YML_FILE}' )\n",
    "print(f'FULL_REPO_MODEL_PATH: {FULL_REPO_MODEL_PATH}' )\n",
    "print(f'TRAINING_CURVES_PATH: {TRAINING_CURVES_PATH}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:08.264310Z",
     "start_time": "2021-12-17T09:18:08.250178Z"
    }
   },
   "source": [
    "## 2. Setting variables and loading datasets using pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5109aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-04T16:38:50.621690Z",
     "start_time": "2022-08-04T16:38:50.354724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "train_set_size  0.7, test_set_size 0.2, val_test_size 0.10000000000000003\n",
      "---------------------------------------------\n",
      "  FILENAMES_len=28\n",
      "======= video_filenames: ('/01NVb-003-044/T3/01NVb-003-044-3 echo.mp4', '/01NVb-003-043/T3/01NVb-003-043-3 echo.mp4', '/01NVb-003-048/T2/01NVb-003-048-2 echo.mp4', '/01NVb-003-040/T2/01NVb-003-040-2 echo.mp4', '/01NVb-003-040/T1/01NVb-003-040-1 echo.mp4', '/01NVb-003-046/T1/01NVb-003-046-1 echo.mp4', '/01NVb-003-042/T1/01NVb-003-042-1 echo.mp4', '/01NVb-003-041/T1/01NVb-003-041-1 echo.mp4', '/01NVb-003-045/T2/01NVB-003-045-2 echo.mp4', '/01NVb-003-045/T3/01NVb-003-045-3 echo.mp4', '/01NVb-003-050/T1/01NVb-003-050-1 echo.mp4', '/01NVb-003-041/T2/01NVb-003-041-2 echo.mp4', '/01NVb-003-050/T3/01NVb-003-050-3 echo.mp4', '/01NVb-003-046/T3/01NVb-003-046-3 echo.mp4', '/01NVb-003-042/T3/01NVb-003-042-3 echo.mp4', '/01NVb-003-047/T3/01NVb-003-047-3 echo.mp4', '/01NVb-003-043/T1/01NVb-003-043-1 echo.mp4', '/01NVb-003-048/T3/01NVb-003-048-3 echo.mp4', '/01NVb-003-047/T1/01NVb-003-047-1 echo.mp4', '/01NVb-003-050/T2/01NVb-003-050-2 echo.mp4', '/01NVb-003-042/T2/01NVb-003-042-2 echo.mp4', '/01NVb-003-044/T1/01NVb-003-044-2 echo.mp4', '/01NVb-003-048/T1/01NVb-003-048-1 echo.mp4', '/01NVb-003-045/T1/01NVb-003-045-1 echo.mp4', '/01NVb-003-046/T2/01NVb-003-046-2 echo.mp4', '/01NVb-003-047/T2/01NVb-003-047-2 echo.mp4', '/01NVb-003-041/T3/01NVb-003-041-3 echo.mp4', '/01NVb-003-043/T1/01NVb-003-043-2 echo.mp4')\n",
      "======= label_filenames: ('/01NVb-003-044/T3/01nvb-003-044-3-4cv.json', '/01NVb-003-043/T3/01nvb-003-043-3-4cv.json', '/01NVb-003-048/T2/01nvb-003-048-2-4cv.json', '/01NVb-003-040/T2/01nvb-003-040-2-4cv.json', '/01NVb-003-040/T1/01nvb-003-040-1-4cv.json', '/01NVb-003-046/T1/01nvb-003-046-1-4cv.json', '/01NVb-003-042/T1/01nvb-003-042-1-4cv.json', '/01NVb-003-041/T1/01nvb-003-041-1-4cv.json', '/01NVb-003-045/T2/01nvb-003-045-2-4cv.json', '/01NVb-003-045/T3/01nvb-003-045-3-4cv.json', '/01NVb-003-050/T1/01NVb-003-050-1-4CV.json', '/01NVb-003-041/T2/01nvb-003-041-2-4cv.json', '/01NVb-003-050/T3/01NVb-003-050-3-4CV.json', '/01NVb-003-046/T3/01nvb-003-046-3-4cv.json', '/01NVb-003-042/T3/01nvb-003-042-3-4cv.json', '/01NVb-003-047/T3/01nvb-003-047-3-4cv.json', '/01NVb-003-043/T1/01nvb-003-043-1-4cv.json', '/01NVb-003-048/T3/01nvb-003-048-3-4cv.json', '/01NVb-003-047/T1/01nvb-003-047-1-4cv.json', '/01NVb-003-050/T2/01NVb-003-050-2-4CV.json', '/01NVb-003-042/T2/01nvb-003-042-2-4cv.json', '/01NVb-003-044/T1/01nvb-003-044-2-4cv.json', '/01NVb-003-048/T1/01nvb-003-048-1-4cv.json', '/01NVb-003-045/T1/01nvb-003-045-1-4cv.json', '/01NVb-003-046/T2/01nvb-003-046-2-4cv.json', '/01NVb-003-047/T2/01nvb-003-047-2-4cv.json', '/01NVb-003-041/T3/01nvb-003-041-3-4cv.json', '/01NVb-003-043/T1/01nvb-003-043-2-4cv.json')\n",
      "---------------------------------------------\n",
      "==TRAIN_len = 19\n",
      "== video_filenames_train: ('/01NVb-003-044/T3/01NVb-003-044-3 echo.mp4', '/01NVb-003-043/T3/01NVb-003-043-3 echo.mp4', '/01NVb-003-048/T2/01NVb-003-048-2 echo.mp4', '/01NVb-003-040/T2/01NVb-003-040-2 echo.mp4', '/01NVb-003-040/T1/01NVb-003-040-1 echo.mp4', '/01NVb-003-046/T1/01NVb-003-046-1 echo.mp4', '/01NVb-003-042/T1/01NVb-003-042-1 echo.mp4', '/01NVb-003-041/T1/01NVb-003-041-1 echo.mp4', '/01NVb-003-045/T2/01NVB-003-045-2 echo.mp4', '/01NVb-003-045/T3/01NVb-003-045-3 echo.mp4', '/01NVb-003-050/T1/01NVb-003-050-1 echo.mp4', '/01NVb-003-041/T2/01NVb-003-041-2 echo.mp4', '/01NVb-003-050/T3/01NVb-003-050-3 echo.mp4', '/01NVb-003-046/T3/01NVb-003-046-3 echo.mp4', '/01NVb-003-042/T3/01NVb-003-042-3 echo.mp4', '/01NVb-003-047/T3/01NVb-003-047-3 echo.mp4', '/01NVb-003-043/T1/01NVb-003-043-1 echo.mp4', '/01NVb-003-048/T3/01NVb-003-048-3 echo.mp4', '/01NVb-003-047/T1/01NVb-003-047-1 echo.mp4')\n",
      "== label_filenames_train: ('/01NVb-003-044/T3/01nvb-003-044-3-4cv.json', '/01NVb-003-043/T3/01nvb-003-043-3-4cv.json', '/01NVb-003-048/T2/01nvb-003-048-2-4cv.json', '/01NVb-003-040/T2/01nvb-003-040-2-4cv.json', '/01NVb-003-040/T1/01nvb-003-040-1-4cv.json', '/01NVb-003-046/T1/01nvb-003-046-1-4cv.json', '/01NVb-003-042/T1/01nvb-003-042-1-4cv.json', '/01NVb-003-041/T1/01nvb-003-041-1-4cv.json', '/01NVb-003-045/T2/01nvb-003-045-2-4cv.json', '/01NVb-003-045/T3/01nvb-003-045-3-4cv.json', '/01NVb-003-050/T1/01NVb-003-050-1-4CV.json', '/01NVb-003-041/T2/01nvb-003-041-2-4cv.json', '/01NVb-003-050/T3/01NVb-003-050-3-4CV.json', '/01NVb-003-046/T3/01nvb-003-046-3-4cv.json', '/01NVb-003-042/T3/01nvb-003-042-3-4cv.json', '/01NVb-003-047/T3/01nvb-003-047-3-4cv.json', '/01NVb-003-043/T1/01nvb-003-043-1-4cv.json', '/01NVb-003-048/T3/01nvb-003-048-3-4cv.json', '/01NVb-003-047/T1/01nvb-003-047-1-4cv.json')\n",
      "---------------------------------------------\n",
      "  TEST_len = 6\n",
      "== video_filenames_test: ('/01NVb-003-050/T2/01NVb-003-050-2 echo.mp4', '/01NVb-003-042/T2/01NVb-003-042-2 echo.mp4', '/01NVb-003-044/T1/01NVb-003-044-2 echo.mp4', '/01NVb-003-048/T1/01NVb-003-048-1 echo.mp4', '/01NVb-003-045/T1/01NVb-003-045-1 echo.mp4', '/01NVb-003-046/T2/01NVb-003-046-2 echo.mp4')\n",
      "== label_filenames_test: ('/01NVb-003-050/T2/01NVb-003-050-2-4CV.json', '/01NVb-003-042/T2/01nvb-003-042-2-4cv.json', '/01NVb-003-044/T1/01nvb-003-044-2-4cv.json', '/01NVb-003-048/T1/01nvb-003-048-1-4cv.json', '/01NVb-003-045/T1/01nvb-003-045-1-4cv.json', '/01NVb-003-046/T2/01nvb-003-046-2-4cv.json')\n",
      "---------------------------------------------\n",
      "  VALIDATION_len = 3\n",
      "== video_filenames_validation: ('/01NVb-003-047/T2/01NVb-003-047-2 echo.mp4', '/01NVb-003-041/T3/01NVb-003-041-3 echo.mp4', '/01NVb-003-043/T1/01NVb-003-043-2 echo.mp4')\n",
      "== label_filenames_validation: ('/01NVb-003-047/T2/01nvb-003-047-2-4cv.json', '/01NVb-003-041/T3/01nvb-003-041-3-4cv.json', '/01NVb-003-043/T1/01nvb-003-043-2-4cv.json')\n",
      "Files were successfully written at /home/mx19/repositories/echocardiography/scripts/config_files/data_lists/\n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-044/T3/01NVb-003-044-3 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.97189215655725 nframes=16095 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-043/T3/01NVb-003-043-3 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.962891699226105 nframes=8395 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-048/T2/01NVb-003-048-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.246142365043703 nframes=16201 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-040/T2/01NVb-003-040-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.97114679618872 nframes=26836 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-040/T1/01NVb-003-040-1 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.971632559729454 nframes=18702 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-046/T1/01NVb-003-046-1 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.971807551878726 nframes=16861 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-042/T1/01NVb-003-042-1 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.971431814361793 nframes=21380 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-041/T1/01NVb-003-041-1 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.36164397750906 nframes=18822 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-045/T2/01NVB-003-045-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.97203398407009 nframes=14956 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-045/T3/01NVb-003-045-3 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.971736011469602 nframes=17568 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-050/T1/01NVb-003-050-1 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.97098873486474 nframes=31260 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-041/T2/01NVb-003-041-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.97271017303212 nframes=11183 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-050/T3/01NVb-003-050-3 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.894639721229318 nframes=18637 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-046/T3/01NVb-003-046-3 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.31047987138222 nframes=15954 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-042/T3/01NVb-003-042-3 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.972288279876253 nframes=13272 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-047/T3/01NVb-003-047-3 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.972108047270993 nframes=14423 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-043/T1/01NVb-003-043-1 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.97199599141347 nframes=15245 \n",
      "  \n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-048/T3/01NVb-003-048-3 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.121017486668457 nframes=26308 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-047/T1/01NVb-003-047-1 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.639684201243103 nframes=20098 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-050/T2/01NVb-003-050-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.97112109865596 nframes=27468 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-042/T2/01NVb-003-042-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.972611145636726 nframes=11612 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-044/T1/01NVb-003-044-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.97213741887112 nframes=14222 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-048/T1/01NVb-003-048-1 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.972025178735784 nframes=15022 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-045/T1/01NVb-003-045-1 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.972716660300705 nframes=11156 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-046/T2/01NVb-003-046-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.972080184147497 nframes=14619 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-047/T2/01NVb-003-047-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.971856185980858 nframes=16412 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-041/T3/01NVb-003-041-3 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.97287612672228 nframes=10531 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  VIDEO_FEATURES\n",
      "    video_name=/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects//01NVb-003-043/T1/01NVb-003-043-2 echo.mp4\n",
      "    Frame_height=1080, frame_width=1920 fps=29.971971788739964 nframes=15435 \n",
      "  \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "##### Setting up device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #or \"cuda:NN\" can also be used e.g., \"cuda:0\"\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "\n",
    "# ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-05-subjects'\n",
    "ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects'\n",
    "# ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-20-subjects'\n",
    "# ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-31-subjects'\n",
    "\n",
    "\n",
    "TRAINING_SPLITTING = 0.70 #config['ntraining'] #Default\n",
    "TEST_FRACTION = 0.20\n",
    "VAL_FRACTION = 1-TRAINING_SPLITTING-TEST_FRACTION\n",
    "print(f'train_set_size  {TRAINING_SPLITTING}, test_set_size {TEST_FRACTION}, val_test_size {VAL_FRACTION}')\n",
    "\n",
    "FLAG_RANDOMISE_DATA=True #config['randomise_file_list'] #Default\n",
    "\n",
    "split_train_validate_sets(  \n",
    "                        ECHODATASET_PATH, #config['echodataset_path']\n",
    "                        config['data_list_output_path'], \n",
    "                        TRAINING_SPLITTING,\n",
    "                        TEST_FRACTION,\n",
    "                        FLAG_RANDOMISE_DATA\n",
    "                        )\n",
    "\n",
    "# PRETRANSFORM_IM_SIZE = [64, 64] #[650, 690] original pixel size for VenueGO\n",
    "PRETRANSFORM_IM_SIZE = [128, 128] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = [256, 256] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = [512, 512] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = config['pretransform_im_size'] ##DEFAULT\n",
    "\n",
    "### >> CHANGE DENSE LAYER FEATURES IN VGG3D\n",
    "### >> `self.fc0 = nn.Linear(in_features=4194304, out_features=500) #128x128`\n",
    "\n",
    "##############################\n",
    "##### Experiments for Basic HYPERPARAMETER Heuristics \n",
    "\n",
    "#### TESTS\n",
    "NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 1; BATCH_SIZE_OF_CLIPS = 20; \n",
    "#LEARNING_RATE= 0.00005; \n",
    "\n",
    "\n",
    "#################  LEARNING_RATE \n",
    "### EXPERIMENT 01,02,03,04\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 1; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.00005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.0000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.00000005; \n",
    "\n",
    "#################  BATCH_SIZE_OF_CLIPS with LEARNING_RATE= 0.000005 as it is the best peformance of prevous LRs \n",
    "### EXPERIMENT 04,06,07,08\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 2; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 5; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 15; LEARNING_RATE= 0.000005; \n",
    "\n",
    "#################  NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP with LEARNING_RATE= 0.000005 and BATCH_SIZE_OF_CLIPS=10\n",
    "### EXPERIMENT 09,10,11,12\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 2; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 7; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 13; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 20; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "\n",
    "##TOADD\n",
    "### * NUMBER_OF_FRAMES_AND PARTICPANTS\n",
    "### * OPTIMISERS \n",
    "\n",
    "# LEARNING_RATE =Trial and Error with diffent values  0.00005; 0.0005; 0.005 and 0.000001; 0.00001; 0.0001; 0.001  \n",
    "\n",
    "\n",
    "MAX_EPOCHS = 500 #Alternatvely, make use of: config['max_epochs']\n",
    "\n",
    "\n",
    "##############################\n",
    "##### Setting up animation\n",
    "interval_between_frames_in_milliseconds=33.3 ## 1/30=0.033333\n",
    "frame_per_seconds_for_animated_frames=30\n",
    "\n",
    "\n",
    "#SUBJECT_ID = '073'\n",
    "#print(SUBJECT_ID)\n",
    "\n",
    "### CUDA out of memory \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10\n",
    "#PRETRANSFORM_IM_SIZE = [128, 128] \n",
    "#RuntimeError: CUDA out of memory. Tried to allocate 7.81 GiB (GPU 0; 15.74 GiB total capacity; 8.51 GiB already allocated; 5.05 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
    "## REBOOT MACHINE\n",
    "#RuntimeError: CUDA out of memory. Tried to allocate 7.81 GiB (GPU 0; 15.74 GiB total capacity; 8.51 GiB already allocated; 5.13 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "## Setting labels\n",
    "label_id = ('BKGR', '4CV')\n",
    "\n",
    "\n",
    "\n",
    "# Defining transforms that apply to the entire dataset.\n",
    "# These transforms are not augmentation.\n",
    "if config['use_pretransform_image_size']:\n",
    "    pretransform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=PRETRANSFORM_IM_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    pretransform = None\n",
    "    \n",
    "    #config['use_train_augmentation']#Default\n",
    "    \n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_train_augmentation']:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),  # in degrees\n",
    "        transforms.RandomEqualize(p=0.5),\n",
    "        transforms.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "    \n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_validation_augmentation']:\n",
    "    val_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=5),  # in degrees\n",
    "    #transforms.RandomEqualize(p=0.5),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "\n",
    "\n",
    "train_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_train'],\n",
    "    participant_path_json_list=config['participant_path_json_list_train'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=train_transform, #None,#transform=train_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "test_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_test'],\n",
    "    participant_path_json_list=config['participant_path_json_list_test'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,#transform=test_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "val_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_validation'],\n",
    "    participant_path_json_list=config['participant_path_json_list_validation'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,#transform=val_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760906",
   "metadata": {},
   "source": [
    "## 3. Plotting Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6472a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-04T16:40:26.846368Z",
     "start_time": "2022-08-04T16:38:50.622490Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mx19/anaconda3/envs/rt-ai-echo-VE/lib/python3.8/site-packages/torchvision/transforms/functional.py:594: UserWarning: torch.lstsq is deprecated in favor of torch.linalg.lstsq and will be removed in a future PyTorch release.\n",
      "torch.linalg.lstsq has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n",
      "To get the qr decomposition consider using torch.linalg.qr.\n",
      "The returned solution in torch.lstsq stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals in the field 'residuals' of the returned named tuple.\n",
      "The unpacking of the solution, as in\n",
      "X, _ = torch.lstsq(B, A).solution[:A.size(1)]\n",
      "should be replaced with\n",
      "X = torch.linalg.lstsq(A, B).solution (Triggered internally at  /tmp/pip-req-build-pma2oi4d/aten/src/ATen/LegacyTHFunctionsCPU.cpp:389.)\n",
      "  res = torch.lstsq(b_matrix, a_matrix)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_distribution(train_dataset): {'BKGR': 37, '4CV': 37}\n",
      "class_distribution(test_dataset): {'BKGR': 12, '4CV': 12}\n",
      "class_distribution(val_dataset): {'BKGR': 8, '4CV': 8}\n",
      "Number of frames for training datasets 74\n",
      "Number of frames for testing datasets 24\n",
      "Number of frames for Validation datasets 16\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set_class_dict = get_class_distribution(train_dataset,label_id)\n",
    "test_set_class_dict = get_class_distribution(test_dataset,label_id)\n",
    "val_set_class_dict = get_class_distribution(val_dataset,label_id)\n",
    "\n",
    "\n",
    "print(f'class_distribution(train_dataset): {train_set_class_dict}')\n",
    "print(f'class_distribution(test_dataset): {test_set_class_dict}')\n",
    "print(f'class_distribution(val_dataset): {val_set_class_dict}' )\n",
    "    \n",
    "#number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip']    \n",
    "print(f'Number of frames for training datasets {len(train_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "print(f'Number of frames for testing datasets {len(test_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "print(f'Number of frames for Validation datasets {len(val_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "\n",
    "plot_title_train_label= f'TRAIN dataset of {len(train_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_test_label= f'TEST dataset of {len(test_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_val_label= f'VALIDATION dataset of {len(val_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,7))\n",
    "plot_from_dict(train_set_class_dict, plot_title=plot_title_train_label, ax=axes[0])\n",
    "plot_from_dict(val_set_class_dict, plot_title=plot_title_test_label, ax=axes[1])\n",
    "plot_from_dict(test_set_class_dict, plot_title=plot_title_val_label, ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d225",
   "metadata": {},
   "source": [
    "## 5. Displayting frames in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d8805e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-04T16:40:27.010785Z",
     "start_time": "2022-08-04T16:40:26.847287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "train_dataset.__len__() = 74\n",
      "====================================================\n",
      "len(train_dataloader): 4 BATCHES of BATCH_SIZE_OF_CLIPS 20\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 0 / 3\n",
      "    sample_batched_labels.size(): torch.Size([20])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([20])\n",
      "    sample_batched_images.size(): torch.Size([20, 1, 1, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 4 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 5 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 6 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 7 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 8 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 9 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 10 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 11 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 12 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 13 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 14 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 15 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 16 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 17 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 18 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 19 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 1 / 3\n",
      "    sample_batched_labels.size(): torch.Size([20])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([20])\n",
      "    sample_batched_images.size(): torch.Size([20, 1, 1, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 4 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 5 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 6 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 7 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 8 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 9 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 10 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 11 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 12 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 13 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 14 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 15 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 16 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 17 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 18 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 19 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 2 / 3\n",
      "    sample_batched_labels.size(): torch.Size([20])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([20])\n",
      "    sample_batched_images.size(): torch.Size([20, 1, 1, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 4 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 5 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 6 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 7 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 8 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 9 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 10 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 11 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 12 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 13 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 14 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 15 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 16 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 17 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 18 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 19 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 3 / 3\n",
      "    sample_batched_labels.size(): torch.Size([14])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([14])\n",
      "    sample_batched_images.size(): torch.Size([14, 1, 1, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 4 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 5 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 6 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 7 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 8 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 9 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 10 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 11 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 12 \n",
      "          label: 1\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "        BATCH_SIZE_IDX 13 \n",
      "          label: 0\n",
      "          Sample_batched_idx_image.size()  torch.Size([1, 1, 128, 128])\n",
      "          Grid size torch.Size([3, 128, 128])\n",
      "====================================================\n",
      " test_dataset.__len__() = 24\n",
      "====================================================\n",
      " validation_dataset.__len__() = 16\n"
     ]
    }
   ],
   "source": [
    "print(f'====================================================')\n",
    "print(f'train_dataset.__len__() = {train_dataset.__len__()}')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f'len(train_dataloader): {len(train_dataloader)} BATCHES of BATCH_SIZE_OF_CLIPS {BATCH_SIZE_OF_CLIPS}')\n",
    "for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "    print(f'  ====================================================')\n",
    "    sample_batched_images=sample_batched[0]\n",
    "    sample_batched_labels=sample_batched[1]\n",
    "    print(f'    BATCH_OF_CLIPS_INDEX : {clip_batch_idx} / {len(train_dataloader) - 1}')\n",
    "    print(f'    sample_batched_labels.size(): {  sample_batched_labels.size()  }')\n",
    "    print(f'    sample_batched_labels.squeeze().size(): {  sample_batched_labels.squeeze().size()  }')\n",
    "    print(f'    sample_batched_images.size(): {sample_batched_images.size()}')\n",
    "\n",
    "    for BATCH_SIZE_IDX, label in enumerate(sample_batched_labels):\n",
    "        print(f'        BATCH_SIZE_IDX {BATCH_SIZE_IDX} ')\n",
    "        print(f'          label: {label}')\n",
    "        sample_batched_idx_image = sample_batched_images[BATCH_SIZE_IDX,...]\n",
    "        print(f'          Sample_batched_idx_image.size()  {sample_batched_idx_image.size() }'  )\n",
    "\n",
    "        grid = utils.make_grid(sample_batched_idx_image)\n",
    "        print(f'          Grid size {grid.size()}' )\n",
    "#         plt.figure(figsize =(20,20) )\n",
    "#         plt.imshow( grid.cpu().detach().numpy().transpose(1, 2, 0) )\n",
    "#         plt.title(f'BATCH_SIZE_IDX {BATCH_SIZE_IDX}; Label: {label_id[label]}')\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f' test_dataset.__len__() = {test_dataset.__len__()}')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f' validation_dataset.__len__() = {val_dataset.__len__()}')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e92336",
   "metadata": {},
   "source": [
    "## 6. Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369a416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-29T15:52:48.060037Z",
     "start_time": "2022-03-29T15:52:48.046766Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5aafb41",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3791b35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-04T16:40:28.550982Z",
     "start_time": "2022-08-04T16:40:27.011988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "MobileNetV2(\n",
      "  (stem_conv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (last_conv): Sequential(\n",
      "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout2d(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=2, bias=True)\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      ")\n",
      " /home/mx19/repositories/echocardiography/data/models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 0 \n",
      "   X_train_batch.size(): torch.Size([16, 1, 1, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([16])\n",
      "==================================================\n",
      "==================================================\n",
      "{'BKGR': 8, '4CV': 8}\n",
      "y_true_list[1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1]\n",
      "y_pred_list[0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "model=MobileNetV2(ch_in=1, n_classes=2)#Total params: 2,225,858 ## Training curves look good\n",
    "## PRINT MODEL\n",
    "print(f'====================================================')\n",
    "print(model)\n",
    "\n",
    "print(f' {FULL_REPO_MODEL_PATH}')\n",
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\")))\n",
    "model.to(DEVICE) # Place model on GPU\n",
    "model.eval()\n",
    "\n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clip_batch_idx, sample_batched in enumerate(val_dataloader):\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(DEVICE), sample_batched[1].to(DEVICE)\n",
    "        print(f'==================================================')\n",
    "        print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        y_test_pred = model(X_train_batch)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)        \n",
    "        \n",
    "        for i in range(len(y_test_pred)):\n",
    "            y_true_list.append(y_train_batch[i].cpu().item())\n",
    "            y_pred_list.append(y_pred_tag[i].cpu().item())\n",
    "            \n",
    "        \n",
    "print(f'==================================================')        \n",
    "print(f'==================================================')        \n",
    "print(get_class_distribution(val_dataset, label_id))\n",
    "print(f'y_true_list{y_true_list}')\n",
    "print(f'y_pred_list{y_pred_list}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "781a2101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-04T16:40:28.645297Z",
     "start_time": "2022-08-04T16:40:28.551873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75         8\n",
      "           1       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.75      0.75      0.75        16\n",
      "weighted avg       0.75      0.75      0.75        16\n",
      "\n",
      "                    Precision  Recall  F1-score  Support\n",
      "0                        0.75    0.75      0.75      8.0\n",
      "1                        0.75    0.75      0.75      8.0\n",
      "weighted avg/Total       0.75    0.75      0.75      NaN\n",
      "Avg/Total                 NaN     NaN       NaN     16.0\n",
      "[[6 2]\n",
      " [2 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Predicted'), Text(0, 0.5, 'True')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "report = classification_report(y_true_list, y_pred_list)\n",
    "print(report)\n",
    "# report_support = precision_recall_fscore_support(y_true_list, y_pred_list)\n",
    "# print(report_support)\n",
    "\n",
    "def metrics_report_to_df(ytrue, ypred):\n",
    "    classification_report_df = pd.DataFrame(data=list(precision_recall_fscore_support(y_true_list, y_pred_list)), \\\n",
    "                                         index=['Precision', 'Recall', 'F1-score', 'Support']).T    \n",
    "    classification_report_df.loc['weighted avg/Total', :] = precision_recall_fscore_support(ytrue, ypred, average='weighted')\n",
    "    classification_report_df.loc['Avg/Total', 'Support'] = classification_report_df['Support'].sum()\n",
    "    return(classification_report_df)\n",
    "\n",
    "classification_report_df = metrics_report_to_df(y_true_list, y_pred_list)\n",
    "print(classification_report_df)\n",
    "\n",
    "#################################\n",
    "### PLOTTING CONFUSION MATRIX\n",
    "cm=confusion_matrix(y_true_list, y_pred_list)\n",
    "print(cm)\n",
    "#cm=confusion_matrix(y_true_list, y_pred_list, normalize='all')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['BGR','4CV'])\n",
    "cmd.plot()\n",
    "cmd.ax_.set(xlabel='Predicted', ylabel='True')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607176b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:44:50.478014Z",
     "start_time": "2022-03-28T16:44:50.470241Z"
    }
   },
   "source": [
    "\n",
    "## File size of models: \n",
    "```\n",
    "$HOME/repositories/echocardiography/data/models$ tree -s\n",
    "\n",
    ".\n",
    " [  201010447]  metric_model.pth\n",
    " [  351538511]  VGG00v00_metric_model.pth\n",
    " [  752946511]  VGG00v01_metric_model.pth\n",
    " [  201010511]  VGG00v02_metric_model.pth\n",
    "\n",
    "0 directories, 4 files\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9403d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72faf859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
