{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef885fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:05.695513Z",
     "start_time": "2021-12-17T09:18:03.125501Z"
    }
   },
   "source": [
    "# Evaluation of learning pipeline\n",
    "\n",
    "**Author**: Miguel Xochicale [@mxochicale](https://github.com/mxochicale)     \n",
    "**Contributors**: Nhat Phung Tran Huy [@huynhatd13](https://github.com/huynhatd13); Hamideh Kerdegari [@hamidehkerdegari](https://github.com/hamidehkerdegari);  Alberto Gomez [@gomezalberto](https://github.com/)  \n",
    "\n",
    "Feb2022; March2022 \n",
    "\n",
    "\n",
    "## Summary\n",
    "This notebook presents a learning pipeline to classify 4 chamber view from echocardiography datasets.\n",
    "\n",
    "### How to run the notebook\n",
    "\n",
    "1. Go to echocardiography repository path: `$HOME/repositories/echocardiography/`\n",
    "2. Open echocardiography repo in pycharm and in the terminal type:\n",
    "    ```\n",
    "    git checkout master # or the branch\n",
    "    git pull # to bring a local branch up-to-date with its remote version\n",
    "    ```\n",
    "3. Launch Notebook server  \n",
    "    Go to you repository path: `cd $HOME/repositories/echocardiography/scripts/dataloaders` and type in the pycharm terminal:\n",
    "    ```\n",
    "    conda activate rt-ai-echo-VE \n",
    "    jupyter notebook\n",
    "    ```\n",
    "    which will open your web-browser.\n",
    "    \n",
    "    \n",
    "### References\n",
    "* \"Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback\". https://www.fda.gov/media/122535/download \n",
    "* Gomez A. et al. 2021 https://github.com/vital-ultrasound/lung/blob/main/multiclass_pytorch/datasets/LUSVideoDataset.py \n",
    "* Kerdegari H. et al. 2021 https://github.com/vital-ultrasound/lung/tree/main/multiclass_tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2aa0",
   "metadata": {},
   "source": [
    "# Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d3efa",
   "metadata": {},
   "source": [
    "## 1. Setting imports and datasets paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f345efe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:40:22.886337Z",
     "start_time": "2022-03-28T16:40:22.135908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 1.9.0\n",
      "Torchvision Version: 0.10.0a0\n",
      "FULL_PATH_FOR_YML_FILE: /home/mx19/repositories/echocardiography/scripts/config_files//users_paths_files/config_users_paths_files_username_mx19.yml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML #to be used with HTML(animation.ArtistAnimation().to_jshtml())\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from source.dataloaders.EchocardiographicVideoDataset import EchoClassesDataset\n",
    "#from source.models.learning_misc import train_loop, test_loop, basicVGGNet\n",
    "from source.helpers.various import concatenating_YAML_via_tags, plot_dataset_classes\n",
    "\n",
    "HOME_PATH = os.path.expanduser(f'~')\n",
    "USERNAME = os.path.split(HOME_PATH)[1]\n",
    "CONFIG_FILES_PATH= 'repositories/echocardiography/scripts/config_files//users_paths_files'\n",
    "YML_FILE =  'config_users_paths_files_username_' + USERNAME + '.yml'\n",
    "FULL_PATH_FOR_YML_FILE = os.path.join(HOME_PATH, CONFIG_FILES_PATH, YML_FILE)\n",
    "\n",
    "yaml.add_constructor('!join', concatenating_YAML_via_tags)  ## register the tag handler\n",
    "with open(FULL_PATH_FOR_YML_FILE, 'r') as yml:\n",
    "    config = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "    \n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')    \n",
    "print(f'FULL_PATH_FOR_YML_FILE: {FULL_PATH_FOR_YML_FILE}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:08.264310Z",
     "start_time": "2021-12-17T09:18:08.250178Z"
    }
   },
   "source": [
    "## 2. Setting variables and loading datasets using pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5109aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:40:24.239270Z",
     "start_time": "2022-03-28T16:40:23.774410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "[ERROR] [EchoClassesDataset.__init__()] Error reading /home/mx19/datasets/vital-us/echocardiography/videos-echo-annotated/01NVb-003-074/T3/01NVb-003-074-3-4CV.json (empty). Removing from list\n",
      "[ERROR] [EchoClassesDataset.__init__()] Error reading /home/mx19/datasets/vital-us/echocardiography/videos-echo-annotated/01NVb-003-077/T1/01NVb-003-077-1-4CV.json (empty). Removing from list\n",
      "[ERROR] [EchoClassesDataset.__init__()] Error reading /home/mx19/datasets/vital-us/echocardiography/videos-echo-annotated/01NVb-003-077/T2/01NVb-003-077-2-4CV.json (empty). Removing from list\n",
      "[ERROR] [EchoClassesDataset.__init__()] Error reading /home/mx19/datasets/vital-us/echocardiography/videos-echo-annotated/01NVb-003-077/T3/01NVb-003-077-3-4CV.json (empty). Removing from list\n",
      "169 19 188\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # \"cuda:NN\" can also be used\n",
    "print(f'Device: {device}')\n",
    "\n",
    "pretransform_im_size = config['pretransform_im_size']\n",
    "\n",
    "# Defining transforms that apply to the entire dataset.\n",
    "# These transforms are not augmentation.\n",
    "if config['use_pretransform_image_size']:\n",
    "    pretransform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=pretransform_im_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    pretransform = None\n",
    "\n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_train_augmentation']:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),  # in degrees\n",
    "        transforms.RandomEqualize(p=0.5),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_validation_augmentation']:\n",
    "    val_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=5),  # in degrees\n",
    "    #transforms.RandomEqualize(p=0.5),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "\n",
    "\n",
    "train_dataset = EchoClassesDataset(\n",
    "    main_data_path=config['main_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_train'],\n",
    "    participant_path_json_list=config['participant_path_json_list_train'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    number_of_frames_per_segment_in_a_clip=config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=device,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    pretransform=pretransform,\n",
    "    transform=train_transform,\n",
    "    use_tmp_storage=True,\n",
    "    )\n",
    "\n",
    "validation_dataset = EchoClassesDataset(\n",
    "    main_data_path=config['main_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_validation'],\n",
    "    participant_path_json_list=config['participant_path_json_list_validation'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    number_of_frames_per_segment_in_a_clip=config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=device,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    pretransform=pretransform,\n",
    "    transform=val_transform,\n",
    "    use_tmp_storage=True,\n",
    "    )\n",
    "\n",
    "\n",
    "## Spliting train_dataset into train_set and test_set\n",
    "Ntdt = train_dataset.__len__()\n",
    "ntraining = 0.9\n",
    "\n",
    "Ntrain=round(Ntdt*ntraining)\n",
    "Ntest = round(Ntdt - (Ntdt*ntraining))\n",
    "print(Ntrain, Ntest, Ntrain+Ntest)\n",
    "train_set, test_set = torch.utils.data.random_split(train_dataset, [Ntrain, Ntest])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760906",
   "metadata": {},
   "source": [
    "## 3. Plotting Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6472a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:40:35.341507Z",
     "start_time": "2022-03-28T16:40:26.858620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BKGR': 84, '4CV': 85}\n",
      "{'BKGR': 13, '4CV': 13}\n",
      "{'BKGR': 10, '4CV': 9}\n",
      "Number of frames for training datasets 10140\n",
      "Number of frames for testing datasets 1140\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDYAAAG5CAYAAAB1DbTmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABLgklEQVR4nO3deZhcZZX48e8hCUuAQIAOhjWAiBhQllZBHEAQBUThpyigaHRUZlxGYJhBYFxwBR1HQRSZiArIEjCiILIatlFZTABZDIqyBgJpwpKwBzi/P97bodL0ltDV1bf7+3meeqrufm7VvafeOnWXyEwkSZIkSZLqaLlWByBJkiRJkrSsLGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKw0U8RkRHx6lbHMZAi4lMR8VBEPBERa7Y6nu5ExN0R8fbq9VERcXKL4vhQRFzay/CdI2LOYMb0SkXEDhFxR/X579PqeOqm62ceEbdFxM6ti2hkMBc3LYajI+L06vUGVSyjWhTLExGxcS/DF38v1EFErBQRv4mIxyPiF62Op46GSltAzRMRH42I37c6joE0VPb9obL/2JZWVwPdlm5JYaP68DsfL0bE0w3dH6oaWIuq7sci4o8RsX038zklIp6PiHW69F/cQKu6MyJuiYjlGvp9PSJOacK6TaqWN3qg5z2Qy4mIMcB3gXdk5iqZOb+bcaZGxF+rz+ij3QzfOCIuiIiFEfFwRHy7YdjmEXF5lcz/HhH/b1nibJSZ38zMT7zS+Szjss/IzHd0dg/Ej6uIeHtE3BART0bEfRHxgYZhW0XErIh4qnre6pUsqwdfBX5Qff6/bsL8R5TMnJyZV7Y6joGwlDm68/FYw/R7R8RNEbGgyg0zqpx1UsP4z3WZx0UDvA4jIhdHxAoR8b2IeCAiHo2IE6t5LrPMvLeK5YVXMp9XsPxVMvNOWPw9//VXMr+IaIuIM6v2xKMRcUbDsBUi4qfVtvpgRPz7K42/G/sCawNrZub7mzD/EaWVbYHeRMQlEfHVbvrvXW1bo6vunauccXiX8XrMJT20a5+scuf8Ksfu1810ERF3RsRfGvpd1JB3F1W5uLP7pOjmx2VE7BUR11fLnB8RZ0TEeg3DP1rF9J9dppsTTSj4d30/mmUAltPrvh8RU6o23oLqvfp2188/IvaPiNnVe/+PiPinVxCPbemBZ1t6AL3StnRLChvVh79KZq4C3Au8u6FfZ4Pj7Gr4WsAVwBKVzohYGXgf8DjwoX4sdh1g/wFbifpbG1gRuK2Xcf4MfBq4oeuAiFgeuAy4HHgVsB7Q+W/faOA84AJgDeAg4PSIeM0Axl9rEfE64Ezgv4DVgK2AWdWw5Snv3+nAeOBU4Lyq/0DakB4+/6oxNCKO6BpJ69pfS5OjGx6rA1SNlNOAwyjb9kbAicCLmfmvDfP9Zpd57DHY6zlEvKJcDBwBtANbAK8BtgG+MMAx1t25wIOUnDcB+E7DsKOBTathbwMOj4jdB3j5GwJ/y8znuxvY7OLbUDLM1/UU4MMREV36fxg4o+HznwI8Uj2/Em+oculm1bJ/EBFf7jLOjpRtfuOIeCNAZu7RkIfPAL7dkIf/tetCImJfSnvleEqbfDLwLPD7iBjfMOojwOcjYtwrXK/hpNd9HxgLHEJ5X98M7Ar8R+fAiNgN+BbwMWBVyud5ZxPjrRXb0kPHkFnXzGzpA7gbeHuXfkcDpzd0vw5IoK2h30eA+4CDgVv7mD6BzwN3AKOrfl8HTuklrv8E5gIPAP9czePV1bB3ATcCC6oYjm6Y7t5q3Ceqx/bAJpQCwHzgYcoXyeoN03weuB9YCPwV2LXqvxyl0fqPatpzgDV6Wk4367ACcFy1Dg9Ur1egNH6fbJj+8j4+o98DH+3S7yDg/3oYf4tqvtHQ71Lga70s45PA7Oo9+AuwTdfto/FzBSZV8R9Urdtc4LCG+b0JmFl9Rg8B3+1huVcB76tev7Wa555V99uBm6rXHwV+X72+uhrvyWo99wN2BuZQfszNq+L5WC/re2ZP7wfwjmp7aHz/7gV272H8U4AfAr+t3r/rgE36+Ez/AbwIPF2twwrAlcA3gD9U/V9N+TLt/FzuBP6lYR6d63x4wzrvA+wJ/I3SyDmqYfzetucVKV8+84HHgD8Ba/exDlcCxwDXUwqc53XOrxq+HfDHan5/BnbuMu0S69rLctYAflZtZ48Cv25c/+5yGWVbnQ6cXb13N1AaoS3PuUv7oB85usuwfan2mz7m2+M8uoxnLn5pPt3l4pnA+xu6Pwjc18s8JlOK0o9QcuNRXT8PXsqvnd+XV9LDvkY/911KLvlNQ/ffgXMauu8DtqpeJyX/HAQsAp6r3p/fNGyT/wHcXMVzNrBiD+v7jmr8UT0Mv59ytExn99eAaT2MuzNLkeerab5Sxb+oWoePU75P/gB8r/ocvk7f2+bdlH3h5mqb+QmlKHYRZXv9HTC+Yfze8t9HKfl8IXAX8KE+1qEz3hOq9/t2qn2jGr5aFc/c6v38euf73d269rGsAW0LDOYDWKl6f3Zs6DceeIYq/1N+yC6k/NH2HNDeMG7nuozuZt6L17lxH+kyzr7VstZs6PfTals6l/Kvctf5ntL1M6Hhuw0I4B7g8C7jLAfcCny14XP+PfAb4MsN481p3Pa6zGNN4HxK/r6esu/9vmH48ZS8sIDyY/Wfqv67s+Q+9eeGHNNTe2Utyp9tj1Xb4f8By1XD1gF+CXRQ9ofP9bacbtZjc0qOfIzyA/c9Pe37/diG/p0l8+Qf+zNds/YfbEvblq5ZW3rQE383K7p4BRr6Hc1LO93ywLGUL/rRDePMAL5N+WJ/nmrn7Tp91Z2Uf2RmAZ+o+vVY2KAks4coP9BXpuw4jY3pnYEtq43r9dW4+3RJEo2xvhrYjbLBt1F25uOqYZtREvc6DdNvUr0+BLiWcjTECsD/Amf1tJxu1uOr1fQTquX+kSoB9Gf6hvl015j+KfBzSqPq4Wrj3rIatiUvL2xcBvyqh/m/n5J83kj5En01sGEPG3jXZHxW9RltSflS6hz3GuDD1etVgO16eY9OqF4fRUkU32oYdnz1+qMs+YW7RKOi2iaer6YZQ0lIT9HQ0Oyy3DspX+K3UJLY6byUmA4FLuoy/gX00FijJONHKF9AoymNmG4b5r3te9VneC/lh8/oaj3eRWlwB7BTtU7bdFnnL1XjfrL6DM6k/LMwmdLI2rgf2/O/UBpEY4FRwLbAuD7iv7Labjr30182bB/rUhL7npT9dLequ62nde1lOb+lJNXx1Xru1LD+vSXjRZSG5hjKj7C7elvOUH103U667ovdjL9x9bl/j/IP+Co9jNfjPBrGMRcvOZ/ucvEs4AMN3R+q5rdaN9OvStVwpTSAVgXe3PXz6BoTve9r/dp3q+3iseqzmkj5sXR/w7BHeemHRuNnfAov/+F1N6URtg6lsTQb+Nce3rMvAZfwUmPvT7y0D4+vlrV2w/j7Arf0MK+dWYo839O2Tvk+eR74N0r+WYlets2Gdb6W0uZZl9IAvgHYuprmcqoflPSS/6rPbwGwWTXuRGByH/F3xntotd77URrAnd9Zv6bsEytTtvHrqRru3a1rL8sZ8LbAYD+AHwMnN3T/Cw2FXsrRG3Mp+8pvgO83DOtcl2UtbIyp3us9qu6x1We9J+UI54eB5btMcwq9FzZeWy1ro25i+gpwTcPn/HvKP+aPNWwbvRU2plF+lK1MyS33s2Q760BK8WM0JWc9SFXA7Pp+VP16a68cA5xUvUdjgH+qxluOkkO/RPm9sTGlffbOnpbTzXv+d0r7cXlgF8oPsM36M3038/s1cGz1ehSlMHJEtYw5wA/oYR/CtrRtadvSQ/rioR+Ics7205QPed+sDuWKiA0oDeYzM/MhSpFjSh/zS+CLwJciYoW+lg38LDNvzcwnKW/sSzPKvDIzb8nMFzPzZkpC2KnHBWf+PTMvy8xnM7ODcj515/gvUDbM10XEmMy8OzP/UQ37F+C/MnNOZj5bxbHvUhzK+SFKNX1etdyvUL5UB8J6lH8cvk9pXP6Wlw7xup3S6PrPiBgTEe+grO/YHub1CcqhkH/K4u+ZeU8/4/hKZj6ZmbdQKoEHVP0XAa+OiLUy84nMvLaH6a/ipc9iR8qXX2f3TtXw/lpEeb8XZeaFlOLOZj2Mux7ls3gfpei2EuXfMChfHo93Gf9xSoLrybmZeX21j5xBaVwsi1My87bMfL5aj99m5j+qz+UqypE3jed3LgK+kZmLKI2UtShfYAsz8zbKvxevr8btbXteRGnAvDozX8jMWZm5oB/x/rxhP/0iJW+MojSILszMC6v99DLKvw579rSu3c08IiYCe1B+ND1avSf93SZmZeb0at7fpfyQ3K6f09bBB6Jcs6DzcQVAlmsj7Ez5QjwHeDjKdRJWWZZlYC7uy0XAwVGuI/Eq4HNV/+7y7V7Ag5n5P5n5TLWfXtfP5fS0r/Vr3622i4WU3LQTpdhwf0S8tur+v8x8sd9rXX4QPpCZj1Aaclv1MN56lH/urqCcNvk/lO+qtSi5FpbMt33l2qXJ8715IDNPqPLP031sm51OyMyHMvN+yj/O12XmjdU2+StKkQP6zn8vAltExEqZObfK1X2ZRym0LMrMsylHNL0rItam5MhDqu/ieZSiZuOpv0usay/LaEZbYLCdCrw/Ilaquj9S9es0hXIK3guUHy4HxCu8Jk6n6rvmYUqxD+C9lFNGLqX8oBtN+YG1NNaqnud2M2xuw/DOGG6qlvf53mZa5Y73AV+qPrdbWfJ9IjNPz8z51XbzP5T83OO+1kd7ZRGliLdhtQ3/X2YmpQjQlplfzcznqjz1Y/p/6vp2lDxybDX95ZT3eqm3v4j4GOW0ws5T5dam/Jjbt1qPrSj7eE+nGtqWti094tvSQ7mwcU6Wc7bXphzutm3DsA8Ds6sECmXj+2BfXw7VDnIv5ZCr3qxD+eeu0xKJISLeHBFXRERHRDwO/CtdknuX8SdExLSIuD8iFlAqimtVMf2dUn07GphXjdd5MdQNgV91/nCg/Cv1AuU96Y91usR+T9VvIDxNqbpelJnPURLxmsDm1ca3D+UL9EFKpf0cSrW5O+tTqrvLouvn1Ll+H6cc5n17RPwpIvbqYfprgNdUjbOtKNcGWL9q9L6J8q9Zf83PJc+jfIqXGs5dPU35wfa3zHyCcr2BzkTxBND1HNVxlB8FPXmwn8vtS+P7SUTsERHXRsQj1Ta4J0tu6/PzpQsMdjZYH2oY/nRDLL1tzz+n/NCZFuUiiN/uZ2Ov6+c/popvQ0rj8rGG5b2V0rDpdl17sD7wSGY+2o9xe4yt+sE2h4Hb/4aCczJz9YbH2zoHZOa1mfmBzGyjfHnvSDkHdmmZi/v2DcrpODdRjgT5NaVxM6+bcQcy13bua0uz715FKXrtWL2+ktLoXdqGL/Q/5z0N3J2ZP6kaU9OqddmBkmthyXzbV65dmjzfm665tsdts0HX3Npbru02/1UN1/0o+8rciPhtVVzqy/3VD8FOndvwhpRtYW7Dsv6XcuRGt+vai2a0BQZVZv6e8m/r3lHu7PNGSgGDiFif8qdc53WKzqM00pe22NCtar9ro/zrDKWIck71g+NZyukoff0J2NXD1fPEboZNbBje6EvAp6pCa0/aKIWW3vL7YVEumvl4tV2tRu/5vbf2yn9Tjnq4NMrFVI+o+m8IrNNlXzmKpcvt9+WSRdl7KIX9fotyN41jKUfbdL6nnW2qE6oC5MOUH3Z7djMLsC1tW9q29JAubABQ7cj/AhxdVXygVMA3jnKV6QcpO/palGpQX75AaWD3dPQAlCr0+g3dG3QZfiblvMD1M3M1yuFtnReLSl7umKr/6zNzHKUCtvjiUpl5Zma+lbIBJeVCQVA+zD26/HhYMcu/Nd0tp6sHqnk2rscD/ZiuP27uLYbMvDkzd8rMNTPznZTD+67vYfT7KIdoLYuun9MD1fLvyMwDKI2rbwHTo1xwtmucT1EOQzyYcq2W5yg/Dv4d+EfDF8xA6+39uw14fcQSFyB7Pb1fXHCgLI4pypFNv6QUrdauCo0X0rDtLqUet+fqB8dXMvN1wFso/yx/pB/z7Pr5d/5jdR+lAt24rJUz89ju1rWPmNeIiNX7s4I9xRblgkrrMXD7X21k5p8oDeotlmFyc3Efsvzb/9nMXDczN6YcJjoru7+jyUDm2kXAw0u573YWNv6pet35L19vhY3+vL+96THXVo2sucAbGnq/gUHOtZVet82l1Gv+y8xLMnM3SuP0dso/1H1Zt8t3Uuc2fB/lqIC1GpY1LjMnN4zb389wwNsCLXIaZR/4MHBpliOLqbqXA35TtV3vpBQ2+vNd1x97Uw5pvz7KHUt2AQ5saCvvC+xZ/djsr79Sfki8v7Fn9Z32PsoR00vIzNspOf+oXubbUcXabX6PcuePz1OO2htftT8ep4f83ld7Jcs/34dVOfLdwL9HxK6Ube6uLvvKqpm5Z3fL6cYDlB/wjb+nNqAc2t8vUS5W/GPKRbpv6exf5ac5/Yihk23pJdmWHoFt6SFf2IDFSfISytXKOy8A9yZKVXArSoP5TPpRic5yC5lb+hj3HOCjEfG6iBgLfLnL8FUpladnIuJNlIu1deqgHOa5cZfxnwAei4h1KRcAAyAiNouIXaoN/xlKVa6zQXoS8I2I2LAaty0i9u5lOV2dBXyhmm4tShX99F7GX0JELB8RK1J2vjERsWJD8j4d2C7KbZZGUf7pfJhSOSQiXl+NPzYi/oPSgDqlh0WdDPxHRGwbxas717kfvlgtYzLl4jxnV8s/MCLaqureY9W4Pd268Crgs7zUsL6yS3d3HqL3974vPwM+FuWWuWMpX+AXNCz/BeBzUW5F+Nmq/+WvYHnLYnnKoZ8dwPMRsQflkO5l1eP2HBFvi4gtq21pASWp9udWkwc27KdfBaZXP+hOB94dEe+MiFHVtrhzNNyerj8ycy7lUP8TI2J8lFOrduzn5NtGxHujHB54CKXx39NhnMNGRLw1Ij4ZEROq7tcC72HZ1t1cTO+5OCLWjYh1qty5HeUw0q7vU6cLgFdFxCFVblk1It7czzC63deWct+9ivKP9UqZOYdyOsXulKP9buxhmleaa38FjI9yW8VRUe7wsC7lYmdQfoR+odq/X0s59fWUV7C8ZdXjtrkMesx/EbF2RLyn+nHybLXM/uTaCZTvpDER8X7KBRMvrHLkpcD/RMS4iFguIjaJiB5PCevFgLcFWuQ0ygUTP8mSp1d8hHIa2lYNj/dRTulZs2G8FarPrPPRa1s9ItaIiA9RLnz4rSy3jP4w5cKDmzUs6zWUH8r9Pk2iOkrnPyj7yAcjYqUoR2KcTPn3+3s9TPoVyueweg/zfYFS/Di6+txex5Lt8lUphY8OYHREfIkl/31/CJjU8N702l6JcrvaV0dEUPLUC9XjemBBRHy+WrdREbFFVHeQ6WY5XV1Hufjl4dW+sTOlcDKth/GXEBG7UI7geV9mdvfn38+Af4tyRNd4Slvigm7GA9vStqVtS9ejsFH5b8opJJ8EzstyXvWDnQ/K1ZP3iog1ep1L8QVeOgfxZTLzIspV6y+nHLrWdSf4NPDViFhIaaCe0zDtU1RXiI1y2M52lAS/DaXa/FtKMu+0Ai9dHPVBSuOhs8p9POXfyEurZV1LuR1UT8vp6uuUc6FuphRzbqj69dellMb9W4Cp1esdq+X/lfKP0kmUi77tTbkS9HPVtJ0XyJpHuX3VblkOhXyZzPxFtS5nUg4R+zW9fD5dXEX5jGYA38nMS6v+uwO3RcQTlPdx/8x8ppd5rMpLh8p17e7O0cCp1Xv/gX7Gulhm/pTS+LmOctjXs1Tnxlfv4T6URtBjlDtB7NPw3g6KzFxYxXQO5TP+IGV7XFY9bs+Uc9+nUxLxbMpn0J8ffj+n/Ah5kPLPV+d7eB9lmzyK8mVyH+WHwrLkvA9Tvhw6rx1zSD+nO49yyPej1Tzemz2cf1hT+0XEE10eEyjb7HuAW6r972LKj8tvL+0CzMWL9ZiLKYX+P1Ia16cCRzTkwSVU+/RulIb3g5Q7hb2tu3G70e2+xlLsu5n5N8oP6f+ruhdQ/rX+Qw9HmEC528brqvf21/2MtXGZj1C2x/+gfO5HAHs3/IP4Zcrh2/dUsf93Zl68tMsZAL1tm0ulj/y3HOX00AcopyzsRNmP+nId5Rz2hynb+77VD2go31XLU+7C8Chle+ju1IW+4m5GW2DQZebdlH1yZarvzCovTAJ+2Nh2zczzKXE3FhueoOzjnY9deljUn6sc+3fK9RUOzcwvVcOmACd2WdaDlDbbUp2OkuWaKh+mXIzxYcrnvBKwQ8M20HWauyg542X/7jf4LOXw+gcpueVnDcMuofwQ+htl33yGJQ95/0X1PD8ibuhHe2VTyp2DnqCcNnFilms0vUDJh1tRLkr4MKVAsFp3y+lmPZ+j5Jc9qmlPBD5S/SHbH1+slnVhw/foRQ3Dv0a54PHfKPn1Rso+8jK2pW1LY1u63LVCqpuImMRLV8Z9vo/RNQxFxJWUKzef3OpYuoqIoykXbzqw1bFIr9RQ3tfUfBHxUcod5d7a6li6si0gLTv3Hw3l7/dlaUvX6YgNSZIkSZKkJVjYkIapiPinbk4VeKI6pLAWeoo/yoXFarccScNTRNzWQw75UKtj64+IOKmH+E+q43IkaSDYlh56y+k1Bk9FkSRJkiRJdeURG5IkSZIkqbZGtzqA/lhrrbVy0qRJrQ5DkpYwa9ashzOzrdVxDAbzsKShylwsSa01FPJwLQobkyZNYubMma0OQ5KWEBH3tDqGwWIeljRUmYslqbWGQh72VBRJkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1VYtrbEhqvUWLFjFnzhyeeeaZVocy6FZccUXWW289xowZ0+pQJI1w5mJzsaTWMg8PzTxsYUNSv8yZM4dVV12VSZMmERGtDmfQZCbz589nzpw5bLTRRq0OR9IIZy42F0tqLfPw0MzDnooiqV+eeeYZ1lxzzRGVwAEigjXXXHNEVuUlDT3mYnOxpNYyDw/NPGxhQ1K/jbQE3mmkrrekoWmk5qSRut6Shp6Rmo+G8npb2JAkSZIkSbVlYUPSkLTnnnvy2GOP9TrOKqus0m3/j370o0yfPr0JUUnSyGIulqTWMg/3jxcPlTSkZCaZyYUXXtjqUCRpxDIXS1JrmYeXjkdsSGqKz3/+85x44omLu48++mi+8pWvsOuuu7LNNtuw5ZZbct555wFw9913s/nmm/PpT3+abbbZhvvuu49Jkybx8MMPA7DPPvuw7bbbMnnyZKZOnbrEcg477DC22WYbdt11Vzo6Ol4Wx6xZs9hpp53Ydttteec738ncuXObuNaSNLSYiyWptczDg6SzEjSUH9tuu21Kaq2//OUvSzX+DTfckDvuuOPi7s033zzvueeefPzxxzMzs6OjIzfZZJN88cUX86677sqIyGuuuWbx+BtuuGF2dHRkZub8+fMzM/Opp57KyZMn58MPP5yZmUCefvrpmZn5la98JT/zmc9kZuaUKVPyF7/4RT733HO5/fbb57x58zIzc9q0afmxj31sWVa/2/UHZuYQyJGD8TAPS0ODudhcLKm1zMNDMw97Koqkpth6662ZN28eDzzwAB0dHYwfP56JEydy6KGHcvXVV7Pccstx//3389BDDwGw4YYbst1223U7r+9///v86le/AuC+++7jjjvuYM0112S55ZZjv/32A+DAAw/kve997xLT/fWvf+XWW29lt912A+CFF15g4sSJzVplSRpyzMWS1Frm4cFhYUNS0+y7775Mnz6dBx98kP33358zzjiDjo4OZs2axZgxY5g0adLie2GvvPLK3c7jyiuv5He/+x3XXHMNY8eOZeedd+7x/tldb0GVmUyePJlrrrlmYFesRiLip8BewLzM3KLqtwZwNjAJuBv4QGY+2qoYJTWXuViSWss83HxeY0NS0+y///5MmzaN6dOns++++/L4448zYcIExowZwxVXXME999zT5zwef/xxxo8fz9ixY7n99tu59tprFw978cUXF1/p+cwzz+Stb33rEtNuttlmdHR0LE7iixYt4rbbbhvANayFU4Ddu/Q7ApiRmZsCM6puScOUuViSWss83HwesSGpaSZPnszChQtZd911mThxIh/60Id497vfTXt7O1tttRWvfe1r+5zH7rvvzkknncTrX/96NttssyUOzVt55ZW57bbb2HbbbVlttdU4++yzl5h2+eWXZ/r06Xzuc5/j8ccf5/nnn+eQQw5h8uTJA76uQ1VmXh0Rk7r03hvYuXp9KnAl8PnBi0rSYDIXS1JrmYebL8q1Poa29vb2nDlzZqvDkEaUe7+65RLdj+92HK/Z8FUtiqa5Vlin76Q+e/ZsNt988yX6RcSszGxvVlwDpSpsXNBwKspjmbl6w/BHM3N8N9MdBBwEsMEGG2zbn38TurPtf562TNPV0az//kirQ9Aw110uGknqnIsHwitpE5uLpaXXtT0MtomHah72VBRJUrcyc2pmtmdme1tbW6vDkSRJkrplYUOSRp6HImIiQPU8r8XxSJIkScvMwoYkjTznA1Oq11OA81oYiyRJkvSKWNiQpGEsIs4CrgE2i4g5EfFx4Fhgt4i4A9it6pYkSZJqadjfFcULJUkayTLzgB4G7TqogUiSJElN4hEbkiRJkiSptob9ERuSmuMt35s1oPP746Hb9jnO2PVfzxav3ZTMZNSoUXzv60ex/Ru35u777ue9Uz7DDZf/GoCfnDGdH592NhedfTLjV1+N4//3VH5yxnTGjBnNchG87a3b8Y3/OpQxY8bwmje/g3Grr0FEMH78eE477TQ23HDDAV03SWqWgT4ytT9Hf44aNYott9xycS7+wQ9+wFve8hbuvvtu9tprL2699VYAfvzjH/OjH/2IGTNmMH78eL773e8ydepUxowZw3LLLceuu+7Kt771LcaMGcOkSZNYddVVzcWSasc28dDgERuSamOlFVfg+st+yZ9+dy5fO/IQvnjs8S8b54zp5/Ojn53BBWdNZfzqq/Hj087md1f/kat/cwazZvyKP1x4Nm1rrcHTzzy7eJorrriCm2++mZ133pmvf/3rg7lKklQ7K620EjfddBN//vOfOeaYYzjyyCNfNs7Pf/5zTjjhBC699FLGjx/PSSedxKWXXsq1117LLbfcwp/+9CcmTJjA008/vXgac7Ek9Y9t4pezsCGplhYsfILxq41bot/08y/mOz/8CRec+WPWWmM8AMd+fyrfP+aLrF6Nu/zyY/jPz36Ccauu8rJ5br/99tx///3ND16ShokFCxYwfvz4Jfqdc845HHvssVx66aWstdZaAHzjG9/gRz/6EauvvjoAyy+/PEcccQTjxo3rOktzsSQtBdvEhaeiSKqNp595ljft9j6eefY5HpzXwcXn/GTxsHvnPMChX/gm117yC141oTSkFz7xJE8+9TQbbbBev+Z/8cUXs88++zQjdEkaNp5++mm22mornnnmGebOncvll1++eNg999zDZz/7WW688UZe9apXAbBw4UKeeOIJNtpoo37N31wsSb2zTfxyHrEhqTY6D7u7+erfcP7pJ/Hxg48iMwFYa801WH/dV/HL31yyePzMJOKl6S+78g+8abf38Zo3v4Nr/nTj4v5ve9vbmDBhAr/73e/44Ac/OGjrI0l11Hkqyu23387FF1/MRz7ykcW5uK2tjQ022IBzzjln8fglF7+UjC+55BK22morJk2axB//+MfF/c3FktQ/tolfzsKGpFrarn0r5j/yKB3zHwFg7Eorct7pJ/Hjn5/DWedeAMC4VVdh7Eorcde9cwDYbecduP6yXzJ5s1fz3KJFi+d1xRVXcM899zB58mS+9KUvDf7KSFJNbb/99jz88MN0dHQAMHbsWC666CJOOukkzjjjDADGjRvHyiuvzF133QXAO9/5Tm666Sa22GILnnvuucXzMhdL0tKzTVxY2JBUS3/9+5288MKLrDl+9cX92tZcg/PPOIkvHXs8l135BwAO/+wn+dyRX+OxxxcApWL9zLPPvWx+K620EscddxynnXYajzzyyKCsgyTV3e23384LL7zAmmuuubhfW1sbF198MUcddRSXXFL+MTzyyCP51Kc+xWOPPQZUufiZZ142P3OxJC0d28SF19iQtEz6cyuqgdZ5PiGUZHzycd9g1KhRS4yz0Qbr8cufncA+H/k00358HAdN2Y+nnnmaf9rrg6ywwhhWGTuW7d+4NVttsfnL5j9x4kQOOOAAfvjDH/LFL35xUNZJkl6J/tyedaB1XmMDSi4+9dRTX56LN9qI888/nz333JNzzz2XT33qUzz11FO8+c1vZoUVVmCVVVZhhx12YOutt37Z/M3FkurENvHQEJ3n4gxl7e3tOXPmzGWadqDv7z6UtaJxo+Hr3q9uuUT347sdx2s2fFWLommuFdaZ3Oc4s2fPZvPNl0z8ETErM9ubFddQYh7uH/Owmq27XDSSmIvNxf1hLtZA6doeBtvEQzUPeyqKJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2mpqYSMiDo2I2yLi1og4KyJWjIg1IuKyiLijeh7fzBgkSZIkSdLw1bTCRkSsC3wOaM/MLYBRwP7AEcCMzNwUmFF1S5IkSZIkLbVmn4oyGlgpIkYDY4EHgL2BU6vhpwL7NDkGSZIkSZI0TI1u1owz8/6I+A5wL/A0cGlmXhoRa2fm3GqcuRExobvpI+Ig4CCADTbYoFlhSlpGD528/4DOb+1PTOv3uC+88AJv2WM/1nnVBH512okAfO+kn/GzM89l9OhRjFpuOQ7+lyncde8cnn3uOb5+5KGLp/3zrbfzkc/8J3++6jcDGr8ktUJ3tyJ8JTb40i39Gu+FF16gvb2dddddlwsuuACA73znO5x88smMHj2aUaNGcdhhh3HnnXfy7LPPcswxxyye9qabbuKAAw5g9uzZAxq7JLWCbeKhoZmnooynHJ2xEbAOsHJEHNjf6TNzama2Z2Z7W1tbs8KUVEM/OPl0Ntt048XdPz7tbGZcfQ2//+1Z3HD5r/nduaeSCfvtvSfTz79kiWl/cf5F7LfPuwY7ZEkaVo4//ng233zzxd0nnXQSl112Gddffz233norV199NZnJAQccwNlnn73EtNOmTeODH/zgYIcsScOObeKXNPNUlLcDd2VmR2YuAs4F3gI8FBETAarneU2MQdIwM+eBB7loxtV87ID3Le73rRN+zPHf/ALjVl0FgNXGrcqHP7A3r3n1Rqw+blWuv+HmxeNO/80lfGDvPQY9bkkaLubMmcNvf/tbPvGJTyzu981vfpMTTzyRcePGAbDaaqsxZcoUNttsM1ZffXWuu+66xeOec8457L//wP7DKUkjjW3iJTWzsHEvsF1EjI2IAHYFZgPnA1OqcaYA5zUxBknDzH9++Vt88wv/znLLBQALn3iSJ558ik0mdX/K2gf22YNfnHcRANfN+jNrjl+NV2+84aDFK0nDzSGHHMK3v/1tlluuNCMXLlzIwoUL2WSTTbod/4ADDmDatHJo9bXXXsuaa67JpptuOmjxtlpE/DQi5kXErQ39/jsibo+ImyPiVxGxegtDlFRDtomX1LTCRmZeB0wHbgBuqZY1FTgW2C0i7gB2q7olqU8XXnYlbWutwTavn7y4X2YS0fM073/PHpz720t58cUX+cV5F/GBvfcchEglaXi64IILmDBhAttuu+3ifiUP95yI999/f6ZPn86LL77ItGnTOOCAAwYj1KHkFGD3Lv0uA7bIzNcDfwOOHOygJNWXbeKXa9rFQwEy88vAl7v0fpZy9IYkLZU/zryR3156JRdf/n88++yzLFj4JJ876muMXWkl7rznPjbecP2XTbP+uhPZcP11ufqamfzqwsu46vwzWhC5JA0Pf/jDHzj//PO58MILeeaZZ1iwYAGf/vSnWXnllbnzzjvZeOONXzbN+uuvz6RJk7jqqqv45S9/yTXXXNOCyFsnM6+OiEld+l3a0HktsO+gBiWp1mwTv1yzb/cqSQPm60ceyj9mzeBv113KaSf+Nzvv8CZOOeFbHP7ZT3LIf32DBQufAGDBwic4+fRfLJ5uv7335PCjv8XGk9ZnvXVe1arwJan2jjnmGObMmcPdd9/NtGnT2GWXXTj99NM58sgj+cxnPsOCBQsAWLBgAVOnTl083QEHHMChhx7KJptswnrrrdeq8IeqfwYu6mlgRBwUETMjYmZHR8cghiVpqLJN/HJNPWJD0vC1NLeiaraDpuzHE089xQ577s+YMaMZM3o0B//LlMXD3/vud3DYl4/le1/zSF9Jw0t/b8/abJ/61Kd44okneOMb38iYMWMYM2YMhx122OLh73//+zn44IM54YQTWhjl0BMR/wU8D/T412lmTqWczk17e3sOUmiS+sk28dBgYUNSLe30ljex01veBEBEcNin/5nDPv3P3Y7btuYaPHHPTYMYnSQNfzvvvDM777wzUPLw4YcfzuGHH97tuG1tbSxatGgQoxv6ImIKsBewa2ZasJC0TGwTFxY2JEmSpEEUEbsDnwd2ysynWh2PJNWd19iQJEmSmiQizgKuATaLiDkR8XHgB8CqwGURcVNEnNTSICWp5jxiQ1I/ZZ+39BuuPEJY0lBiLq6XzOzu/rY/GfRAJA0Q28RDkUdsSOqXUQvu47EnnxvSCa0ZMpP58+ez4oortjoUSWLFFVdk/vz55mJJahHbxEMzD3vEhqR+GXvjj3mET9Ixbn1geFWoRz/ee413xRVX9PaEkoaE9dZbjzlz5jASb/tpLpY0FNgmHpp52MKGpH5Z7rmFrHLdd1sdRlMMldslSlJfxowZw0YbbdTqMCRpxLJNPDRZ2JAkSRqC7v3qlq0OYdDUuTEtSWo9r7EhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2mlbYiIjNIuKmhseCiDgkItaIiMsi4o7qeXyzYpAkSZIkScNb0wobmfnXzNwqM7cCtgWeAn4FHAHMyMxNgRlVtyRJkiRJ0lIbrFNRdgX+kZn3AHsDp1b9TwX2GaQYJEmSJEnSMDNYhY39gbOq12tn5lyA6nlCdxNExEERMTMiZnZ0dAxSmJIkSZIkqU6aXtiIiOWB9wC/WJrpMnNqZrZnZntbW1tzgpMkSZIkSbU2GEds7AHckJkPVd0PRcREgOp53iDEIEmSJEmShqHBKGwcwEunoQCcD0ypXk8BzhuEGCRJkiRJ0jDU1MJGRIwFdgPObeh9LLBbRNxRDTu2mTFIkiRJkqTha3QzZ56ZTwFrduk3n3KXFEmSJEmSpFdksO6KIkkaYiLi0Ii4LSJujYizImLFVsckSZIkLS0LG5I0AkXEusDngPbM3AIYRbk1tyRJklQrFjYkaeQaDawUEaOBscADLY5HkiRJWmoWNiRpBMrM+4HvAPcCc4HHM/PSxnEi4qCImBkRMzs6OloRpiRJktQnCxuSNAJFxHhgb2AjYB1g5Yg4sHGczJyame2Z2d7W1taKMCVJkqQ+WdiQpJHp7cBdmdmRmYsot+V+S4tjkiRJkpaahQ1JGpnuBbaLiLEREZTbcM9ucUySJEnSUrOwIUkjUGZeB0wHbgBuoXwfTG1pUJIkSdIyGN3qACRJrZGZXwa+3Oo4JEmSpFfCIzYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJapKI+GlEzIuIWxv6rRERl0XEHdXz+FbGKEl1Z2FDkiRJap5TgN279DsCmJGZmwIzqm5J0jKysCFJkiQ1SWZeDTzSpffewKnV61OBfQYzJkkabixsSJIkSYNr7cycC1A9T+hpxIg4KCJmRsTMjo6OQQtQkurEwoYkSZI0RGXm1Mxsz8z2tra2VocjSUOShQ1JkiRpcD0UERMBqud5LY5HkmrNwoYkSZI0uM4HplSvpwDntTAWSao9CxuSJElSk0TEWcA1wGYRMSciPg4cC+wWEXcAu1XdkqRlNLrVAUiSJEnDVWYe0MOgXQc1EEkaxjxiQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFtNLWxExOoRMT0ibo+I2RGxfUSsERGXRcQd1fP4ZsYgSZIkSZKGr2YfsXE8cHFmvhZ4AzAbOAKYkZmbAjOqbkmSJEmSpKXWtMJGRIwDdgR+ApCZz2XmY8DewKnVaKcC+zQrBkmSJEmSNLw184iNjYEO4GcRcWNEnBwRKwNrZ+ZcgOp5QncTR8RBETEzImZ2dHQ0MUxJkiRJklRXzSxsjAa2AX6UmVsDT7IUp51k5tTMbM/M9ra2tmbFKEmSJEmSaqyZhY05wJzMvK7qnk4pdDwUERMBqud5TYxBkiRJkiQNY00rbGTmg8B9EbFZ1WtX4C/A+cCUqt8U4LxmxSBJkiRJkoa30U2e/78BZ0TE8sCdwMcoxZRzIuLjwL3A+5scgyRJkiRJGqaaWtjIzJuA9m4G7drM5UqSJEmSpJGhmdfYkCRJkiRJaioLG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJktQCEXFoRNwWEbdGxFkRsWKrY5KkOrKwIUmSJA2yiFgX+BzQnplbAKOA/VsblSTVk4UNSZIkqTVGAytFxGhgLPBAi+ORpFqysCFJkiQNssy8H/gOcC8wF3g8My/tOl5EHBQRMyNiZkdHx2CHKUm1YGFDkiRJGmQRMR7YG9gIWAdYOSIO7DpeZk7NzPbMbG9raxvsMCWpFixsSNIIFRGrR8T0iLg9ImZHxPatjkmSRpC3A3dlZkdmLgLOBd7S4pgkqZZGtzoASVLLHA9cnJn7RsTylPO7JUmD415gu4gYCzwN7ArMbG1IklRPFjYkaQSKiHHAjsBHATLzOeC5VsYkSSNJZl4XEdOBG4DngRuBqa2NSpLqyVNRJGlk2hjoAH4WETdGxMkRsXLjCF6wTpKaKzO/nJmvzcwtMvPDmflsq2OSpDqysCFJI9NoYBvgR5m5NfAkcETjCF6wTpIkSXXQ1MJGRNwdEbdExE0RMbPqt0ZEXBYRd1TP45sZgySpW3OAOZl5XdU9nVLokCRJkmplMI7YeFtmbpWZ7VX3EcCMzNwUmEGXfwglSc2XmQ8C90XEZlWvXYG/tDAkSZIkaZm04uKhewM7V69PBa4EPt+COCRppPs34Izqjih3Ah9rcTySJEnSUmt2YSOBSyMigf/NzKnA2pk5FyAz50bEhO4mjIiDgIMANthggyaHKUkjT2beBLT3NZ4kSZI0lDW7sLFDZj5QFS8ui4jb+zthVQSZCtDe3p7NClCSJEmSJNVXU6+xkZkPVM/zgF8BbwIeioiJANXzvGbGIEmSJEmShq+mFTYiYuWIWLXzNfAO4FbgfGBKNdoU4LxmxSBJkiRJkoa3Zp6Ksjbwq4joXM6ZmXlxRPwJOCciPg7cC7y/iTFIkiRJkqRhrGmFjcy8E3hDN/3nU24rKEmSJEmS9Io09RobkiRJkiRJzWRhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNVWn4WNiFg7In4SERdV3a+LiI83PzRJUidzsSS1lnlYkoau/hyxcQpwCbBO1f034JAmxSNJ6t4pmIslqZVOwTwsSUNSfwoba2XmOcCLAJn5PPBCU6OSJHVlLpak1jIPS9IQ1Z/CxpMRsSaQABGxHfB4U6OSJHVlLpak1jIPS9IQNbof4/w7cD6wSUT8AWgD9m1qVJKkrszFktRa5mFJGqL6LGxk5g0RsROwGRDAXzNzUdMjkyQtZi6WpNYyD0vS0NVnYSMiPtKl1zYRQWae1qSYJEldmIslqbXMw5I0dPXnVJQ3NrxeEdgVuAEwiUvS4DEXS1JrmYclaYjqz6ko/9bYHRGrAT9vWkSSpJcxF0tSa5mHJWno6s9dUbp6Cth0oAORJC0Vc7EktZZ5WJKGiP5cY+M3VLe1ohRCXgec08ygJElLMhdLUmuZhyVp6OrPNTa+0/D6eeCezJzTpHgkSd0zF0tSa5mHJWmI6s81Nq4ajEAkST0zF0tSa5mHJWno6rGwERELeelwuyUGAZmZ45oWlSQJMBdLUquZhyVp6OuxsJGZqw5mIJKklzMXS1JrmYclaejrzzU2AIiICZR7dgOQmfc2JSJJUo/MxZLUWuZhSRp6+rzda0S8JyLuAO4CrgLuBi5qclySpAbmYklqLfOwJA1dfRY2gK8B2wF/y8yNgF2BPzQ1KklSV+ZiSWot87AkDVH9KWwsysz5wHIRsVxmXgFs1dywJEldmIslqbXMw5I0RPXnGhuPRcQqwP8BZ0TEPMq9uyVJg8dcLEmtZR6WpCGqP0dsXA2sDhwMXAz8A3h3E2OSJL2cuViSWss8LElDVH8KGwFcAlwJrAKcXR2GJ0kaPOZiSWot87AkDVF9FjYy8yuZORn4DLAOcFVE/K7pkUmSFjMXS1JrmYclaejqzxEbneYBDwLzgQnNCUeS1AdzsSS1lnlYkoaYPgsbEfGpiLgSmAGsBXwyM1/f7MAkSS8xF0tSa5mHJWno6s9dUTYEDsnMm5ociySpZ+ZiSWot87AkDVF9FjYy84jBCESS1DNzsSS1lnlYkoaupbnGxjKJiFERcWNEXFB1rxERl0XEHdXz+GbHIEmSJEmShqemFzYo9/qe3dB9BDAjMzelnKNo9VuSJEmSJC2TphY2ImI94F3AyQ299wZOrV6fCuzTzBgkSZIkSdLw1ewjNo4DDgdebOi3dmbOBaieu71NVkQcFBEzI2JmR0dHk8OUJEmSBldErB4R0yPi9oiYHRHbtzomSaqjphU2ImIvYF5mzlqW6TNzama2Z2Z7W1vbAEcnSZIktdzxwMWZ+VrgDSx5+rYkqZ/6c7vXZbUD8J6I2BNYERgXEacDD0XExMycGxETgXlNjEGSJEkaciJiHLAj8FGAzHwOeK6VMUlSXTXtiI3MPDIz18vMScD+wOWZeSBwPjClGm0KcF6zYpAkSZKGqI2BDuBn1R0ET46IlVsdlCTV0WDcFaWrY4HdIuIOYLeqW5IkSRpJRgPbAD/KzK2BJ+nmboFed06S+jYohY3MvDIz96pez8/MXTNz0+r5kcGIQZIkSRpC5gBzMvO6qns6pdCxBK87J0l9a8URG5IkSdKIlpkPAvdFxGZVr12Bv7QwJEmqrWZePFSSJElSz/4NOCMilgfuBD7W4ngkqZYsbEiSJEktkJk3Ae2tjkOS6s5TUSRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSRrCIGBURN0bEBa2ORZIkSVoWFjYkaWQ7GJjd6iAkSZKkZWVhQ5JGqIhYD3gXcHKrY5EkSZKWlYUNSRq5jgMOB17sbmBEHBQRMyNiZkdHx6AGJkmSJPWXhQ1JGoEiYi9gXmbO6mmczJyame2Z2d7W1jaI0UmSJEn9Z2FDkkamHYD3RMTdwDRgl4g4vbUhSZIkSUvPwoYkjUCZeWRmrpeZk4D9gcsz88AWhyVJkiQtNQsbkiRJkiSptka3OgBJUmtl5pXAlS0OQ5IkSVomHrEhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNpqWmEjIlaMiOsj4s8RcVtEfKXqv0ZEXBYRd1TP45sVgyRJkiRJGt6aecTGs8AumfkGYCtg94jYDjgCmJGZmwIzqm5JkiRJkqSl1rTCRhZPVJ1jqkcCewOnVv1PBfZpVgySJEmSJGl4a+o1NiJiVETcBMwDLsvM64C1M3MuQPU8oYdpD4qImRExs6Ojo5lhSpIkSZKkmmpqYSMzX8jMrYD1gDdFxBZLMe3UzGzPzPa2tramxShJkiRJkuprUO6KkpmPAVcCuwMPRcREgOp53mDEIEmSJEmShp9m3hWlLSJWr16vBLwduB04H5hSjTYFOK9ZMUiSJEmSpOFtdBPnPRE4NSJGUQoo52TmBRFxDXBORHwcuBd4fxNjkCRJkiRJw1jTChuZeTOwdTf95wO7Nmu5kiRJkiRp5BiUa2xIkiRJkiQ1g4UNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZJaJCJGRcSNEXFBq2ORpLqysCFJkiS1zsHA7FYHIUl1ZmFDkiRJaoGIWA94F3Byq2ORpDqzsCFJkiS1xnHA4cCLPY0QEQdFxMyImNnR0TFogUlSnVjYkCRJkgZZROwFzMvMWb2Nl5lTM7M9M9vb2toGKTpJqhcLG5IkSdLg2wF4T0TcDUwDdomI01sbkiTVk4UNSZIkaZBl5pGZuV5mTgL2By7PzANbHJYk1ZKFDUmSJEmSVFujWx2AJEmSNJJl5pXAlS0OQ5JqyyM2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNWWhQ1JkiRJklRbFjYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm01rbAREetHxBURMTsibouIg6v+a0TEZRFxR/U8vlkxSJIkSZKk4a2ZR2w8DxyWmZsD2wGfiYjXAUcAMzJzU2BG1S1JkiRJkrTUmlbYyMy5mXlD9XohMBtYF9gbOLUa7VRgn2bFIEmSJEmShrdBucZGREwCtgauA9bOzLlQih/AhMGIQZIkSZIkDT9NL2xExCrAL4FDMnPBUkx3UETMjIiZHR0dzQtQkiRJkiTVVlMLGxExhlLUOCMzz616PxQRE6vhE4F53U2bmVMzsz0z29va2poZpiRJkiRJqqlm3hUlgJ8AszPzuw2DzgemVK+nAOc1KwZJkiRJkjS8jW7ivHcAPgzcEhE3Vf2OAo4FzomIjwP3Au9vYgySJEmSJGkYa1phIzN/D0QPg3dt1nIlSZIkSdLIMSh3RZEkSZIkSWoGCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJI1AEbF+RFwREbMj4raIOLjVMUmSJEnLopm3e5UkDV3PA4dl5g0RsSowKyIuy8y/tDowSZIkaWl4xIYkjUCZOTczb6heLwRmA+u2NipJkiRp6VnYkKQRLiImAVsD13Xpf1BEzIyImR0dHS2JTZIkSeqLhQ1JGsEiYhXgl8AhmbmgcVhmTs3M9sxsb2tra02AkiRJUh8sbEjSCBURYyhFjTMy89xWxyNJkiQtCwsbkjQCRUQAPwFmZ+Z3Wx2PJEmStKwsbEjSyLQD8GFgl4i4qXrs2eqgJEmSpKXl7V4laQTKzN8D0eo4JEmSpFfKIzYkSZIkSVJtWdiQJEmSJEm1ZWFDkiRJkiTVloUNSZIkSZJUWxY2JEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkqRBFhHrR8QVETE7Im6LiINbHZMk1dXoVgcgSZIkjUDPA4dl5g0RsSowKyIuy8y/tDowSaobj9iQJEmSBllmzs3MG6rXC4HZwLqtjUqS6snChiRJktRCETEJ2Bq4rpthB0XEzIiY2dHRMeixSVIdWNiQJEmSWiQiVgF+CRySmQu6Ds/MqZnZnpntbW1tgx+gJNWAhQ1JkiSpBSJiDKWocUZmntvqeCSprixsSJIkSYMsIgL4CTA7M7/b6ngkqc4sbEiSJEmDbwfgw8AuEXFT9diz1UFJUh15u1dJkiRpkGXm74FodRySNBx4xIYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqq2mFjYj4aUTMi4hbG/qtERGXRcQd1fP4Zi1fkiRJkiQNf808YuMUYPcu/Y4AZmTmpsCMqluSJEmSJGmZNK2wkZlXA4906b03cGr1+lRgn2YtX5IkSZIkDX+DfY2NtTNzLkD1PKGnESPioIiYGREzOzo6Bi1ASZIkSZJUH0P24qGZOTUz2zOzva2trdXhSJIkSZKkIWiwCxsPRcREgOp53iAvX5IkSZIkDSODXdg4H5hSvZ4CnDfIy5ckSZIkScNIM2/3ehZwDbBZRMyJiI8DxwK7RcQdwG5VtyRJkiRJ0jIZ3awZZ+YBPQzatVnLlCRJkiRJI8uQvXioJEmSJElSXyxsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTasrAhSZIkSZJqy8KGJEmSJEmqLQsbkiRJkiSptixsSJIkSZKk2rKwIUmSJEmSasvChiRJkiRJqi0LG5IkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxIkiRJkqTaaklhIyJ2j4i/RsTfI+KIVsQgSSOduViSWss8LEkDY9ALGxExCvghsAfwOuCAiHjdYMchSSOZuViSWss8LEkDpxVHbLwJ+Htm3pmZzwHTgL1bEIckjWTmYklqLfOwJA2Q0S1Y5rrAfQ3dc4A3dx0pIg4CDqo6n4iIvw5CbLUW35myFvBwq+PQsDFytqcvx7JOueFAhjHI+szF5uGlZx7WABs529Oy52Goby62Tdwk5mINoJG1LdW4TdyKwkZ371a+rEfmVGBq88MZPiJiZma2tzoODQ9uT8Nen7nYPLz03G80kNyehj3bxE3ivqOB4rZUH604FWUOsH5D93rAAy2IQ5JGMnOxJLWWeViSBkgrCht/AjaNiI0iYnlgf+D8FsQhSSOZuViSWss8LEkDZNBPRcnM5yPis8AlwCjgp5l522DHMUx5mKIGktvTMGYubhr3Gw0kt6dhzDzcVO47GihuSzURmS87lU+SJEmSJKkWWnEqiiRJkiRJ0oCwsCFJkiRJkmrLwsYQFREvRMRNEfHniLghIt5S9Z8UEbc2jPfJavj4qvvfI+L2iLilmva7ETGmGnZ31f/miLgqIlp+v2G1TkSMiogbI+KChn7/UW0/t1bbz0ci4uiIOKbLtFtFxOzBj1oaPOZhNZt5WOqbuVjNZi4eHixsDF1PZ+ZWmfkG4EjgmK4jRMSHgX8D3pGZj0bEvwLvALbLzC2BNwLzgJUaJntbZr4euBL4QpPXQUPbwcDiRFxtP7sBb8rMLYAdgQDOAvbrMu3+wJmDFKfUKuZhNZt5WOqbuVjNZi4eBixs1MM44NHGHhHxAeAISgJ/uOr9X8CnMvMxgMx8LjOPzcwF3czzGmDd5oWsoSwi1gPeBZzc0Pso4NOd20tmPp6Zp2bmX4HHIuLNDeN+AJg2aAFLrWce1oAyD0vLxFysAWUuHj4G/Xav6reVIuImYEVgIrBLw7ANgR8AW2fmgwARsSqwSmbe1c/57w78esCiVd0cBxwOrAqLt59VM/MfPYx/FqUifV1EbAfMz8w7BiNQqYXMw2qm4zAPS/1hLlYzHYe5eFjwiI2hq/Owu9dSEu5pERHVsA7gXkqFsFMAi+/dGxHvrM5HvLvzXMTKFRExD3g7HjY1IkXEXsC8zJzV2JuG7acb04B9I2I5SjI/q4khSkOFeVhNYR6Wloq5WE1hLh5eLGzUQGZeA6wFtFW9ngL2AP41Ij5UjbMAeDIiNqq6L8nMrYBbgeUbZvc2SnX7NuCrg7ICGmp2AN4TEXdTkvMuwImU7Wfj7ibIzPuAu4GdgPcB5wxKpNIQYR7WADMPS8vAXKwBZi4eRixs1EBEvBYYBczv7JeZHZSq9Tcj4p1V72OAH0XE6tV0QTlsbwmZ+TRwCPCRiFijqcFryMnMIzNzvcycRKk0X56ZB1K2nx9GxDiAiBgXEQc1THoW8D3gH5k5Z7DjllrJPKyBZB6Wlo25WAPJXDy8eI2NoavzfEIoh0RNycwXXjryDjLzroh4D3BhRLwX+BEwlnLO17PAE8AfgBu7zjwz50bEWcBngK81dU1UFz8CVgH+FBGLgEXA/zQM/wVwPOWq49JIYB7WYDMPSy9nLtZgMxfXUGT2dgqRJEmSJEnS0OWpKJIkSZIkqbYsbEiSJEmSpNqysCFJkiRJkmrLwoYkSZIkSaotCxuSJEmSJKm2LGxo2IuICzvvY97LOE/00P+UiNi3KYFJ0ghhHpak1jMXazgb3eoApGaJcoPzyMw9Wx2LJI1E5mFJaj1zsUYCj9jQkBcR34qITzd0Hx0RX46IGRFxQ0TcEhF7V8MmRcTsiDgRuAFYPyLujoi1quG/johZEXFbRBzUZTn/U81vRkS0dRPHthFxVTX9JRExsblrLklDg3lYklrPXCz1zMKG6mAasF9D9weAnwH/LzO3Ad4G/E9VjQbYDDgtM7fOzHu6zOufM3NboB34XESsWfVfGbihmt9VwJcbJ4qIMcAJwL7V9D8FvjFgayhJQ5t5WJJaz1ws9cBTUTTkZeaNETEhItYB2oBHgbnA9yJiR+BFYF1g7WqSezLz2h5m97mI+H/V6/WBTYH51TzOrvqfDpzbZbrNgC2Ay6rvilFVDJI07JmHJan1zMVSzyxsqC6mA/sCr6JUqz9ESejbZuaiiLgbWLEa98nuZhAROwNvB7bPzKci4sqGabrKrpMDt2Xm9su+CpJUa+ZhSWo9c7HUDU9FUV1MA/anJPLpwGrAvCqBvw3YsB/zWA14tErgrwW2axi2XDVvgA8Cv+8y7V+BtojYHspheBExeZnXRpLqxzwsSa1nLpa64REbqoXMvC0iVgXuz8y5EXEG8JuImAncBNzej9lcDPxrRNxMScqNh+Y9CUyOiFnA4yx5/iKZ+VyUW1x9PyJWo+w7xwG3vbI1k6R6MA9LUuuZi6XuRWbXo4skSZIkSZLqwVNRJEmSJElSbVnYkCRJkiRJtWVhQ5IkSZIk1ZaFDUmSJEmSVFsWNiRJkiRJUm1Z2JAkSZIkSbVlYUOSJEmSJNXW/wdLK9/6JYbv2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_id = ('BKGR', '4CV')\n",
    "number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip'] \n",
    "\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_class_dict = {\n",
    "   'BKGR': 0 ,\n",
    "   \"4CV\": 0\n",
    "    }\n",
    "    \n",
    "    for clip_index_i in range(len(dataset_obj)):\n",
    "        data_idx = dataset_obj[clip_index_i]\n",
    "        label_id_idx = data_idx[1]\n",
    "        label = label_id[label_id_idx]\n",
    "        count_class_dict[label]+= 1\n",
    "        #count_class_dict[label]+= 1* number_of_frames_per_segment_in_a_clip\n",
    "\n",
    "    return count_class_dict\n",
    "        \n",
    "        \n",
    "def plot_from_dict(dict_obj, plot_title, **kwargs):\n",
    "    return sns.barplot(data = pd.DataFrame.from_dict([dict_obj]).melt(), \n",
    "                       x = \"variable\", y=\"value\", hue=\"variable\", **kwargs).set_title(plot_title)\n",
    "\n",
    "\n",
    "print(get_class_distribution(train_set))\n",
    "print(get_class_distribution(validation_dataset))\n",
    "print(get_class_distribution(test_set))\n",
    "    \n",
    "number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip']    \n",
    "print(f'Number of frames for training datasets {Ntrain*number_of_frames_per_segment_in_a_clip}')\n",
    "print(f'Number of frames for testing datasets {Ntest*number_of_frames_per_segment_in_a_clip}')\n",
    "#print(f'Number of frames for training datasets {Ntrain*number_of_frames_per_segment_in_a_clip}')\n",
    "\n",
    "plot_title_train_label= f'TRAIN dataset of {len(train_set)} clips with {number_of_frames_per_segment_in_a_clip} n_frames_per_clip'\n",
    "plot_title_test_label= f'TEST dataset of {len(test_set)} clips with {number_of_frames_per_segment_in_a_clip} n_frames_per_clip'\n",
    "plot_title_val_label= f'VALIDATION dataset of {len(validation_dataset)} clips with {number_of_frames_per_segment_in_a_clip} n_frames_per_clip'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,7))\n",
    "plot_from_dict(get_class_distribution(train_set), plot_title=plot_title_train_label, ax=axes[0])\n",
    "plot_from_dict(get_class_distribution(test_set), plot_title=plot_title_test_label, ax=axes[1])\n",
    "plot_from_dict(get_class_distribution(validation_dataset), plot_title=plot_title_val_label, ax=axes[2])\n",
    "plt.show()\n",
    "\n",
    "###########\n",
    "## NOTES ##\n",
    "# HamidehK on Thu 23 Mar 14:00:00 GMT 2022\n",
    "    # 65 clips with 60 frames each create 3900 frames which is a low number for traininig data.\n",
    "    # Hamideh recomends to increase the training data or perhaps reduce the convs to one!\n",
    "\n",
    "# MiguelX on Mon 28 Mar 11:35:55 BST 2022\n",
    "    # Make use of 169 clips with 60 frames which results in 10140 frames for training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d225",
   "metadata": {},
   "source": [
    "## 5. Displayting frames in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d8805e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:40:38.664997Z",
     "start_time": "2022-03-28T16:40:35.342716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train_dataset.__len__() = 169\n",
      " test_dataset.__len__() = 19\n",
      " validation_dataset.__len__() = 26\n",
      "====================================================\n",
      "len(train_dataloader): 43\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 0 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 1 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 2 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 3 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 4 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 5 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 6 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 7 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 8 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 9 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 10 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 11 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 12 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 13 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 14 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 15 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 16 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 17 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 18 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 19 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 20 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 21 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 22 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 23 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 24 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 25 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 26 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 27 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 28 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 29 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 30 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 31 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 32 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 33 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 34 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 35 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 36 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 37 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 38 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 39 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 40 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 41 / 42\n",
      "    sample_batched_labels.size(): torch.Size([4])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([4])\n",
      "    sample_batched_images.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 1 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 2 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "        BATCH_SIZE_IDX 3 label: 0\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n",
      "  ====================================================\n",
      "    BATCH_OF_CLIPS_INDEX : 42 / 42\n",
      "    sample_batched_labels.size(): torch.Size([1])\n",
      "    sample_batched_labels.squeeze().size(): torch.Size([])\n",
      "    sample_batched_images.size(): torch.Size([1, 1, 60, 128, 128])\n",
      "        BATCH_SIZE_IDX 0 label: 1\n",
      "        Sample_batched_idx_image.size()  torch.Size([1, 60, 128, 128])\n",
      "        Grid size torch.Size([60, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "##### Setting up BATCH_SIZE_OF_CLIPS\n",
    "BATCH_SIZE_OF_CLIPS = 4\n",
    "##############################\n",
    "##############################\n",
    "\n",
    "print(f' train_dataset.__len__() = {train_set.__len__()}')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "print(f' test_dataset.__len__() = {test_set.__len__()}')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f' validation_dataset.__len__() = {validation_dataset.__len__()}')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f'len(train_dataloader): {len(train_dataloader)}')\n",
    "for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "    print(f'  ====================================================')\n",
    "    sample_batched_images=sample_batched[0]\n",
    "    sample_batched_labels=sample_batched[1]\n",
    "    print(f'    BATCH_OF_CLIPS_INDEX : {clip_batch_idx} / {len(train_dataloader) - 1}')\n",
    "    print(f'    sample_batched_labels.size(): {  sample_batched_labels.size()  }')\n",
    "    print(f'    sample_batched_labels.squeeze().size(): {  sample_batched_labels.squeeze().size()  }')\n",
    "    print(f'    sample_batched_images.size(): {sample_batched_images.size()}')\n",
    "\n",
    "    for BATCH_SIZE_IDX, label in enumerate(sample_batched_labels):\n",
    "        print(f'        BATCH_SIZE_IDX {BATCH_SIZE_IDX} label: {label}')\n",
    "        sample_batched_idx_image = sample_batched_images[BATCH_SIZE_IDX,...]\n",
    "        print(f'        Sample_batched_idx_image.size()  {sample_batched_idx_image.size() }'  )\n",
    "\n",
    "        grid = utils.make_grid(sample_batched_idx_image)\n",
    "        print(f'        Grid size {grid.size()}' )\n",
    "#         plt.figure(figsize =(20,20) )\n",
    "#         plt.imshow( grid.cpu().detach().numpy().transpose(1, 2, 0) )\n",
    "#         plt.title(f'BATCH_SIZE_IDX {BATCH_SIZE_IDX}; Label: {label_id[label]}')\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e92336",
   "metadata": {},
   "source": [
    "## 6. Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e369a416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:40:58.036647Z",
     "start_time": "2022-03-28T16:40:58.024032Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "################################\n",
    "##### Define VGG00 architecture\n",
    "class VGG00(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, n_classes=2):\n",
    "        \"\"\"\n",
    "        Simple Video classifier to classify into two classes:\n",
    "        Args:\n",
    "            input_size:  shape of the input image. Should be a 2 element vector for a 2D video (width, height) [e.g. 128, 128].\n",
    "            n_classes: number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        super(VGG00, self).__init__()\n",
    "        self.name = 'VGG00'\n",
    "        self.input_size = input_size\n",
    "        self.n_classes = n_classes\n",
    "        self.n_frames_per_clip = config['number_of_frames_per_segment_in_a_clip']\n",
    "        self.n_features = np.prod(self.input_size)*self.n_frames_per_clip\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.conv0 = nn.Conv3d(in_channels=1, out_channels=64,\n",
    "                               kernel_size = (1, 3, 3),  ## (-depth, -height, -width)\n",
    "                               stride =      (4, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "                               padding =     (0, 0, 0)\n",
    "                               )\n",
    "        #NOTES\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html\n",
    "        #IN: [N,Cin,D,H,W]; OUT: (N,Cout,Dout,Hout,Wout)\n",
    "        #[batch_size, channels, depth, height, width].\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=64, out_channels=128,\n",
    "                               kernel_size = (1, 3, 3),  # (-depth, -height, -width)\n",
    "                               stride =      (4, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "                               padding =     (0, 0, 0)\n",
    "                               )\n",
    "        \n",
    "#         self.conv2 = nn.Conv3d(in_channels=128, out_channels=256,\n",
    "#                                kernel_size =  (1, 3, 3),  # (-depth, -height, -width)\n",
    "#                                stride =       (1, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding =      (0, 0, 0)\n",
    "#                                )\n",
    "        \n",
    "        \n",
    "#         self.conv3 = nn.Conv3d(in_channels=256, out_channels=512,\n",
    "#                                kernel_size=(2, 2, 2),  # (-depth, -height, -width)\n",
    "#                                stride=(2, 2, 2), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding = (0, 0, 0)\n",
    "#                                )\n",
    "        \n",
    "        \n",
    "#         self.pool0 = nn.MaxPool3d(\n",
    "#                                 kernel_size = (1, 3, 3),  # (-depth, -height, -width)\n",
    "#                                 stride =      (1, 1, 1), \n",
    "#                                 padding =     (0, 0, 0), \n",
    "#                                 dilation =    (1, 1, 1)\n",
    "#                                 )\n",
    "#         #NOTES\n",
    "#         #Keeps the training to 50% after 100 epochs\n",
    "\n",
    "\n",
    "        self.fc0 = nn.Linear(in_features=100352, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=self.n_classes)\n",
    "        #self.fc1 = nn.Linear(in_features=2048, out_features=self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f'x.shape(): {x.size()}') ##[batch_size, channels, depth, height, width]\n",
    "        \n",
    "        x = F.relu( self.conv0(x) )\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape(): x.shape(): torch.Size([2, 64, 60, 128, 128]) with kernel_size=(1, 1, 1)\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape():torch.Size([2, 64, 51, 29, 29]) with kernel_size=(10, 100, 100)\n",
    "        #print(f'conv0.size(): {x.size()}')\n",
    "        \n",
    "        x = F.relu( self.conv1(x) )\n",
    "        #print(f'x.shape(): {x.size()}') with kernel_size=(1, 10, 10) #x.shape(): torch.Size([2, 32, 60, 20, 20])\n",
    "        #print(f'conv1.size(): {x.size()}')\n",
    "        \n",
    "        #x = F.relu( self.conv2(x) )\n",
    "        #x = F.relu( self.conv3(x) )\n",
    "        \n",
    "        #x = self.pool0(x)\n",
    "        #print(f'x.pool0..shape(): {x.size()}') \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        #print(f'self.flatten(x) size() {x.size()}') #x.shape(): torch.Size([4, 983040])\n",
    "        x = self.fc0(x)\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape(): torch.Size([4, 32])\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=0.5) #dropout was included to combat overfitting\n",
    "        \n",
    "        #print(f'x.shape(): {x.size()}') # x.shape(): torch.Size([4, 2])\n",
    "        #x = self.sigmoid(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        #print(f'x.shape(): {x.size()}')  #x.shape(): torch.Size([4, 2])\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "################################\n",
    "##### Define VGG architecture\n",
    "class basicVGGNet(nn.Module):\n",
    "\n",
    "    def __init__(self, tensor_shape_size, n_classes=2, cnn_channels=(1, 16, 32)):\n",
    "        \"\"\"\n",
    "        Simple Visual Geometry Group Network (VGGNet) to classify two US image classes (background and 4CV).\n",
    "\n",
    "        Args:\n",
    "            tensor_shape_size: [Batch_clips, Depth, Channels, Height, Depth]\n",
    "\n",
    "        \"\"\"\n",
    "        super(basicVGGNet, self).__init__()\n",
    "        self.name = 'basicVGGNet'\n",
    "\n",
    "        self.tensor_shape_size = tensor_shape_size\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # define the CNN\n",
    "        self.n_output_channels = cnn_channels ##  self.n_output_channels::: (1, 16, 32)\n",
    "        self.kernel_size = (3, ) * (len(cnn_channels) -1) ## self.kernel_size::: (3, 3)\n",
    "\n",
    "        self.n_batch_size_of_clip_numbers = self.tensor_shape_size[0]\n",
    "        self.n_frames_per_clip = self.tensor_shape_size[1]\n",
    "        self.n_number_of_image_channels = self.tensor_shape_size[2]\n",
    "        self.input_shape_tensor = self.n_batch_size_of_clip_numbers * self.n_frames_per_clip * self.n_number_of_image_channels\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=self.n_number_of_image_channels, out_channels=64,\n",
    "                               kernel_size=(1, 1, 1), stride=(1, 1, 1), padding = (0, 0, 0)\n",
    "                               )\n",
    "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=2,\n",
    "                               kernel_size=(1, 1, 1), stride=(1, 1, 1), padding = (0, 0, 0)\n",
    "                               )\n",
    "                    #IN: [N,Cin,D,H,W]; OUT: (N,Cout,Dout,Hout,Wout)\n",
    "                    #[batch_size, channels, depth, height, width].\n",
    "\n",
    "        self.pool0 = nn.MaxPool3d(kernel_size=(60, 128, 128), stride=(1, 1, 1), padding=(0, 0, 0), dilation=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 0, 0))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=(3, 3, 3), padding=(0, 0, 0))\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(16, 16, 16), stride=(16, 16, 16), padding=(0, 0, 0))\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=(32, 32, 32), stride=(32, 32, 32), padding=(0, 0, 0))\n",
    "        self.pool6 = nn.MaxPool3d(kernel_size=(30, 30, 30), stride=(30, 30, 30), padding=(0, 0, 0))\n",
    "        self.pool7 = nn.MaxPool3d(kernel_size= 60, stride= 60, padding=0)\n",
    "        self.pool8 = nn.MaxPool3d(kernel_size=(64, 64, 64), stride=(64, 64, 64), padding=(0, 0, 0))\n",
    "        self.pool9 = nn.MaxPool3d(kernel_size=(128, 128, 128), stride=(128, 128, 128), padding=(0, 0, 0))\n",
    "\n",
    "        self.bn1 = nn.BatchNorm3d(num_features = 64)\n",
    "        self.bn2 = nn.BatchNorm3d(num_features = 12)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc0 = nn.Linear(in_features=10, out_features=self.n_classes)\n",
    "        self.fc1 = nn.Linear(in_features=62914560, out_features=self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f'x.shape(): {x.size()}') ##[batch_size, channels, depth, height, width]\n",
    "        \n",
    "        #x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(f'x.shape(): {x.size()}')\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(f'x.shape():: {x.size()}')\n",
    "        x = self.pool0(x)\n",
    "        #print(f'x.shape()::: {x.size()}')\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"After flattening, x.shape: {x.shape}\")\n",
    "\n",
    "        # x = x.reshape(x.shape[0], -1)\n",
    "        # x = F.dropout(x, p=0.5) #dropout was included to combat overfitting\n",
    "        #x = self.fc0(x)\n",
    "        #print(f\"After fc1, x.shape: {x.shape}\")\n",
    "\n",
    "        return x    \n",
    "\n",
    "    \n",
    "    \n",
    "###########\n",
    "## NOTES ##\n",
    "# Miguel on Mon 28 Mar 15:47:26 BST 2022\n",
    "    #adding more conv3d might create reduced number of pixel size\n",
    "    #RuntimeError: Given input size: (256x60x4x4). Calculated output size: (256x60x-5x-5). Output size is too small\n",
    "    \n",
    "\n",
    "# Miguel on Thu 24 Mar 09:30:09 GMT 2022\n",
    "    # Implementations of conv3D are nearly finalised. I will then move on to conv2D with indivial frames to create \n",
    "    # sketchs comparison metrics \n",
    "        \n",
    "# Alberto on Thu 22 Mar 09:00:00 GMT 2022\n",
    "    # The use of conv3D might consume more resources vs conv2D with indivial frames (MX: maybe there is a Trade-off here)\n",
    "    # For features per frame, AG prototyped the following: \n",
    "    #     def forward(self, data):\n",
    "\n",
    "    #         n_frames = data.shape[2]\n",
    "    #         features = []\n",
    "    #         for f in range(n_frames):\n",
    "    #             feat_i = self.frame_features(data[:, :, f, ...])\n",
    "    #             features.append(feat_i)\n",
    "\n",
    "    #         feature_vector = torch.stack(features, dim=1)\n",
    "\n",
    "    #         out = self.classifier(feature_vector)\n",
    "    #         return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aafb41",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3791b35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:41:35.779615Z",
     "start_time": "2022-03-28T16:41:33.237905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "VGG00(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (conv0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(4, 3, 3))\n",
      "  (conv1): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(4, 3, 3))\n",
      "  (fc0): Linear(in_features=100352, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=2, bias=True)\n",
      ")\n",
      " /home/mx19/tmp/model\n",
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 0 \n",
      "   X_train_batch.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59431/3550200339.py:99: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 1 \n",
      "   X_train_batch.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([4])\n",
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 2 \n",
      "   X_train_batch.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([4])\n",
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 3 \n",
      "   X_train_batch.size(): torch.Size([4, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([4])\n",
      "==================================================\n",
      " BATCH_OF_CLIPS_INDEX: 4 \n",
      "   X_train_batch.size(): torch.Size([3, 1, 60, 128, 128])\n",
      "   y_train_batch.size(): torch.Size([3])\n",
      "==================================================\n",
      "==================================================\n",
      "{'BKGR': 10, '4CV': 9}\n",
      "y_pred_list[1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "y_true_list[1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "model = VGG00(config['pretransform_im_size']) #print(config['pretransform_im_size']) #(128, 128)\n",
    "## PRINT MODEL\n",
    "print(f'====================================================')\n",
    "print(model)\n",
    "\n",
    "\n",
    "model_path = '/home/mx19/tmp/model' \n",
    "print(f' {model_path}')\n",
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    #os.path.join(model_path, \"VGG00v00_metric_model.pth\")\n",
    "    #os.path.join(model_path, \"VGG00v01_metric_model.pth\")\n",
    "    os.path.join(model_path, \"VGG00v02_metric_model.pth\")\n",
    "    ))\n",
    "\n",
    "model.to(device) # Place model on GPU\n",
    "\n",
    "model.eval()\n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clip_batch_idx, sample_batched in enumerate(test_dataloader):\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(device), sample_batched[1].to(device)\n",
    "        print(f'==================================================')\n",
    "        print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        y_test_pred = model(X_train_batch)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)        \n",
    "        \n",
    "        for i in range(len(y_test_pred)):\n",
    "            y_true_list.append(y_train_batch[i].cpu().item())\n",
    "            y_pred_list.append(y_pred_tag[i].cpu().item())\n",
    "            \n",
    "        \n",
    "print(f'==================================================')        \n",
    "print(f'==================================================')        \n",
    "print(get_class_distribution(test_set))\n",
    "print(f'y_pred_list{y_pred_list}')\n",
    "print(f'y_true_list{y_true_list}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781a2101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:41:45.095112Z",
     "start_time": "2022-03-28T16:41:45.008370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73        10\n",
      "           1       0.71      0.56      0.63         9\n",
      "\n",
      "    accuracy                           0.68        19\n",
      "   macro avg       0.69      0.68      0.68        19\n",
      "weighted avg       0.69      0.68      0.68        19\n",
      "\n",
      "[[8 2]\n",
      " [4 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Predicted'), Text(0, 0.5, 'True')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(classification_report(y_true_list, y_pred_list))\n",
    "print(confusion_matrix(y_true_list, y_pred_list))\n",
    "\n",
    "cm=confusion_matrix(y_true_list, y_pred_list)\n",
    "#cm=confusion_matrix(y_true_list, y_pred_list, normalize='all')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['BGR','4CV'])\n",
    "cmd.plot()\n",
    "cmd.ax_.set(xlabel='Predicted', ylabel='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607176b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T16:44:50.478014Z",
     "start_time": "2022-03-28T16:44:50.470241Z"
    }
   },
   "source": [
    "\n",
    "## File size of models: \n",
    "```\n",
    "~/tmp/model$ tree -s\n",
    ".\n",
    " [  351538511]  VGG00v00_metric_model.pth\n",
    " [  752946511]  VGG00v01_metric_model.pth\n",
    " [  201010511]  VGG00v02_metric_model.pth\n",
    "\n",
    "0 directories, 3 files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9403d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
