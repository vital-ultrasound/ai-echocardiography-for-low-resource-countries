{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef885fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:05.695513Z",
     "start_time": "2021-12-17T09:18:03.125501Z"
    }
   },
   "source": [
    "# Learning pipeline\n",
    "\n",
    "**Author**: Miguel Xochicale [@mxochicale](https://github.com/mxochicale)     \n",
    "**Contributors**: Nhat Phung Tran Huy [@huynhatd13](https://github.com/huynhatd13); Hamideh Kerdegari [@hamidehkerdegari](https://github.com/hamidehkerdegari);  Alberto Gomez [@gomezalberto](https://github.com/)  \n",
    "\n",
    "Feb2022; March2022; May2022\n",
    "\n",
    "\n",
    "## Summary\n",
    "This notebook presents a learning pipeline to classify 4 chamber view from echocardiography datasets.\n",
    "\n",
    "### How to run the notebook\n",
    "\n",
    "1. Go to echocardiography repository path: `$HOME/repositories/echocardiography/`\n",
    "2. Open echocardiography repo in pycharm and in the terminal type:\n",
    "    ```\n",
    "    git checkout master # or the branch\n",
    "    git pull # to bring a local branch up-to-date with its remote version\n",
    "    ```\n",
    "3. Launch Notebook server  \n",
    "    Go to you repository path: `cd $HOME/repositories/echocardiography/scripts/dataloaders` and type in the pycharm terminal:\n",
    "    ```\n",
    "    conda activate rt-ai-echo-VE \n",
    "    jupyter notebook\n",
    "    ```\n",
    "    which will open your web-browser.\n",
    "    \n",
    "    \n",
    "### References\n",
    "* \"Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback\". https://www.fda.gov/media/122535/download \n",
    "* Gomez A. et al. 2021 https://github.com/vital-ultrasound/lung/blob/main/multiclass_pytorch/datasets/LUSVideoDataset.py \n",
    "* Kerdegari H. et al. 2021 https://github.com/vital-ultrasound/lung/tree/main/multiclass_tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2aa0",
   "metadata": {},
   "source": [
    "# Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d3efa",
   "metadata": {},
   "source": [
    "## 1. Setting imports and datasets paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345efe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:26:06.723022Z",
     "start_time": "2022-05-16T17:26:06.703200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML #to be used with HTML(animation.ArtistAnimation().to_jshtml())\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from source.dataloaders.EchocardiographicVideoDataset import EchoClassesDataset\n",
    "from source.models.learning_misc import train_loop, test_loop#, basicVGGNet\n",
    "from source.helpers.various import concatenating_YAML_via_tags, plot_dataset_classes, split_train_validate_sets\n",
    "\n",
    "HOME_PATH = os.path.expanduser(f'~')\n",
    "USERNAME = os.path.split(HOME_PATH)[1]\n",
    "\n",
    "REPOSITORY_PATH='repositories/echocardiography'\n",
    "FULL_REPO_PATH = HOME_PATH+'/'+REPOSITORY_PATH\n",
    "FULL_REPO_MODEL_PATH = HOME_PATH +'/' + REPOSITORY_PATH + '/models'\n",
    "CONFIG_FILES_PATH= REPOSITORY_PATH + '/scripts/config_files/users_paths_files'\n",
    "YML_FILE =  'config_users_paths_files_username_' + USERNAME + '.yml'\n",
    "FULL_PATH_FOR_YML_FILE = os.path.join(HOME_PATH, CONFIG_FILES_PATH, YML_FILE)\n",
    "PATH_for_temporal_files = os.path.join(HOME_PATH, 'datasets/vital-us/echocardiography/temporal-files')\n",
    "\n",
    "yaml.add_constructor('!join', concatenating_YAML_via_tags)  ## register the tag handler\n",
    "with open(FULL_PATH_FOR_YML_FILE, 'r') as yml:\n",
    "    config = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')    \n",
    "print(f'FULL_PATH_FOR_YML_FILE: {FULL_PATH_FOR_YML_FILE}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08abb29",
   "metadata": {},
   "source": [
    "## 2. Generate list text files for train and validate datasets\n",
    "\n",
    "Edit config_users_paths_files_username_$USER.yml at '../config_files/users_paths_files/config_users_paths_files_username_template.yml' with the right paths and percentage of `ntraining`:  \n",
    "```\n",
    "echodataset_path: !join [*HOME_DIR, /datasets/vital-us/echocardiography/videos-echo-test]\n",
    "data_list_output_path: !join [*HOME_DIR, /repositories/echocardiography/scripts/config_files/data_lists/]\n",
    "ntraining: 0.8\n",
    "randomise_file_list: False\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52167b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:26:07.927640Z",
     "start_time": "2022-05-16T17:26:07.919224Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Setting ECHODATASET_PATH; \n",
    "#ECHODATASET_PATH = config['echodataset_path'] # Default\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-ALL'\n",
    "ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-BTEST'\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-in-verification40-49'\n",
    "\n",
    "TRAINING_SPLITTING = 0.5 #config['ntraining'] #Default\n",
    "\n",
    "split_train_validate_sets(  \n",
    "                        ECHODATASET_PATH, #config['echodataset_path']\n",
    "                        config['data_list_output_path'], \n",
    "                        TRAINING_SPLITTING,\n",
    "                        config['randomise_file_list']\n",
    "                        )\n",
    "\n",
    "SUBJECT_ID = '073'\n",
    "NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 30\n",
    "PRETRANSFORM_IM_SIZE = [128, 128] #[650, 690] original pixel size for VenueGO\n",
    "#PRETRANSFORM_IM_SIZE = config['pretransform_im_size'] ##DEFAULT\n",
    "\n",
    "interval_between_frames_in_milliseconds=33.3 ## 1/30=0.033333\n",
    "frame_per_seconds_for_animated_frames=30\n",
    "\n",
    "\n",
    "label_id = ('BKGR', '4CV')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:08.264310Z",
     "start_time": "2021-12-17T09:18:08.250178Z"
    }
   },
   "source": [
    "## 2. Setting variables and loading datasets using pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:26:11.037547Z",
     "start_time": "2022-05-16T17:26:10.769782Z"
    }
   },
   "outputs": [],
   "source": [
    "# device = torch.device(if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #or \"cuda:NN\" can also be used e.g., \"cuda:0\"\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# Defining transforms that apply to the entire dataset.\n",
    "# These transforms are not augmentation.\n",
    "if config['use_pretransform_image_size']:\n",
    "    pretransform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=PRETRANSFORM_IM_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    pretransform = None\n",
    "\n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_train_augmentation']:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),  # in degrees\n",
    "        transforms.RandomEqualize(p=0.5),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_validation_augmentation']:\n",
    "    val_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=5),  # in degrees\n",
    "    #transforms.RandomEqualize(p=0.5),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "\n",
    "\n",
    "train_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_train'],\n",
    "    participant_path_json_list=config['participant_path_json_list_train'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,#transform=train_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "validation_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_validation'],\n",
    "    participant_path_json_list=config['participant_path_json_list_validation'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,#transform=train_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "\n",
    "## Spliting train_dataset into train_set and test_set\n",
    "Ntdt = train_dataset.__len__()\n",
    "ntraining = 0.9\n",
    "\n",
    "Ntrain=round(Ntdt*ntraining)\n",
    "Ntest = round(Ntdt - (Ntdt*ntraining))\n",
    "print(f'Ntrain size: {Ntrain}, Ntest size: {Ntest}, \"Ntrain+Ntest\" size={Ntrain+Ntest}')\n",
    "train_set, test_set = torch.utils.data.random_split(train_dataset, [Ntrain, Ntest])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15737900",
   "metadata": {},
   "source": [
    "## 3. Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d07f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:26:13.888472Z",
     "start_time": "2022-05-16T17:26:13.877965Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_class_distribution(dataset_obj):\n",
    "    count_class_dict = {\n",
    "   'BKGR': 0 ,\n",
    "   \"4CV\": 0\n",
    "    }\n",
    "    \n",
    "    for clip_index_i in range(len(dataset_obj)):\n",
    "        data_idx = dataset_obj[clip_index_i]\n",
    "        label_id_idx = data_idx[1]\n",
    "        label = label_id[label_id_idx]\n",
    "        count_class_dict[label]+= 1\n",
    "        #count_class_dict[label]+= 1* number_of_frames_per_segment_in_a_clip\n",
    "\n",
    "    return count_class_dict\n",
    "        \n",
    "        \n",
    "def plot_from_dict(dict_obj, plot_title, **kwargs):\n",
    "    return sns.barplot(data = pd.DataFrame.from_dict([dict_obj]).melt(), \n",
    "                       x = \"variable\", y=\"value\", hue=\"variable\", **kwargs).set_title(plot_title)\n",
    "\n",
    "\n",
    "def creating_pair_of_clips(dataset):\n",
    "    number_of_clips = len(dataset)\n",
    "    clips=[]\n",
    "    for clip_index in range( int(number_of_clips)  ):\n",
    "        data_idx = dataset[clip_index]\n",
    "        data_clip_idx = data_idx[0]\n",
    "        label_clip_idx = data_idx[1]\n",
    "        clip_frame_clip_idx = data_idx[2]\n",
    "        n_available_frames_clip_idx = data_idx[3]\n",
    "        print(f' CLIP:{clip_index:02d} of {label_id[label_clip_idx]} label for {data_clip_idx.size()} TOTAL_FRAMES: {n_available_frames_clip_idx} from clip_frame_clip_idx {clip_frame_clip_idx}')    \n",
    "        clips.append([data_clip_idx, label_clip_idx, clip_index, clip_frame_clip_idx, n_available_frames_clip_idx ]) \n",
    "\n",
    "    return(clips)\n",
    "\n",
    "\n",
    "def pair_clips_labels(clips):\n",
    "    pair_clips_labels_=[]    \n",
    "    number_of_clips=len(clips)\n",
    "    for clip_index_i_A in range( int(number_of_clips/2)  ):\n",
    "        clip_index_i_B=int(number_of_clips/2) + clip_index_i_A \n",
    "        print(f' pair_clips_labels[{clip_index_i_A}]-- BKRG:{clip_index_i_A}, 4CV:{clip_index_i_B}')\n",
    "        data_clip_i_A=clips[clip_index_i_A][0]\n",
    "        label_i_A=clips[clip_index_i_A][1]\n",
    "        clip_i_A=clips[clip_index_i_A][2]\n",
    "        number_of_frames_A=clips[clip_index_i_A][4]\n",
    "        data_clip_i_B=clips[clip_index_i_B][0]\n",
    "        label_i_B=clips[clip_index_i_B][1]\n",
    "        clip_i_B=clips[clip_index_i_B][2]\n",
    "        number_of_frames_B=clips[clip_index_i_B][4]\n",
    "        pair_clips_labels_.append([data_clip_i_A, label_i_A, clip_i_A, number_of_frames_A, data_clip_i_B, label_i_B, clip_i_B, number_of_frames_B])\n",
    "    \n",
    "    return(pair_clips_labels_)\n",
    "\n",
    "\n",
    "def animate_clips(pair_clips_labels):\n",
    "    #print(f' CLIP: for {label_id[pair_clips_labels[1]]} ')\n",
    "    fig = plt.figure()    \n",
    "    pair_of_clip_index_i_frames=[]   \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "    data_clip_tensor_A=pair_clips_labels[0]\n",
    "    label_clip_A=pair_clips_labels[1]\n",
    "    clip_i_A=pair_clips_labels[2]\n",
    "    number_of_frames_A=pair_clips_labels[3]\n",
    "    data_clip_tensor_B=pair_clips_labels[4]\n",
    "    label_clip_B=pair_clips_labels[5]\n",
    "    clip_i_B=pair_clips_labels[6]\n",
    "    number_of_frames_B=pair_clips_labels[7]\n",
    "    \n",
    "    \n",
    "    ax1.title.set_text(f' CLIP: {clip_i_A:02d}--{label_id[label_clip_A]} with {number_of_frames_A} of {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} frames [for Subject {SUBJECT_ID}]')\n",
    "    ax2.title.set_text(f' CLIP: {clip_i_B:02d}--{label_id[label_clip_B]} with {number_of_frames_B} of {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} frames [for Subject {SUBJECT_ID}]')\n",
    "    for frames_idx in range(data_clip_tensor_A[:,:,...].size()[1]):\n",
    "        imA = ax1.imshow(data_clip_tensor_A[:,frames_idx,...].cpu().detach().numpy().transpose(1,2,0) , cmap=plt.get_cmap('gray') )  \n",
    "        imB = ax2.imshow(data_clip_tensor_B[:,frames_idx,...].cpu().detach().numpy().transpose(1,2,0) , cmap=plt.get_cmap('gray') )  \n",
    "        pair_of_clip_index_i_frames.append( [imA, imB] )\n",
    "    fig.tight_layout()    \n",
    "    #return fig, pair_of_clip_index_i_frames\n",
    "\n",
    "    anim = animation.ArtistAnimation(fig, pair_of_clip_index_i_frames, interval=interval_between_frames_in_milliseconds, blit=True, repeat_delay=1000)\n",
    "    return anim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760906",
   "metadata": {},
   "source": [
    "## 4. Plotting Class Distribution (creates temp clips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6472a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:28:08.230311Z",
     "start_time": "2022-05-16T17:26:42.888081Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'class_distribution(train_set): {get_class_distribution(train_set)}')\n",
    "print(f'class_distribution(validation_dataset): {get_class_distribution(validation_dataset)}' )\n",
    "print(f'class_distribution(test_set): {get_class_distribution(test_set)}')\n",
    "    \n",
    "number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip']    \n",
    "print(f'Number of frames for training datasets {Ntrain*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "print(f'Number of frames for testing datasets {Ntest*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "\n",
    "plot_title_train_label= f'TRAIN dataset of {len(train_set)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_test_label= f'TEST dataset of {len(test_set)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_val_label= f'VALIDATION dataset of {len(validation_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,7))\n",
    "plot_from_dict(get_class_distribution(train_set), plot_title=plot_title_train_label, ax=axes[0])\n",
    "plot_from_dict(get_class_distribution(test_set), plot_title=plot_title_test_label, ax=axes[1])\n",
    "plot_from_dict(get_class_distribution(validation_dataset), plot_title=plot_title_val_label, ax=axes[2])\n",
    "plt.show()\n",
    "\n",
    "###########\n",
    "## NOTES ##\n",
    "# HamidehK on Thu 23 Mar 14:00:00 GMT 2022\n",
    "    # 65 clips with 60 frames each create 3900 frames which is a low number for traininig data.\n",
    "    # Hamideh recomends to increase the training data or perhaps reduce the convs to one!\n",
    "\n",
    "# MiguelX on Mon 28 Mar 11:35:55 BST 2022\n",
    "    # Make use of 169 clips with 60 frames which results in 10140 frames for training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ea2a6",
   "metadata": {},
   "source": [
    "## 5. Animating frames of one clip of the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b57ab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:28:18.139781Z",
     "start_time": "2022-05-16T17:28:18.042567Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'---------------------------------------')\n",
    "clips=creating_pair_of_clips(train_dataset)\n",
    "pair_clips_and_labels = pair_clips_labels(clips)\n",
    "#print(SUBJECT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49ba14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:28:39.559397Z",
     "start_time": "2022-05-16T17:28:36.675755Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#average_HR =\n",
    "#fps = 30\n",
    "# 60 # beats per minute\n",
    "#Beats-per-minute: 60 BPM\n",
    "#Beats-per-second: 1 Hz\n",
    "#Cycle-per-second: 1 (Cycle/s)\n",
    "\n",
    "PAIR_OF_CLIPS = pair_clips_and_labels[0]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "HTML(animated_frames.to_jshtml())      \n",
    "\n",
    "# ##SAVE ANIMATIONS\n",
    "# for idx in range(0,len(pair_clips_labels)):\n",
    "#     PAIR_OF_CLIPS = pair_clips_labels[idx]\n",
    "#     print( f' pair_clips_labels {str(PAIR_OF_CLIPS[2])} {str(PAIR_OF_CLIPS[6])}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d225",
   "metadata": {},
   "source": [
    "## 6. Displayting frames in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8805e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:28:39.647053Z",
     "start_time": "2022-05-16T17:28:39.560675Z"
    }
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "##### Setting up BATCH_SIZE_OF_CLIPS\n",
    "BATCH_SIZE_OF_CLIPS = 4\n",
    "##############################\n",
    "##############################\n",
    "\n",
    "print(f' train_dataset.__len__() = {train_set.__len__()}')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "print(f' test_dataset.__len__() = {test_set.__len__()}')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f' validation_dataset.__len__() = {validation_dataset.__len__()}')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f'len(train_dataloader): {len(train_dataloader)}')\n",
    "for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "    print(f'  ====================================================')\n",
    "    sample_batched_images=sample_batched[0]\n",
    "    sample_batched_labels=sample_batched[1]\n",
    "    print(f'    BATCH_OF_CLIPS_INDEX : {clip_batch_idx} / {len(train_dataloader) - 1}')\n",
    "    print(f'    sample_batched_labels.size(): {  sample_batched_labels.size()  }')\n",
    "    print(f'    sample_batched_labels.squeeze().size(): {  sample_batched_labels.squeeze().size()  }')\n",
    "    print(f'    sample_batched_images.size(): {sample_batched_images.size()}')\n",
    "\n",
    "    for BATCH_SIZE_IDX, label in enumerate(sample_batched_labels):\n",
    "        print(f'        BATCH_SIZE_IDX {BATCH_SIZE_IDX} label: {label}')\n",
    "        sample_batched_idx_image = sample_batched_images[BATCH_SIZE_IDX,...]\n",
    "        print(f'        Sample_batched_idx_image.size()  {sample_batched_idx_image.size() }'  )\n",
    "\n",
    "        grid = utils.make_grid(sample_batched_idx_image)\n",
    "        print(f'        Grid size {grid.size()}' )\n",
    "#         plt.figure(figsize =(20,20) )\n",
    "#         plt.imshow( grid.cpu().detach().numpy().transpose(1, 2, 0) )\n",
    "#         plt.title(f'BATCH_SIZE_IDX {BATCH_SIZE_IDX}; Label: {label_id[label]}')\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e92336",
   "metadata": {},
   "source": [
    "## 7. Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369a416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:39:40.246199Z",
     "start_time": "2022-05-16T17:39:40.237422Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "################################\n",
    "##### Define VGG00 architecture\n",
    "class VGG00(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, n_classes=2):\n",
    "        \"\"\"\n",
    "        Simple Video classifier to classify into two classes:\n",
    "        Args:\n",
    "            input_size:  shape of the input image. Should be a 2 element vector for a 2D video (width, height) [e.g. 128, 128].\n",
    "            n_classes: number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        super(VGG00, self).__init__()\n",
    "        self.name = 'VGG00'\n",
    "        self.input_size = input_size\n",
    "        self.n_classes = n_classes\n",
    "        self.n_frames_per_clip = NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP\n",
    "        self.n_features = np.prod(self.input_size)*self.n_frames_per_clip\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.conv0 = nn.Conv3d(in_channels=1, out_channels=64,\n",
    "                               kernel_size = (1, 3, 3),  ## (-depth, -height, -width)\n",
    "                               stride =      (4, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "                               padding =     (0, 0, 0)\n",
    "                               )\n",
    "        #NOTES\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html\n",
    "        #IN: [N,Cin,D,H,W]; OUT: (N,Cout,Dout,Hout,Wout)\n",
    "        #[batch_size, channels, depth, height, width].\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=64, out_channels=128,\n",
    "                               kernel_size = (1, 3, 3),  # (-depth, -height, -width)\n",
    "                               stride =      (4, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "                               padding =     (0, 0, 0)\n",
    "                               )\n",
    "        \n",
    "#         self.conv2 = nn.Conv3d(in_channels=128, out_channels=256,\n",
    "#                                kernel_size =  (1, 3, 3),  # (-depth, -height, -width)\n",
    "#                                stride =       (1, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding =      (0, 0, 0)\n",
    "#                                )\n",
    "        \n",
    "        \n",
    "#         self.conv3 = nn.Conv3d(in_channels=256, out_channels=512,\n",
    "#                                kernel_size=(2, 2, 2),  # (-depth, -height, -width)\n",
    "#                                stride=(2, 2, 2), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding = (0, 0, 0)\n",
    "#                                )\n",
    "        \n",
    "        \n",
    "#         self.pool0 = nn.MaxPool3d(\n",
    "#                                 kernel_size = (1, 3, 3),  # (-depth, -height, -width)\n",
    "#                                 stride =      (1, 1, 1), \n",
    "#                                 padding =     (0, 0, 0), \n",
    "#                                 dilation =    (1, 1, 1)\n",
    "#                                 )\n",
    "#         #NOTES\n",
    "#         #Keeps the training to 50% after 100 epochs\n",
    "\n",
    "\n",
    "        self.fc0 = nn.Linear(in_features=50176, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=self.n_classes)\n",
    "        #self.fc1 = nn.Linear(in_features=2048, out_features=self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f'x.shape(): {x.size()}') ##[batch_size, channels, depth, height, width]\n",
    "        #x = x.permute(0,2,1,3,4)##[batch_size, depth,channels, height, width]\n",
    "        print(f'x.shape(): {x.size()}')\n",
    "        \n",
    "        x = F.relu( self.conv0(x) )\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape(): x.shape(): torch.Size([2, 64, 60, 128, 128]) with kernel_size=(1, 1, 1)\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape():torch.Size([2, 64, 51, 29, 29]) with kernel_size=(10, 100, 100)\n",
    "        print(f'conv0.size(): {x.size()}')\n",
    "        \n",
    "        x = F.relu( self.conv1(x) )\n",
    "        #print(f'x.shape(): {x.size()}') with kernel_size=(1, 10, 10) #x.shape(): torch.Size([2, 32, 60, 20, 20])\n",
    "        #print(f'conv1.size(): {x.size()}')\n",
    "        \n",
    "        #x = F.relu( self.conv2(x) )\n",
    "        #x = F.relu( self.conv3(x) )\n",
    "        \n",
    "        #x = self.pool0(x)\n",
    "        #print(f'x.pool0..shape(): {x.size()}') \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        #print(f'self.flatten(x) size() {x.size()}') #x.shape(): torch.Size([4, 983040])\n",
    "        x = self.fc0(x)\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape(): torch.Size([4, 32])\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=0.5) #dropout was included to combat overfitting\n",
    "        \n",
    "        #print(f'x.shape(): {x.size()}') # x.shape(): torch.Size([4, 2])\n",
    "        #x = self.sigmoid(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        #print(f'x.shape(): {x.size()}')  #x.shape(): torch.Size([4, 2])\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb1dd6",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Sanity checks for the model and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5dd8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:41:56.427732Z",
     "start_time": "2022-05-16T17:41:56.304527Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "##### Setting up Model Parameters\n",
    "LEARNING_RATE = 0.000005  # Trial and Error with diffent values 0.0000005 or 0.00005\n",
    "MAX_EPOCHS = 3 #Alternatvely, make use of: config['max_epochs']\n",
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "##### Sanity Checks\n",
    "#tensor_shape_size = [BATCH_SIZE_OF_CLIPS, config['number_of_frames_per_segment_in_a_clip'], 1, 128, 128]\n",
    "#model = basicVGGNet(tensor_shape_size)\n",
    "\n",
    "model = VGG00(PRETRANSFORM_IM_SIZE) #print(config['pretransform_im_size']) #(128, 128)\n",
    "model.to(DEVICE) # Place model on GPU-\n",
    "\n",
    "### Sanity check\n",
    "#print(len(train_dataloader)) #6 BATCHES of 10=BATCH_SIZE_OF_CLIPS\n",
    "sample_batched = next(iter(train_dataloader))\n",
    "#print(sample_batched[0].shape) #torch.Size([10, 60, 1, 128, 128])\n",
    "#print(sample_batched[1])#tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1])\n",
    "#print(sample_batched[2]) #tensor([47, 42,  0, 51, 49, 75, 67, 67, 62, 84])\n",
    "#print(sample_batched[3]) #tensor([105, 102,  43, 106,  94, 161, 151, 183, 150, 151])\n",
    "\n",
    "clip_batch = sample_batched[0]\n",
    "#print(clip_batch.size()) #torch.Size([4, 60, 1, 128, 128])\n",
    "##[batch_size, channels, depth, height, width]\n",
    "\n",
    "# # frames = image.to(device)\n",
    "# # # ###Sanity check-\n",
    "print(model(clip_batch).shape) #torch.Size([4, 2])\n",
    "\n",
    "# #https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
    "del sample_batched\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22976ae2",
   "metadata": {},
   "source": [
    "## 8. Define Optimizer\n",
    "1. Set learning rate for how much the model is updated per batch.\n",
    "2. Set total epoch number, as we have shuffle and random transforms, so the training data of every epoch is different.\n",
    "3. Set the number of clips per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295fe94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:42:02.730233Z",
     "start_time": "2022-05-16T17:42:02.617959Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = VGG00(PRETRANSFORM_IM_SIZE) #print(config['pretransform_im_size']) #(128, 128)\n",
    "model.to(DEVICE) # Place model on GPU\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "## PRINT MODEL\n",
    "print(f'====================================================')\n",
    "print(model)\n",
    "\n",
    "# ### PRINT model.named_parameters\n",
    "# print(f'====================================================')\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98807711",
   "metadata": {},
   "source": [
    "## 9. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee627c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:43:39.822702Z",
     "start_time": "2022-05-16T17:43:38.193804Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TRAINING\n",
    "#clip_batch_size = tuple(train_dataloader.dataset.__getitem__(0)[0].shape) ##(60, 1, 128, 128) frames, chs, [width, height]\n",
    "#print(clip_batch_size)\n",
    "\n",
    "############################\n",
    "####### BINARY ACCURACY MODULE\n",
    "def binary_accuracy(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    binary_accuracy to calculate accuracy per epoch.\n",
    "    \"\"\"\n",
    "    y_pred_tag = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_tag, dim = 1)\n",
    "    correct_results_sum = (y_pred_tags == y_test).sum().float()\n",
    "    accuracy = correct_results_sum/y_test.shape[0]\n",
    "    accuracy = torch.round(accuracy * 100)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "############################\n",
    "####### TRAIN LOOP MODULE\n",
    "def train_loop(train_dataloader, model, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    train_loop\n",
    "    Arguments:\n",
    "        dataloader, model, criterion, optimizer, device\n",
    "\n",
    "    Return:\n",
    "        train_epoch_loss\n",
    "    \"\"\"\n",
    "    train_epoch_loss = 0\n",
    "    train_acc_loss_epoch = 0\n",
    "    step_train = 0\n",
    "    #size = len(train_dataloader.dataset)\n",
    "    for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "        step_train += 1\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(device), sample_batched[1].to(device)\n",
    "\n",
    "        #print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        # print(f'----------------------------------------------------------')\n",
    "        # print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        # print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        y_train_pred = model(X_train_batch) #torch.Size([9, 2])\n",
    "        #y_train_pred = model(X_train_batch).squeeze()  # torch.Size([9, 2])\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = binary_accuracy(y_train_pred, y_train_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if clip_batch_idx % 10 == 0: ## Print loss values every 10 clip batches\n",
    "        #     train_loss, current = train_loss.item(), clip_batch_idx * len(X_train_batch)\n",
    "        #     print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        train_epoch_loss += train_loss.detach().item()\n",
    "        train_acc_loss_epoch += train_acc.detach().item()\n",
    "\n",
    "    train_epoch_loss /= step_train\n",
    "    train_acc_loss_epoch /= step_train\n",
    "\n",
    "    return train_epoch_loss, train_acc_loss_epoch\n",
    "\n",
    "\n",
    "############################\n",
    "####### TEST LOOP MODULE\n",
    "def test_loop(dataloader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Test loop \n",
    "    \n",
    "    Arguments:\n",
    "        dataloader, model, criterion, optimizer, device\n",
    "\n",
    "    Return:\n",
    "        test_epoch_loss, correct\n",
    "    \"\"\"\n",
    "\n",
    "    train_epoch_acc = 0\n",
    "    step_test = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_epoch_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #model.eval()\n",
    "        #val_epoch_loss = 0\n",
    "        #val_epoch_acc = 0\n",
    "        for clip_batch_idx, sample_val_batched in enumerate(dataloader):\n",
    "            step_test += 1\n",
    "            X_val_batch, y_val_batch = sample_val_batched[0].to(device), sample_val_batched[1].to(device)\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            test_epoch_loss += criterion(y_val_pred, y_val_batch).detach().item()\n",
    "            correct += (y_val_pred.argmax(1) == y_val_batch).type(torch.float).sum().detach().item()\n",
    "\n",
    "    test_epoch_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    return test_epoch_loss, correct\n",
    "\n",
    "\n",
    "#Dictionaries to store the accuracy/epoch and loss/epoch for both train and validation sets.\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    #\"val\": []\n",
    "}\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    #\"val\": []\n",
    "}\n",
    "\n",
    "#for epoch in tqdm(range(1, MAX_EPOCHS)):   \n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"EPOCH {epoch + 1}/{MAX_EPOCHS}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    \n",
    "    train_epoch_loss, train_acc_loss_epoch = train_loop(train_dataloader, model, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    test_epoch_loss, correct = test_loop(val_dataloader, model, criterion, DEVICE)\n",
    "\n",
    "    #print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} | Average Train Acc: {train_epoch_acc:.5f} | Average Validation Loss: {val_epoch_loss:.3f} | Average Validation Acc: {val_epoch_acc:.5f} ')\n",
    "    #print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} |Average Train Acc: {train_epoch_acc:.5f}  ')\n",
    "    \n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_epoch_loss:>8f} \\n\")\n",
    "\n",
    "    print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} Average Train Accuracy Loss: {(train_acc_loss_epoch):>0.1f}% ')\n",
    "    \n",
    "    print(f\"Test Error: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg Test loss: {test_epoch_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "    loss_stats['train'].append(train_epoch_loss)\n",
    "    loss_stats['test'].append(test_epoch_loss)\n",
    "    accuracy_stats['train'].append(train_acc_loss_epoch)\n",
    "    accuracy_stats['test'].append(100*correct)\n",
    "            \n",
    "print(\"DONE TRAINING LOOP!\")\n",
    "\n",
    "\n",
    "#model_path = '/home/mx19/tmp/model' \n",
    "print(f' {FULL_REPO_MODEL_PATH}')\n",
    "torch.save(model.state_dict(), os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\"))\n",
    "print(\"Saved metric model\")\n",
    "\n",
    "print(loss_stats)\n",
    "print(accuracy_stats)\n",
    "\n",
    "## NOTES\n",
    "    # executed time of 13m 5s for training loop with 100 epochs with LR=0.000005 of 3900 frames on Thu 24 Mar 10:02:26 GMT 2022\n",
    "    # executed time of 7m 33s s for training loop with 100 epochs with LR=0.000005 of 10140 frames on Mon 28 Mar 11:48:43 BST 2022\n",
    "    # executed time of 22m 45s s for training loop with 300 epochs with LR=0.000005 of 10140 frames on Mon 28 Mar 12:25:52 BST 2022\n",
    "    # executed time of 28m 46s s for training loop with 300 epochs with LR=0.000005 of 10140 frames on 16:53:09 2022-03-28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a7882",
   "metadata": {},
   "source": [
    "## 10. Visualize accuracy and loss performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd312af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:43:50.842717Z",
     "start_time": "2022-05-16T17:43:50.607345Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "sns.lineplot(data=loss_df, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title(f'Loss for EPOCHS={MAX_EPOCHS} BATCH_SIZE_OF_CLIPS={BATCH_SIZE_OF_CLIPS} LEARNING_RATE={LEARNING_RATE}')\n",
    "sns.lineplot(data=acc_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title(f'Train-Val Accuracy/Epoch EPOCHS={MAX_EPOCHS} BATCH_SIZE_OF_CLIPS={BATCH_SIZE_OF_CLIPS} LEARNING_RATE={LEARNING_RATE}')\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aafb41",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791b35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:44:50.784062Z",
     "start_time": "2022-05-16T17:44:50.704818Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clip_batch_idx, sample_batched in enumerate(test_dataloader):\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(DEVICE), sample_batched[1].to(DEVICE)\n",
    "        print(f'==================================================')\n",
    "        print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        y_test_pred = model(X_train_batch)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)        \n",
    "        \n",
    "        for i in range(len(y_test_pred)):\n",
    "            y_true_list.append(y_train_batch[i].cpu().item())\n",
    "            y_pred_list.append(y_pred_tag[i].cpu().item())\n",
    "            \n",
    "        \n",
    "print(f'==================================================')        \n",
    "print(f'==================================================')        \n",
    "print(get_class_distribution(test_set))\n",
    "print(f'y_pred_list{y_pred_list}')\n",
    "print(f'y_true_list{y_true_list}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a2101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:44:56.917377Z",
     "start_time": "2022-05-16T17:44:56.825114Z"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true_list, y_pred_list))\n",
    "print(confusion_matrix(y_true_list, y_pred_list))\n",
    "\n",
    "cm=confusion_matrix(y_true_list, y_pred_list)\n",
    "#cm=confusion_matrix(y_true_list, y_pred_list, normalize='all')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['BGR','4CV'])\n",
    "cmd.plot()\n",
    "cmd.ax_.set(xlabel='Predicted', ylabel='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060e041",
   "metadata": {},
   "source": [
    "## 12. [**!WARNING!**] Cleanup temporal data directory \n",
    "Remove directory if a temporary was used.\n",
    "\n",
    "```\n",
    "       Make sure you know which path you will remove as you do not like to remove important files.\n",
    "       shutil.rmtree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ae88d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T17:45:02.022417Z",
     "start_time": "2022-05-16T17:45:01.953177Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_files_path = config['temporal_data_path']\n",
    "\n",
    "shutil.rmtree(temporal_files_path)\n",
    "print(f' {temporal_files_path} is empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eea63f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T15:53:10.760424Z",
     "start_time": "2022-03-28T15:53:10.754191Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f0279",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
