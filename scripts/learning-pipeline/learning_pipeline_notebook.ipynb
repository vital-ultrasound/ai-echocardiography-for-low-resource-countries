{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef885fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:05.695513Z",
     "start_time": "2021-12-17T09:18:03.125501Z"
    }
   },
   "source": [
    "# Learning pipeline\n",
    "\n",
    "**Author**: Miguel Xochicale [@mxochicale](https://github.com/mxochicale)     \n",
    "**Contributors**: Nhat Phung Tran Huy [@huynhatd13](https://github.com/huynhatd13); Hamideh Kerdegari [@hamidehkerdegari](https://github.com/hamidehkerdegari);  Alberto Gomez [@gomezalberto](https://github.com/)  \n",
    "\n",
    "\n",
    "## History\n",
    "* Feb2022: Adding initial models with small dataset   \n",
    "* March2022: Improved datasets representation\n",
    "* April2022: Adds dataloader for clips and videos\n",
    "* May2022: Tidies VGG2D and VGG3D   \n",
    "* June2022: Tidies basicVGG model and adds heuristics for hyperarameters \n",
    "* Week1, July2022: Integreate modules in source path\n",
    "* Week2, July2022: Implements Tromp2022Net DOI: https://doi.org/10.1016/S2589-7500(21)00235-17\n",
    "* Week3, July2022: Adds LeNet, AlexNet and VGGNets, MobileNetV1, MobileNetV2\n",
    "* Week4, July2022: Adds ShuffleNetV1, ShuffleNetV2 and merge all to main!\n",
    "\n",
    "\n",
    "## Summary\n",
    "This notebook presents a learning pipeline to classify 4 chamber view from echocardiography datasets.\n",
    "\n",
    "### How to run the notebook\n",
    "\n",
    "1. Go to echocardiography repository path: `$HOME/repositories/echocardiography/`\n",
    "2. Open echocardiography repo in pycharm and in the terminal type:\n",
    "    ```\n",
    "    git checkout master # or the branch\n",
    "    git pull # to bring a local branch up-to-date with its remote version\n",
    "    ```\n",
    "3. Launch Notebook server  \n",
    "    Go to you repository path: `cd $HOME/repositories/echocardiography/scripts/dataloaders` and type in the pycharm terminal:\n",
    "    ```\n",
    "    conda activate rt-ai-echo-VE \n",
    "    jupyter notebook\n",
    "    ```\n",
    "    which will open your web-browser.\n",
    "    \n",
    "    \n",
    "### References\n",
    "* \"Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback\". https://www.fda.gov/media/122535/download \n",
    "* https://nestedsoftware.com/2019/09/09/pytorch-image-recognition-with-convolutional-networks-4k17.159805.html \n",
    "* https://ai.stackexchange.com/questions/5769/in-a-cnn-does-each-new-filter-have-different-weights-for-each-input-channel-or\n",
    "* Gomez A. et al. 2021 https://github.com/vital-ultrasound/lung/blob/main/multiclass_pytorch/datasets/LUSVideoDataset.py \n",
    "* Kerdegari H. et al. 2021 https://github.com/vital-ultrasound/lung/tree/main/multiclass_tensorflow\n",
    "* https://learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/\n",
    "* https://github.com/shanglianlm0525/PyTorch-Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2aa0",
   "metadata": {},
   "source": [
    "# Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d3efa",
   "metadata": {},
   "source": [
    "## 1. Setting imports and datasets paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345efe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T16:58:25.887561Z",
     "start_time": "2022-07-25T16:58:25.090037Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "#import matplotlib.animation as animation\n",
    "from IPython.display import HTML #to be used with HTML(animation.ArtistAnimation().to_jshtml())\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from source.dataloaders.EchocardiographicVideoDataset import EchoClassesDataset\n",
    "from source.helpers.various import concatenating_YAML_via_tags, \\\n",
    "                                    plot_dataset_classes, \\\n",
    "                                    split_train_validate_sets\n",
    "from source.helpers.learning_pipeline import get_class_distribution, \\\n",
    "                                            plot_from_dict, \\\n",
    "                                            creating_pair_of_clips, \\\n",
    "                                            pair_clips_labels, \\\n",
    "                                            animate_clips\n",
    "from source.models.learning_misc import train_loop, \\\n",
    "                                        test_loop, \\\n",
    "                                        basicVGG #, VGG3D\n",
    "                                        \n",
    "                \n",
    "\n",
    "HOME_PATH = os.path.expanduser(f'~')\n",
    "USERNAME = os.path.split(HOME_PATH)[1]\n",
    "\n",
    "REPOSITORY_PATH='repositories/echocardiography'\n",
    "FULL_REPO_PATH = HOME_PATH+'/'+REPOSITORY_PATH\n",
    "FULL_REPO_MODEL_PATH = HOME_PATH +'/' + REPOSITORY_PATH + '/data/models'\n",
    "CONFIG_FILES_PATH= REPOSITORY_PATH + '/scripts/config_files/users_paths_files'\n",
    "YML_FILE =  'config_users_paths_files_username_' + USERNAME + '.yml'\n",
    "FULL_PATH_FOR_YML_FILE = os.path.join(HOME_PATH, CONFIG_FILES_PATH, YML_FILE)\n",
    "PATH_for_temporal_files = os.path.join(HOME_PATH, 'datasets/vital-us/echocardiography/temporal-files')\n",
    "\n",
    "## Setting TRAINING_CURVES_PATH\n",
    "#CURRENT_PATH=os.path.abspath(os.getcwd())\n",
    "RESULTS_PATH='scripts/learning-pipeline/results'\n",
    "TRAINING_CURVES_PATH = os.path.join(FULL_REPO_PATH, RESULTS_PATH)\n",
    "\n",
    "## Setting FULL_PATH_FOR_YML_FILE\n",
    "yaml.add_constructor('!join', concatenating_YAML_via_tags)  ## register the tag handler\n",
    "with open(FULL_PATH_FOR_YML_FILE, 'r') as yml:\n",
    "    config = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "\n",
    "## Printing Versions and paths\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')    \n",
    "print(f'FULL_PATH_FOR_YML_FILE: {FULL_PATH_FOR_YML_FILE}' )\n",
    "print(f'FULL_REPO_MODEL_PATH: {FULL_REPO_MODEL_PATH}' )\n",
    "print(f'TRAINING_CURVES_PATH: {TRAINING_CURVES_PATH}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08abb29",
   "metadata": {},
   "source": [
    "## 2. Generate list text files for train and validate datasets\n",
    "\n",
    "Edit config_users_paths_files_username_$USER.yml at '../config_files/users_paths_files/config_users_paths_files_username_template.yml' with the right paths and percentage of `ntraining`:  \n",
    "```\n",
    "#ECHODATASET_PATH = config['echodataset_path'] # Default\n",
    "ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-GOOD'\n",
    "TRAINING_SPLITTING = 0.8 #config['ntraining'] #Default\n",
    "randomise_file_list: False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52167b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T16:58:26.167434Z",
     "start_time": "2022-07-25T16:58:26.054073Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "##### Setting up device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #or \"cuda:NN\" can also be used e.g., \"cuda:0\"\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "\n",
    "### Trump et al. 2022 \n",
    "##\"We trained the models on 55487 images from 1145 individual echocardiograms (appendix pp 2–3).\" \n",
    "##Appendix pp 2–3.\n",
    "##AC4: TRAINING: total videos 740 total frames 9615; T\n",
    "##      TESTING: total videos:64 total frames 1218                \n",
    "\n",
    "\n",
    "##############################\n",
    "## Setting ECHODATASET_PATH; \n",
    "#ECHODATASET_PATH = config['echodataset_path'] # Default\n",
    "\n",
    "ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-05-subjects'\n",
    "    #\"BATCH_SIZE_OF_CLIPS\": 20,\n",
    "    #\"FRAMES_PER_CLIP\": 1,\n",
    "    #\"Train Dataset Size\": 720,\n",
    "    #\"Test Dataset Size\": 320,\n",
    "    #\"Validation Dataset Size\": 280,\n",
    "\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects'\n",
    "    #\"BATCH_SIZE_OF_CLIPS\": 20,\n",
    "    #\"FRAMES_PER_CLIP\": 1,\n",
    "    #\"Train Dataset Size\": 1280,\n",
    "    #\"Test Dataset Size\": 560,\n",
    "    #\"Validation Dataset Size\": 440,\n",
    "\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-20-subjects'\n",
    "    #\"BATCH_SIZE_OF_CLIPS\": 20,\n",
    "    #\"FRAMES_PER_CLIP\": 1,\n",
    "    #\"Train Dataset Size\": 2240,\n",
    "    #\"Test Dataset Size\": 960,\n",
    "    #\"Validation Dataset Size\": 720,\n",
    "\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-31-subjects'\n",
    "    #\"BATCH_SIZE_OF_CLIPS\": 20,\n",
    "    #\"FRAMES_PER_CLIP\": 1,\n",
    "    #\"Train Dataset Size\": 3620,\n",
    "    #\"Test Dataset Size\": 1540,\n",
    "    #\"Validation Dataset Size\": 880,\n",
    "\n",
    "\n",
    "\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-in-verification40-49'\n",
    "\n",
    "\n",
    "\n",
    "TRAINING_SPLITTING = 0.80 #config['ntraining'] #Default\n",
    "FLAG_RANDOMISE_DATA=True #config['randomise_file_list'] #Default\n",
    "\n",
    "split_train_validate_sets(  \n",
    "                        ECHODATASET_PATH, #config['echodataset_path']\n",
    "                        config['data_list_output_path'], \n",
    "                        TRAINING_SPLITTING,\n",
    "                        FLAG_RANDOMISE_DATA\n",
    "                        )\n",
    "\n",
    "# PRETRANSFORM_IM_SIZE = [64, 64] #[650, 690] original pixel size for VenueGO\n",
    "PRETRANSFORM_IM_SIZE = [128, 128] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = [256, 256] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = [512, 512] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = config['pretransform_im_size'] ##DEFAULT\n",
    "\n",
    "### >> CHANGE DENSE LAYER FEATURES IN VGG3D\n",
    "### >> `self.fc0 = nn.Linear(in_features=4194304, out_features=500) #128x128`\n",
    "\n",
    "##############################\n",
    "##### Experiments for Basic HYPERPARAMETER Heuristics \n",
    "\n",
    "#### TESTS\n",
    "NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 1; BATCH_SIZE_OF_CLIPS = 13; LEARNING_RATE= 0.00005; \n",
    "\n",
    "\n",
    "#################  LEARNING_RATE \n",
    "### EXPERIMENT 01,02,03,04\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 1; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.00005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.0000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.00000005; \n",
    "\n",
    "#################  BATCH_SIZE_OF_CLIPS with LEARNING_RATE= 0.000005 as it is the best peformance of prevous LRs \n",
    "### EXPERIMENT 04,06,07,08\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 2; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 5; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 15; LEARNING_RATE= 0.000005; \n",
    "\n",
    "#################  NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP with LEARNING_RATE= 0.000005 and BATCH_SIZE_OF_CLIPS=10\n",
    "### EXPERIMENT 09,10,11,12\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 2; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 7; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 13; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 20; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "\n",
    "##TOADD\n",
    "### * NUMBER_OF_FRAMES_AND PARTICPANTS\n",
    "### * OPTIMISERS \n",
    "\n",
    "# LEARNING_RATE =Trial and Error with diffent values  0.00005; 0.0005; 0.005 and 0.000001; 0.00001; 0.0001; 0.001  \n",
    "\n",
    "\n",
    "MAX_EPOCHS = 500 #Alternatvely, make use of: config['max_epochs']\n",
    "\n",
    "\n",
    "##############################\n",
    "##### Setting up animation\n",
    "interval_between_frames_in_milliseconds=33.3 ## 1/30=0.033333\n",
    "frame_per_seconds_for_animated_frames=30\n",
    "\n",
    "\n",
    "#SUBJECT_ID = '073'\n",
    "#print(SUBJECT_ID)\n",
    "\n",
    "### CUDA out of memory \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10\n",
    "#PRETRANSFORM_IM_SIZE = [128, 128] \n",
    "#RuntimeError: CUDA out of memory. Tried to allocate 7.81 GiB (GPU 0; 15.74 GiB total capacity; 8.51 GiB already allocated; 5.05 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
    "## REBOOT MACHINE\n",
    "#RuntimeError: CUDA out of memory. Tried to allocate 7.81 GiB (GPU 0; 15.74 GiB total capacity; 8.51 GiB already allocated; 5.13 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "## Setting labels\n",
    "label_id = ('BKGR', '4CV')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:08.264310Z",
     "start_time": "2021-12-17T09:18:08.250178Z"
    }
   },
   "source": [
    "## 2. Setting variables and loading datasets using pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T16:58:31.512456Z",
     "start_time": "2022-07-25T16:58:31.220610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining transforms that apply to the entire dataset.\n",
    "# These transforms are not augmentation.\n",
    "if config['use_pretransform_image_size']:\n",
    "    pretransform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=PRETRANSFORM_IM_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    pretransform = None\n",
    "    \n",
    "    #config['use_train_augmentation']#Default\n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_train_augmentation']:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),  # in degrees\n",
    "        transforms.RandomEqualize(p=0.5),\n",
    "        #transforms.RandomRotation(degrees=2),  # in degrees\n",
    "        #transforms.RandomEqualize(p=0.2),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_validation_augmentation']:\n",
    "    val_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=5),  # in degrees\n",
    "    #transforms.RandomEqualize(p=0.5),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "\n",
    "\n",
    "train_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_train'],\n",
    "    participant_path_json_list=config['participant_path_json_list_train'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    #transform=None,#transform=train_transform,\n",
    "    transform=train_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "validation_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_validation'],\n",
    "    participant_path_json_list=config['participant_path_json_list_validation'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,#transform=train_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "\n",
    "## Spliting train_dataset into train_set and test_set\n",
    "Ntdt = train_dataset.__len__()\n",
    "ntraining = 0.7\n",
    "\n",
    "Ntrain=round(Ntdt*ntraining)\n",
    "Ntest = round(Ntdt - (Ntdt*ntraining))\n",
    "print(f'Ntrain size: {Ntrain}, Ntest size: {Ntest}, \"Ntrain+Ntest\" size={Ntrain+Ntest}')\n",
    "train_set, test_set = torch.utils.data.random_split(train_dataset, [Ntrain, Ntest])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760906",
   "metadata": {},
   "source": [
    "## 3. Plotting Class Distribution (creates temp clips and it takes few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6472a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T16:59:26.172576Z",
     "start_time": "2022-07-25T16:58:32.298683Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set_class_dict = get_class_distribution(train_set,label_id)\n",
    "val_set_class_dict = get_class_distribution(validation_dataset,label_id)\n",
    "test_set_class_dict = get_class_distribution(test_set,label_id)\n",
    "\n",
    "print(f'class_distribution(train_set): {train_set_class_dict}')\n",
    "print(f'class_distribution(validation_dataset): {val_set_class_dict}' )\n",
    "print(f'class_distribution(test_set): {test_set_class_dict}')\n",
    "    \n",
    "#number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip']    \n",
    "print(f'Number of frames for training datasets {len(train_set)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "print(f'Number of frames for Validation datasets {len(validation_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "print(f'Number of frames for testing datasets {len(test_set)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "\n",
    "plot_title_train_label= f'TRAIN dataset of {len(train_set)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_val_label= f'VALIDATION dataset of {len(validation_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_test_label= f'TEST dataset of {len(test_set)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,7))\n",
    "plot_from_dict(train_set_class_dict, plot_title=plot_title_train_label, ax=axes[0])\n",
    "plot_from_dict(val_set_class_dict, plot_title=plot_title_test_label, ax=axes[1])\n",
    "plot_from_dict(test_set_class_dict, plot_title=plot_title_val_label, ax=axes[2])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ea2a6",
   "metadata": {},
   "source": [
    "## 4. Animating frames of one clip of the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b57ab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T16:59:30.878351Z",
     "start_time": "2022-07-25T16:59:30.794514Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'---------------------------------------')\n",
    "clips=creating_pair_of_clips(train_dataset, label_id)\n",
    "pair_clips_and_labels = pair_clips_labels(clips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49ba14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T16:59:34.428384Z",
     "start_time": "2022-07-25T16:59:34.108448Z"
    }
   },
   "outputs": [],
   "source": [
    "#average_HR =\n",
    "#fps = 30\n",
    "# 60 # beats per minute\n",
    "#Beats-per-minute: 60 BPM\n",
    "#Beats-per-second: 1 Hz\n",
    "#Cycle-per-second: 1 (Cycle/s)\n",
    "\n",
    "PAIR_OF_CLIPS = pair_clips_and_labels[0]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS, label_id, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP,\n",
    "                  interval_between_frames_in_milliseconds)\n",
    "HTML(animated_frames.to_jshtml())      \n",
    "\n",
    "\n",
    "# ##SAVE ANIMATIONS\n",
    "# for idx in range(0,len(pair_clips_labels)):\n",
    "#     PAIR_OF_CLIPS = pair_clips_labels[idx]\n",
    "#     print( f' pair_clips_labels {str(PAIR_OF_CLIPS[2])} {str(PAIR_OF_CLIPS[6])}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d225",
   "metadata": {},
   "source": [
    "## 5. Displayting frames in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8805e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T16:59:38.162493Z",
     "start_time": "2022-07-25T16:59:38.093885Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f'====================================================')\n",
    "print(f'train_dataset.__len__() = {train_set.__len__()}')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f'len(train_dataloader): {len(train_dataloader)} BATCHES of BATCH_SIZE_OF_CLIPS {BATCH_SIZE_OF_CLIPS}')\n",
    "for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "    print(f'  ====================================================')\n",
    "    sample_batched_images=sample_batched[0]\n",
    "    sample_batched_labels=sample_batched[1]\n",
    "    print(f'    BATCH_OF_CLIPS_INDEX : {clip_batch_idx} / {len(train_dataloader) - 1}')\n",
    "    print(f'    sample_batched_labels.size(): {  sample_batched_labels.size()  }')\n",
    "    print(f'    sample_batched_labels.squeeze().size(): {  sample_batched_labels.squeeze().size()  }')\n",
    "    print(f'    sample_batched_images.size(): {sample_batched_images.size()}')\n",
    "\n",
    "    for BATCH_SIZE_IDX, label in enumerate(sample_batched_labels):\n",
    "        print(f'        BATCH_SIZE_IDX {BATCH_SIZE_IDX} ')\n",
    "        print(f'          label: {label}')\n",
    "        sample_batched_idx_image = sample_batched_images[BATCH_SIZE_IDX,...]\n",
    "        print(f'          Sample_batched_idx_image.size()  {sample_batched_idx_image.size() }'  )\n",
    "\n",
    "        grid = utils.make_grid(sample_batched_idx_image)\n",
    "        print(f'          Grid size {grid.size()}' )\n",
    "#         plt.figure(figsize =(20,20) )\n",
    "#         plt.imshow( grid.cpu().detach().numpy().transpose(1, 2, 0) )\n",
    "#         plt.title(f'BATCH_SIZE_IDX {BATCH_SIZE_IDX}; Label: {label_id[label]}')\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f' test_dataset.__len__() = {test_set.__len__()}')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f' validation_dataset.__len__() = {validation_dataset.__len__()}')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e92336",
   "metadata": {},
   "source": [
    "## 7. Define networks\n",
    "See `$HOMErepositories/echocardiography/source/models/learning_misc.py` to amend or implement other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8249058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T17:42:01.404263Z",
     "start_time": "2022-07-25T17:42:01.380733Z"
    }
   },
   "outputs": [],
   "source": [
    "# ######################################\n",
    "# ##### Define TrompNet2022 architecture\n",
    "# class TrompNet2022(nn.Module):\n",
    "\n",
    "#     def __init__(self, input_pixel_size, n_batch_size_of_clips, n_frames_per_clip, n_classes=2):\n",
    "#         \"\"\"\n",
    "#         Simple Video classifier by Tromp et al. 2022. DOI https://doi.org/10.1016/S2589-7500(21)00235-1\n",
    "        \n",
    "        \n",
    "#         The first classifier was a supervised CNN, composed of \n",
    "#             * four convolutional layers, \n",
    "#             * a dense layer, and \n",
    "#             * a softmax output layer. \n",
    "#         This model was trained with a categorical cross-entropy loss function.\n",
    "        \n",
    "#         Args:\n",
    "#             input_pixel_size:  shape of the input image. Should be a 2 element vector for a 2D video (width, height) [e.g. 128, 128].\n",
    "#             n_batch_size_of_clips: (self explanatory)\n",
    "#             n_frames_per_clip: (self explanatory)\n",
    "#             n_classes: number of output classes\n",
    "#         \"\"\"\n",
    "        \n",
    "        \n",
    "#         super(TrompNet2022, self).__init__()\n",
    "#         self.name = 'tromp2022Net'\n",
    "        \n",
    "#         self.input_pixel_size = input_pixel_size #[128, 128]\n",
    "#         self.n_batch_size_of_clips = n_batch_size_of_clips  #BATCH_SIZE_OF_CLIPS\n",
    "#         self.n_frames_per_clip = n_frames_per_clip #NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP\n",
    "#         self.n_classes = n_classes\n",
    "#         #print(f'self.n_frames_per_clip {self.n_frames_per_clip}')\n",
    "#         self.n_features = np.prod(self.input_pixel_size) * self.n_frames_per_clip\n",
    "        \n",
    "\n",
    "#         #self.n_batch_size_of_clips    \n",
    "#         self.conv0 = nn.Conv2d(in_channels=1, \n",
    "#                                out_channels=64,\n",
    "#                                kernel_size=(3, 3), \n",
    "#                                stride=(1, 1), #H_{in}/strideA, W_{in}/strideB\n",
    "#                                padding = (0, 0), \n",
    "#                                #dilation=(0, 0)\n",
    "#                               )\n",
    "#         #Input: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})\n",
    "#         #Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})\n",
    "#         #N is a batch size\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(in_channels=64, \n",
    "#                        out_channels=128,\n",
    "#                        kernel_size=(3, 3), \n",
    "#                        stride=(1, 1), #H_{in}/strideA, W_{in}/strideB\n",
    "#                        padding = (0, 0), \n",
    "#                        #dilation=(0, 0)\n",
    "#                       )\n",
    "\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(in_channels=128, \n",
    "#                        out_channels=32,\n",
    "#                        kernel_size=(3, 3), \n",
    "#                        stride=(1, 1), #H_{in}/strideA, W_{in}/strideB\n",
    "#                        padding = (0, 0), \n",
    "#                        #dilation=(0, 0)\n",
    "#                       )        \n",
    "        \n",
    "#         self.conv3 = nn.Conv2d(in_channels=32, \n",
    "#                        out_channels=10,\n",
    "#                        kernel_size=(3, 3), \n",
    "#                        stride=(1, 1), #H_{in}/strideA, W_{in}/strideB\n",
    "#                        padding = (0, 0), \n",
    "#                        #dilation=(0, 0)\n",
    "#                       )\n",
    "                \n",
    "        \n",
    "#         self.flatten = nn.Flatten()\n",
    "#         #self.relu = nn.ReLU()\n",
    "#         #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#         self.fc0 = nn.Linear(in_features=144000, out_features=self.n_classes)\n",
    "\n",
    "#         ### Softmax\n",
    "#         #self.softmax = nn.Softmax() # UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
    "#         #self.softmax = nn.Softmax(dim=0) #along row\n",
    "#         self.softmax = nn.Softmax(dim=1) #along the column (for linear output)\n",
    "#         #https://discuss.pytorch.org/t/implicit-dimension-choice-for-softmax-warning/12314/12\n",
    "        \n",
    "\n",
    "        \n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         # print(f'x.shape(): {x.size()}') ##[batch_size, channels, depth, height, width]\n",
    "#         # x = x.permute(0,2,1,3,4)##[batch_size, depth, channels, height, width]\n",
    "#         #print(f'x.shape(): {x.size()}') # torch.Size([20, 1, 1, 128, 128]) \n",
    "#         x = torch.squeeze(x,dim=1)\n",
    "#         #x = torch.unsqueeze(x,dim=1)\n",
    "#         #print(f'x.shape(): {x.size()}') # torch.Size([20, 1, 128, 128])\n",
    "        \n",
    "#         #print(f'X.shape(): {x.size()}')\n",
    "#         x = self.conv0(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         #print(f'conv3(x): {x.size()}')\n",
    "        \n",
    "\n",
    "#         x = self.flatten(x)\n",
    "#         #print(f'self.flatten(x) size() {x.size()}')  # x.shape(): torch.Size([4, 983040])\n",
    "#         x = self.fc0(x)\n",
    "#         #print(f'fc0(): {x.size()}')\n",
    "        \n",
    "#         x = self.softmax(x)\n",
    "#         #print(f'x.shape(): {x.size()}')\n",
    "\n",
    "#         return x\n",
    "\n",
    "# ######################################\n",
    "# ##### LeNet5 https://medium.datadriveninvestor.com/cnn-architectures-from-scratch-c04d66ac20c2\n",
    "# class LeNet5(nn.Module):\n",
    "#     def __init__(self, n_classes=2):\n",
    "        \n",
    "#         super(LeNet5, self).__init__()\n",
    "#         self.name = 'LeNet5'\n",
    "        \n",
    "#         self.tanh = nn.Tanh()      \n",
    "#         self.pool = nn.AvgPool2d(kernel_size=(2,2),stride=(2,2))\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=(5,5),stride=(1,1))\n",
    "#         self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=(5,5),stride=(1,1))\n",
    "#         self.conv3 = nn.Conv2d(in_channels=16,out_channels=120,kernel_size=(5,5),stride=(1,1))\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear1 = nn.Linear(75000, 84)\n",
    "#         self.linear2 = nn.Linear(84, n_classes)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = torch.squeeze(x,dim=1)\n",
    "#         x = self.tanh(self.conv1(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = self.tanh(self.conv2(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = self.tanh(self.conv3(x))\n",
    "#         #print(f'x.shape(): {x.size()}') \n",
    "#         #x = x.reshape(x.shape[0],-1) #75000\n",
    "#         x = self.flatten(x)\n",
    "# #         print(f'x.shape(): {x.size()}') \n",
    "#         x = self.tanh(self.linear1(x))\n",
    "#         x = self.linear2(x)\n",
    "#         return x    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######################################\n",
    "# ##### LeNet5 https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/\n",
    "# class LeNet5(nn.Module):\n",
    "#     def __init__(self, n_classes=2):\n",
    "        \n",
    "#         super(LeNet5, self).__init__()\n",
    "#         self.name = 'LeNet5'\n",
    "        \n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(6),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "#         self.fc = nn.Linear(13456, 120)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc1 = nn.Linear(120, 84)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(84, n_classes)\n",
    "    \n",
    "#     def forward(self,x):\n",
    "#         x = torch.squeeze(x,dim=1)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = x.reshape(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         return x    \n",
    "\n",
    "\n",
    "\n",
    "# #https://github.com/lychengr3x/LeNet-5-Implementation-Using-Pytorch/blob/master/LeNet-5%20Implementation%20Using%20Pytorch.ipynb\n",
    "# class LeNet5(nn.Module):\n",
    "\n",
    "#     # network structure\n",
    "#     def __init__(self, n_classes=2):\n",
    "        \n",
    "#         super(LeNet5, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         #self.fc1   = nn.Linear(16*5*5, 120)\n",
    "#         self.fc1   = nn.Linear(14400, 120)\n",
    "#         self.fc2   = nn.Linear(120, 84)\n",
    "#         self.fc3   = nn.Linear(84, n_classes)\n",
    "#         self.flatten = nn.Flatten()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         '''\n",
    "#         One forward pass through the network.\n",
    "        \n",
    "#         Args:\n",
    "#             x: input\n",
    "#         '''\n",
    "#         x = torch.squeeze(x,dim=1)\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "#         #x = x.view(-1, self.num_flat_features(x))\n",
    "#         #x = x.reshape(x.size(0), -1)\n",
    "#         x = self.flatten(x)\n",
    "#         #print(f'x.shape(): {x.size()}') \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# ##     def num_flat_features(self, x):\n",
    "# ##         '''\n",
    "# ##         Get the number of features in a batch of tensors `x`.\n",
    "# ##         '''\n",
    "# ##         size = x.size()[1:]\n",
    "# ##         return np.prod(size)\n",
    "\n",
    "\n",
    "\n",
    "# class AlexNet(nn.Module):\n",
    "#     def __init__(self, n_classes=2):\n",
    "#         super(AlexNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.LocalResponseNorm(size=5,alpha=0.0001,beta=0.75,k=2),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         )\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(256 * 6 * 6, 4096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(4096, n_classes),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.squeeze(x,dim=1)\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# #https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py    \n",
    "# class AlexNet(nn.Module):\n",
    "#     def __init__(self, n_classes=2):\n",
    "#         super(AlexNet, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "# #             nn.LocalResponseNorm(size=5,alpha=0.0001,beta=0.75,k=2),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#         )\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(256 * 6 * 6, 4096),\n",
    "#             #nn.Linear(9216, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(4096, n_classes),\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.squeeze(x,dim=1)\n",
    "#         #print(f'x.shape(): {x.size()}') \n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         #print(f'x.shape(): {x.size()}') \n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "# ##https://blog.paperspace.com/alexnet-pytorch/    \n",
    "# # WORKING OK\n",
    "# class AlexNet(nn.Module):\n",
    "#     def __init__(self, n_classes=2):\n",
    "#         super(AlexNet, self).__init__()\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=0),\n",
    "#             nn.BatchNorm2d(96),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(384),\n",
    "#             nn.ReLU())\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(384),\n",
    "#             nn.ReLU())\n",
    "#         self.layer5 = nn.Sequential(\n",
    "#             nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Dropout(0.5),\n",
    "#             #nn.Linear(9216, 4096),            \n",
    "#             nn.Linear(1024, 4096),\n",
    "#             nn.ReLU())\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU())\n",
    "#         self.fc2= nn.Sequential(\n",
    "#             nn.Linear(4096, n_classes))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.squeeze(x,dim=1)\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = self.layer5(x)\n",
    "#         x = x.reshape(x.size(0), -1)\n",
    "#         #print(f'x.shape(): {x.size()}')\n",
    "#         #x = torch.flatten(x, 1)\n",
    "#         x = self.fc(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x \n",
    "\n",
    "\n",
    "# #https://developpaper.com/example-of-pytorch-implementing-alexnet/\n",
    "# class AlexNet(nn.Module):\n",
    "#   def __init__(self,n_classes=2):\n",
    "#     super(AlexNet,self).__init__()\n",
    "#     self.feature_extraction = nn.Sequential(\n",
    "#       nn.Conv2d(in_channels=1,out_channels=96,kernel_size=11,stride=4,padding=2,bias=False),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.MaxPool2d(kernel_size=3,stride=2,padding=0),\n",
    "#       nn.Conv2d(in_channels=96,out_channels=192,kernel_size=5,stride=1,padding=2,bias=False),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.MaxPool2d(kernel_size=3,stride=2,padding=0),\n",
    "#       nn.Conv2d(in_channels=192,out_channels=384,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n",
    "#     )\n",
    "#     self.classifier = nn.Sequential(\n",
    "#       nn.Dropout(p=0.5),\n",
    "#       #nn.Linear(in_features=256*6*6,out_features=4096),\n",
    "#       nn.Linear(in_features=2304,out_features=4096),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.Dropout(p=0.5),\n",
    "#       nn.Linear(in_features=4096, out_features=4096),\n",
    "#       nn.ReLU(inplace=True),\n",
    "#       nn.Linear(in_features=4096, out_features=n_classes),\n",
    "#     )\n",
    "#   def forward(self,x):\n",
    "#     x = torch.squeeze(x,dim=1)\n",
    "#     x = self.feature_extraction(x)\n",
    "#     #print(f'x.shape(): {x.size()}')\n",
    "#     #x = x.view(x.size(0),256*6*6)\n",
    "#     x = torch.flatten(x, 1)\n",
    "#     #print(f'x.shape(): {x.size()}')\n",
    "#     x = self.classifier(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# #https://github.com/jmjeon94/MobileNet-Pytorch/blob/master/MobileNetV1.py\n",
    "# #https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69\n",
    "# class MobileNetV1(nn.Module):\n",
    "#     def __init__(self, ch_in, n_classes):\n",
    "#         super(MobileNetV1, self).__init__()\n",
    "\n",
    "#         def conv_bn(inp, oup, stride):\n",
    "#             return nn.Sequential(\n",
    "#                 nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "#                 nn.BatchNorm2d(oup),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#                 )\n",
    "\n",
    "#         def conv_dw(inp, oup, stride):\n",
    "#             return nn.Sequential(\n",
    "#                 # dw: depthwise convolution \n",
    "#                 nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "#                 nn.BatchNorm2d(inp),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "\n",
    "#                 # pw: pointwise convolution\n",
    "#                 nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "#                 nn.BatchNorm2d(oup),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 )\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             conv_bn(ch_in, 32, 2),\n",
    "#             conv_dw(32, 64, 1),\n",
    "#             conv_dw(64, 128, 2),\n",
    "#             conv_dw(128, 128, 1),\n",
    "#             conv_dw(128, 256, 2),\n",
    "#             conv_dw(256, 256, 1),\n",
    "#             conv_dw(256, 512, 2),\n",
    "#             conv_dw(512, 512, 1),\n",
    "#             conv_dw(512, 512, 1),\n",
    "#             conv_dw(512, 512, 1),\n",
    "#             conv_dw(512, 512, 1),\n",
    "#             conv_dw(512, 512, 1),\n",
    "#             conv_dw(512, 1024, 2),\n",
    "#             conv_dw(1024, 1024, 1),\n",
    "#             nn.AdaptiveAvgPool2d(1)\n",
    "#         )\n",
    "#         self.fc = nn.Linear(1024, n_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.squeeze(x,dim=1)\n",
    "#         x = self.model(x)\n",
    "#         x = x.view(-1, 1024)\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# ############################################################################\n",
    "# ## https://github.com/jmjeon94/MobileNet-Pytorch/blob/master/MobileNetV2.py\n",
    "# ## MobileNetV2: Inverted Residuals and Linear Bottlenecks\n",
    "# ## https://arxiv.org/abs/1801.04381\n",
    "# def dwise_conv(ch_in, stride=1):\n",
    "#     return (\n",
    "#         nn.Sequential(\n",
    "#             #depthwise\n",
    "#             nn.Conv2d(ch_in, ch_in, kernel_size=3, padding=1, stride=stride, groups=ch_in, bias=False),\n",
    "#             nn.BatchNorm2d(ch_in),\n",
    "#             nn.ReLU6(inplace=True),\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# def conv1x1(ch_in, ch_out):\n",
    "#     return (\n",
    "#         nn.Sequential(\n",
    "#             nn.Conv2d(ch_in, ch_out, kernel_size=1, padding=0, stride=1, bias=False),\n",
    "#             nn.BatchNorm2d(ch_out),\n",
    "#             nn.ReLU6(inplace=True)\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# def conv3x3(ch_in, ch_out, stride):\n",
    "#     return (\n",
    "#         nn.Sequential(\n",
    "#             nn.Conv2d(ch_in, ch_out, kernel_size=3, padding=1, stride=stride, bias=False),\n",
    "#             nn.BatchNorm2d(ch_out),\n",
    "#             nn.ReLU6(inplace=True)\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# class InvertedBlock(nn.Module):\n",
    "#     def __init__(self, ch_in, ch_out, expand_ratio, stride):\n",
    "#         super(InvertedBlock, self).__init__()\n",
    "\n",
    "#         self.stride = stride\n",
    "#         assert stride in [1,2]\n",
    "\n",
    "#         hidden_dim = ch_in * expand_ratio\n",
    "\n",
    "#         self.use_res_connect = self.stride==1 and ch_in==ch_out\n",
    "\n",
    "#         layers = []\n",
    "#         if expand_ratio != 1:\n",
    "#             layers.append(conv1x1(ch_in, hidden_dim))\n",
    "#         layers.extend([\n",
    "#             #dw\n",
    "#             dwise_conv(hidden_dim, stride=stride),\n",
    "#             #pw\n",
    "#             conv1x1(hidden_dim, ch_out)\n",
    "#         ])\n",
    "\n",
    "#         self.layers = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.use_res_connect:\n",
    "#             return x + self.layers(x)\n",
    "#         else:\n",
    "#             return self.layers(x)\n",
    "\n",
    "# class MobileNetV2(nn.Module):\n",
    "#     def __init__(self, ch_in=3, n_classes=1000):\n",
    "#         super(MobileNetV2, self).__init__()\n",
    "\n",
    "#         self.configs=[\n",
    "#             # t, c, n, s\n",
    "#             [1, 16, 1, 1],\n",
    "#             [6, 24, 2, 2],\n",
    "#             [6, 32, 3, 2],\n",
    "#             [6, 64, 4, 2],\n",
    "#             [6, 96, 3, 1],\n",
    "#             [6, 160, 3, 2],\n",
    "#             [6, 320, 1, 1]\n",
    "#         ]\n",
    "\n",
    "#         self.stem_conv = conv3x3(ch_in, 32, stride=2)\n",
    "\n",
    "#         layers = []\n",
    "#         input_channel = 32\n",
    "#         for t, c, n, s in self.configs:\n",
    "#             for i in range(n):\n",
    "#                 stride = s if i == 0 else 1\n",
    "#                 layers.append(InvertedBlock(ch_in=input_channel, ch_out=c, expand_ratio=t, stride=stride))\n",
    "#                 input_channel = c\n",
    "\n",
    "#         self.layers = nn.Sequential(*layers)\n",
    "\n",
    "#         self.last_conv = conv1x1(input_channel, 1280)\n",
    "\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout2d(0.2),\n",
    "#             nn.Linear(1280, n_classes)\n",
    "#         )\n",
    "#         self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.squeeze(x, dim=1)\n",
    "#         x = self.stem_conv(x)\n",
    "#         x = self.layers(x)\n",
    "#         x = self.last_conv(x)\n",
    "#         x = self.avg_pool(x).view(-1, 1280)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ############################################################################\n",
    "# ## https://github.com/nrjanjanam/shufflenet-v1-pytorch/blob/main/code/ShuffleNet_Implementation.ipynb\n",
    "# ## ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\n",
    "# ## Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun [v2] Thu, 7 Dec 2017 18:06:34 UTC (110 KB)\n",
    "# ## https://arxiv.org/abs/1707.01083 \n",
    "# ##\n",
    "# ## > RuntimeError: Given groups=1, weight of size [24, 3, 1, 1], \n",
    "# ## expected input[13, 1, 128, 128] to have 3 channels, but got 1 channels instead\n",
    "# ## \n",
    "# ## criterion = nn.CrossEntropyLoss()\n",
    "# ## optimizer = optim.SGD(net3.parameters(), lr = 0.01,momentum=0.9, weight_decay=5e-4)\n",
    "# ##\n",
    "# class ShuffleBlock(nn.Module):\n",
    "#   def __init__(self, groups):\n",
    "#     super(ShuffleBlock, self).__init__()\n",
    "#     self.groups = groups\n",
    "#   def forward(self, x):\n",
    "#     '''Channel shuffle: [N,C,H,W] -> [N, g, C/g, H, W] -> [N, C/g, g, H, W] -> [N,C,H,W]'''\n",
    "#     N,C,H,W = x.size()\n",
    "#     g = self.groups\n",
    "#     return x.view(N, g, C//g, H, W).permute(0, 2, 1, 3, 4).reshape(N, C, H, W)\n",
    "\n",
    "\n",
    "\n",
    "# class Bottleneck(nn.Module): \n",
    "#   def __init__(self, in_planes, out_planes, stride, groups):\n",
    "#     super(Bottleneck, self).__init__()\n",
    "#     self.stride = stride\n",
    "#     mid_planes = int(out_planes/4)\n",
    "#     g = 1 if in_planes == 24 else groups\n",
    "#     self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n",
    "#     self.bn1 = nn.BatchNorm2d(mid_planes)\n",
    "#     self.shuffle1 = ShuffleBlock(groups= g)\n",
    "#     self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)\n",
    "#     self.bn2 = nn.BatchNorm2d(mid_planes)\n",
    "#     self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n",
    "#     self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "#     self.shortcut = nn.Sequential()\n",
    "#     if stride==2:\n",
    "#       self.shortcut = nn.Sequential(nn.AvgPool2d(3,stride=2, padding =1))\n",
    "#   def forward(self,x):\n",
    "#     out = functions.relu(self.bn1(self.conv1(x)))\n",
    "#     out = self.shuffle1(out)\n",
    "#     out = functions.relu(self.bn2(self.conv2(out)))\n",
    "#     out = self.bn3(self.conv3(out))\n",
    "#     res = self.shortcut(x)\n",
    "#     out = functions.relu(torch.cat([out,res], 1)) if self.stride==2 else functions.relu(out+res)\n",
    "#     return out\n",
    "\n",
    "\n",
    "\n",
    "# class ShuffleNetV1(nn.Module):\n",
    "#   def __init__(self, cfg):\n",
    "#     super(ShuffleNetV1, self).__init__()\n",
    "#     out_planes = cfg['out_planes']\n",
    "#     num_blocks = cfg['num_blocks']\n",
    "#     groups = cfg['groups']\n",
    "#     self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias = False)\n",
    "#     self.bn1 = nn.BatchNorm2d(24)\n",
    "#     self.in_planes = 24\n",
    "#     self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n",
    "#     self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n",
    "#     self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n",
    "#     self.linear = nn.Linear(out_planes[2], 2) #2 as there are 2 classes\n",
    "\n",
    "#   def _make_layer(self, out_planes, num_blocks, groups):\n",
    "#     layers = []\n",
    "#     for i in range(num_blocks):\n",
    "#       stride = 2 if i == 0 else 1\n",
    "#       cat_planes = self.in_planes if i==0 else 0\n",
    "#       layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))\n",
    "#       self.in_planes = out_planes\n",
    "#     return nn.Sequential(*layers)\n",
    "  \n",
    "#   def forward(self,x):\n",
    "#     x = torch.squeeze(x, dim=1)\n",
    "#     x = F.relu(self.bn1(self.conv1(x)))\n",
    "#     x = self.layer1(x)\n",
    "#     x = self.layer2(x)\n",
    "#     x = self.layer3(x)\n",
    "#     x = F.avg_pool2d(x, 4)\n",
    "#     x = out.view(x.size(0), -1)\n",
    "#     x = self.linear(x)\n",
    "#     return x\n",
    "\n",
    "# # def ShuffleNetG2():\n",
    "# #   cfg = {'out_planes': [200, 400, 800],\n",
    "# #          'num_blocks': [4, 8, 4],\n",
    "# #          'groups': 2\n",
    "# #          }\n",
    "# #   return ShuffleNet(cfg)\n",
    "\n",
    "# def ShuffleNetV1_G3():\n",
    "#   cfg = {'out_planes': [240, 480, 960],\n",
    "#          'num_blocks': [4, 8, 4],\n",
    "#          'groups': 3\n",
    "#          }\n",
    "#   return ShuffleNetV1(cfg)\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "## https://github.com/ericsun99/Shufflenet-v2-Pytorch/blob/master/ShuffleNetV2.py\n",
    "## ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\n",
    "## Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun [v1] Mon, 30 Jul 2018 04:18:25 UTC (4,805 KB)\n",
    "## https://arxiv.org/abs/1807.11164\n",
    "##\n",
    "## Given groups=1, weight of size [24, 3, 3, 3], expected input[13, 1, 128, 128] to have 3 channels, but got 1 channels instead\n",
    "##\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "    batchsize, num_channels, height, width = x.data.size()\n",
    "\n",
    "    channels_per_group = num_channels // groups\n",
    "    \n",
    "    # reshape\n",
    "    x = x.view(batchsize, groups, \n",
    "        channels_per_group, height, width)\n",
    "\n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "\n",
    "    # flatten\n",
    "    x = x.view(batchsize, -1, height, width)\n",
    "\n",
    "    return x\n",
    "    \n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, benchmodel):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.benchmodel = benchmodel\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        oup_inc = oup//2\n",
    "        \n",
    "        if self.benchmodel == 1:\n",
    "            #assert inp == oup_inc\n",
    "            self.banch2 = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup_inc),\n",
    "                nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n",
    "                nn.BatchNorm2d(oup_inc),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup_inc),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )                \n",
    "        else:                  \n",
    "            self.banch1 = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup_inc),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )        \n",
    "                \n",
    "            self.banch2 = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, oup_inc, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup_inc),\n",
    "                nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(oup_inc, oup_inc, 3, stride, 1, groups=oup_inc, bias=False),\n",
    "                nn.BatchNorm2d(oup_inc),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(oup_inc, oup_inc, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup_inc),\n",
    "                nn.ReLU(inplace=True),\n",
    "                )\n",
    "          \n",
    "    @staticmethod\n",
    "    def _concat(x, out):\n",
    "        # concatenate along channel axis\n",
    "        return torch.cat((x, out), 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if 1==self.benchmodel:\n",
    "            x1 = x[:, :(x.shape[1]//2), :, :]\n",
    "            x2 = x[:, (x.shape[1]//2):, :, :]\n",
    "            out = self._concat(x1, self.banch2(x2))\n",
    "        elif 2==self.benchmodel:\n",
    "            out = self._concat(self.banch1(x), self.banch2(x))\n",
    "\n",
    "        return channel_shuffle(out, 2)\n",
    "\n",
    "\n",
    "class ShuffleNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(ShuffleNetV2, self).__init__()\n",
    "        \n",
    "        assert input_size % 32 == 0\n",
    "        \n",
    "        self.stage_repeats = [4, 8, 4]\n",
    "        # index 0 is invalid and should never be called.\n",
    "        # only used for indexing convenience.\n",
    "        if width_mult == 0.5:\n",
    "            self.stage_out_channels = [-1, 24,  48,  96, 192, 1024]\n",
    "        elif width_mult == 1.0:\n",
    "            self.stage_out_channels = [-1, 24, 116, 232, 464, 1024]\n",
    "        elif width_mult == 1.5:\n",
    "            self.stage_out_channels = [-1, 24, 176, 352, 704, 1024]\n",
    "        elif width_mult == 2.0:\n",
    "            self.stage_out_channels = [-1, 24, 224, 488, 976, 2048]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"\"\"{} groups is not supported for\n",
    "                       1x1 Grouped Convolutions\"\"\".format(num_groups))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = self.stage_out_channels[1]\n",
    "        self.conv1 = conv_bn(3, input_channel, 2)    \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.features = []\n",
    "        # building inverted residual blocks\n",
    "        for idxstage in range(len(self.stage_repeats)):\n",
    "            numrepeat = self.stage_repeats[idxstage]\n",
    "            output_channel = self.stage_out_channels[idxstage+2]\n",
    "            for i in range(numrepeat):\n",
    "                if i == 0:\n",
    "                    #inp, oup, stride, benchmodel):\n",
    "                    self.features.append(InvertedResidual(input_channel, output_channel, 2, 2))\n",
    "                else:\n",
    "                    self.features.append(InvertedResidual(input_channel, output_channel, 1, 1))\n",
    "                input_channel = output_channel\n",
    "                \n",
    "                \n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building last several layers\n",
    "        self.conv_last      = conv_1x1_bn(input_channel, self.stage_out_channels[-1])\n",
    "        self.globalpool = nn.Sequential(nn.AvgPool2d(int(input_size/32)))              \n",
    "    \n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(nn.Linear(self.stage_out_channels[-1], n_class))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.squeeze(x, dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.features(x)\n",
    "        x = self.conv_last(x)\n",
    "        x = self.globalpool(x)\n",
    "        x = x.view(-1, self.stage_out_channels[-1])\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def shufflenetv2(width_mult=1.):\n",
    "    model = ShuffleNetV2(width_mult=width_mult)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb1dd6",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Sanity checks for the model and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5dd8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T17:42:02.448951Z",
     "start_time": "2022-07-25T17:42:02.372480Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "##### Tensor Shape\n",
    "#tensor_shape_size = [BATCH_SIZE_OF_CLIPS, config['number_of_frames_per_segment_in_a_clip'], 1, 128, 128]\n",
    "#model = basicVGGNet(tensor_shape_size)\n",
    "\n",
    "#n_frames_per_clip \n",
    "#model = VGG3D(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "#model = basicVGG(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #torch.Size([BATCH_SIZE_OF_CLIPS, CLASS_NUMBER])\n",
    "#model = TrompNet2022(PRETRANSFORM_IM_SIZE, BATCH_SIZE_OF_CLIPS, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "#model = LeNet5()\n",
    "#model = AlexNet()\n",
    "#model=MobileNetV1(ch_in=1, n_classes=2)\n",
    "#model=MobileNetV2(ch_in=1, n_classes=2)\n",
    "#model=ShuffleNetV1_G3()\n",
    "model = shufflenetv2()\n",
    "\n",
    "model.to(DEVICE) # Place model on GPU\n",
    "\n",
    "print(model) \n",
    "#print(str(summary(model, (1, 32, 32), depth=1)))\n",
    "\n",
    "### Sanity check\n",
    "#print(len(train_dataloader)) #6 BATCHES of 10=BATCH_SIZE_OF_CLIPS\n",
    "sample_batched = next(iter(train_dataloader))\n",
    "#print(sample_batched[0].shape) #torch.Size([10, 60, 1, 128, 128])\n",
    "#print(sample_batched[1])#tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1])\n",
    "#print(sample_batched[2]) #tensor([47, 42,  0, 51, 49, 75, 67, 67, 62, 84])\n",
    "#print(sample_batched[3]) #tensor([105, 102,  43, 106,  94, 161, 151, 183, 150, 151])\n",
    "\n",
    "clip_batch = sample_batched[0]\n",
    "#print(f'clip_batch.size() {clip_batch.size()}') ##torch.Size([4, 60, 1, 128, 128])\n",
    "#                                                ##[batch_size, channels, depth, height, width]\n",
    "\n",
    "# # frames = image.to(device)\n",
    "print(model(clip_batch).shape) #torch.Size([4, 2])\n",
    "print(  type(model(clip_batch))  )  #torch.Size([4, 2]) #<class 'torch.Tensor'>\n",
    "\n",
    "# #https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
    "del sample_batched\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22976ae2",
   "metadata": {},
   "source": [
    "## 8. Define Optimizer\n",
    "1. Set learning rate for how much the model is updated per batch.\n",
    "2. Set total epoch number, as we have shuffle and random transforms, so the training data of every epoch is different.\n",
    "3. Set the number of clips per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295fe94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:42:53.304366Z",
     "start_time": "2022-07-21T18:42:53.271992Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model = VGG3D(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "#model = basicVGG(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #torch.Size([BATCH_SIZE_OF_CLIPS, CLASS_NUMBER])\n",
    "#model = TrompNet2022(PRETRANSFORM_IM_SIZE, BATCH_SIZE_OF_CLIPS, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# ##############\n",
    "# #### LeNet5\n",
    "# model = LeNet5()\n",
    "# gamma=.001 #LetNet_5()\n",
    "# rho=.9 #LetNet_5()\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum=rho) #LeNet5\n",
    "\n",
    "##############\n",
    "# #### AlexNet\n",
    "# model = AlexNet()\n",
    "# #gamma=0.005#AlexNet\n",
    "# gamma=0.001#AlexNet\n",
    "# pshi=0.005#AlexNet\n",
    "# rho=0.9#AlexNet\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, weight_decay = pshi, momentum = rho)  #AlexNet\n",
    "\n",
    "##############\n",
    "# #### MobileNetV1\n",
    "# #https://docs.netspresso.ai/docs/classification-mobilenet-v1-on-cifar100\n",
    "# model=MobileNetV1(ch_in=1, n_classes=2)\n",
    "# gamma=0.1\n",
    "# #pshi=0.005\n",
    "# rho=0.9\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum = rho) \n",
    "\n",
    "\n",
    "\n",
    "#### MobileNetV2\n",
    "model=MobileNetV2(ch_in=1, n_classes=2)\n",
    "gamma=0.01\n",
    "#pshi=0.005\n",
    "rho=0.9\n",
    "model.to(DEVICE) # Place model on GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum = rho) \n",
    "\n",
    "# A Flower Classification Approach with MobileNetV2 and Transfer Learning (2020)\n",
    "#   https://isciia2020.bit.edu.cn/docs/20201114083020836285.pdf                  \n",
    "\n",
    "\n",
    "# #############\n",
    "# #### MobileNetV2\n",
    "# #MobileNetV2: Inverted Residuals and Linear Bottlenecks: paper: https://arxiv.org/abs/1801.04381\n",
    "# model=MobileNetV2(ch_in=1, n_classes=2)\n",
    "# rho=0.045 #Learning rate\n",
    "# mu=0.9 # momentum\n",
    "# lambda_ = 0.00004 # weight decay\n",
    "# #learning rate decay rate of 0.98 per epoch?\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=rho, momentum = mu, weight_decay=lambda_) \n",
    "# # https://arxiv.org/pdf/1801.04381.pdf\n",
    "# # https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html \n",
    "\n",
    "\n",
    "\n",
    "# #https://github.com/d-li14/mobilenetv2.pytorch/issues/2#issuecomment-454282418\n",
    "#torch.optim.SGD                    \n",
    "\n",
    "\n",
    "# rho=0.001 #Learning rate #https://www.kaggle.com/code/gpiosenka/mobilenet-v2-transfer-learning-99-accuracy/notebook\n",
    "# optimizer = torch.optim.Adam(lr=rho)\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## PRINT MODEL\n",
    "print(f'====================================================')\n",
    "print(model)\n",
    "\n",
    "# ### PRINT model.named_parameters\n",
    "# print(f'====================================================')\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98807711",
   "metadata": {},
   "source": [
    "## 9. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee627c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:48:07.502078Z",
     "start_time": "2022-07-21T18:42:53.305323Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TRAINING\n",
    "#clip_batch_size = tuple(train_dataloader.dataset.__getitem__(0)[0].shape) ##(60, 1, 128, 128) frames, chs, [width, height]\n",
    "#print(clip_batch_size)\n",
    "\n",
    "\n",
    "startt = time.time()\n",
    "print(f'Starting training loop {startt}')\n",
    "\n",
    "\n",
    "############################\n",
    "####### BINARY ACCURACY MODULE\n",
    "def binary_accuracy(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    binary_accuracy to calculate accuracy per epoch.\n",
    "    \"\"\"\n",
    "    y_pred_tag = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_tag, dim = 1)\n",
    "    correct_results_sum = (y_pred_tags == y_test).sum().float()\n",
    "    accuracy = correct_results_sum/y_test.shape[0]\n",
    "    accuracy = torch.round(accuracy * 100)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "############################\n",
    "####### TRAIN LOOP MODULE\n",
    "def train_loop(train_dataloader, model, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    train_loop\n",
    "    Arguments:\n",
    "        dataloader, model, criterion, optimizer, device\n",
    "\n",
    "    Return:\n",
    "        train_epoch_loss\n",
    "    \"\"\"\n",
    "    train_epoch_loss = 0\n",
    "    train_acc_loss_epoch = 0\n",
    "    step_train = 0\n",
    "    #size = len(train_dataloader.dataset)\n",
    "    for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "        step_train += 1\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(device), sample_batched[1].to(device)\n",
    "\n",
    "        #print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        # print(f'----------------------------------------------------------')\n",
    "        # print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        # print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        y_train_pred = model(X_train_batch) #torch.Size([9, 2])\n",
    "        #y_train_pred = model(X_train_batch).squeeze()  # torch.Size([9, 2])\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = binary_accuracy(y_train_pred, y_train_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if clip_batch_idx % 10 == 0: ## Print loss values every 10 clip batches\n",
    "        #     train_loss, current = train_loss.item(), clip_batch_idx * len(X_train_batch)\n",
    "        #     print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        train_epoch_loss += train_loss.detach().item()\n",
    "        train_acc_loss_epoch += train_acc.detach().item()\n",
    "\n",
    "    train_epoch_loss /= step_train\n",
    "    train_acc_loss_epoch /= step_train\n",
    "\n",
    "    return train_epoch_loss, train_acc_loss_epoch\n",
    "\n",
    "\n",
    "############################\n",
    "####### TEST LOOP MODULE\n",
    "def test_loop(dataloader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Test loop \n",
    "    \n",
    "    Arguments:\n",
    "        dataloader, model, criterion, optimizer, device\n",
    "\n",
    "    Return:\n",
    "        test_epoch_loss, correct\n",
    "    \"\"\"\n",
    "\n",
    "    train_epoch_acc = 0\n",
    "    step_test = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_epoch_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #model.eval()\n",
    "        #val_epoch_loss = 0\n",
    "        #val_epoch_acc = 0\n",
    "        for clip_batch_idx, sample_val_batched in enumerate(dataloader):\n",
    "            step_test += 1\n",
    "            X_val_batch, y_val_batch = sample_val_batched[0].to(device), sample_val_batched[1].to(device)\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            test_epoch_loss += criterion(y_val_pred, y_val_batch).detach().item()\n",
    "            correct += (y_val_pred.argmax(1) == y_val_batch).type(torch.float).sum().detach().item()\n",
    "\n",
    "    test_epoch_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    return test_epoch_loss, correct\n",
    "\n",
    "\n",
    "#Dictionaries to store the accuracy/epoch and loss/epoch for both train and validation sets.\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    #\"val\": []\n",
    "}\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    #\"val\": []\n",
    "}\n",
    "\n",
    "#for epoch in tqdm(range(1, MAX_EPOCHS)):   \n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"EPOCH {epoch + 1}/{MAX_EPOCHS}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    \n",
    "    train_epoch_loss, train_acc_loss_epoch = train_loop(train_dataloader, model, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    test_epoch_loss, correct = test_loop(val_dataloader, model, criterion, DEVICE)\n",
    "\n",
    "    #print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} | Average Train Acc: {train_epoch_acc:.5f} | Average Validation Loss: {val_epoch_loss:.3f} | Average Validation Acc: {val_epoch_acc:.5f} ')\n",
    "    #print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} |Average Train Acc: {train_epoch_acc:.5f}  ')\n",
    "    \n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_epoch_loss:>8f} \\n\")\n",
    "\n",
    "    print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} Average Train Accuracy Loss: {(train_acc_loss_epoch):>0.1f}% ')\n",
    "    \n",
    "    print(f\"Test Error: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg Test loss: {test_epoch_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "    loss_stats['train'].append(train_epoch_loss)\n",
    "    loss_stats['test'].append(test_epoch_loss)\n",
    "    accuracy_stats['train'].append(train_acc_loss_epoch)\n",
    "    accuracy_stats['test'].append(100*correct)\n",
    "            \n",
    "print(\"DONE TRAINING LOOP!\")\n",
    "\n",
    "\n",
    "\n",
    "endt = time.time()\n",
    "elapsed_time = endt - startt\n",
    "print(f'Finishing training loop {endt}')\n",
    "print(f'Elapsed time for the training loop: {elapsed_time} (s)')\n",
    "\n",
    "#model_path = ' /home/mx19/repositories/echocardiography/models' \n",
    "print(f' {FULL_REPO_MODEL_PATH}')\n",
    "torch.save(model.state_dict(), os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\"))\n",
    "print(\"Saved metric model\")\n",
    "\n",
    "print(loss_stats)\n",
    "print(accuracy_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a7882",
   "metadata": {},
   "source": [
    "## 10. Visualize accuracy and loss performance and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd312af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T19:02:59.107229Z",
     "start_time": "2022-07-21T19:02:58.675054Z"
    }
   },
   "outputs": [],
   "source": [
    "## Convert stats as dataframes\n",
    "loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "## Concatenate dataframes and dictionaries\n",
    "loss_df.insert(0, 'curve', 'Loss', True)\n",
    "acc_df.insert(0, 'curve', 'Acc', True)\n",
    "valuesall = [loss_df, acc_df]\n",
    "values_all = pd.concat(valuesall)\n",
    "\n",
    "all_stats = {\n",
    "    'ACC': accuracy_stats,\n",
    "    'LOS': loss_stats,\n",
    "}\n",
    "\n",
    "\n",
    "## Saving training curves at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "os.chdir(TRAINING_CURVES_PATH)\n",
    "\n",
    "\n",
    "#################################\n",
    "#### PLOTING\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(45,10))\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.lineplot(\n",
    "    data=acc_df, \n",
    "    x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0],    \n",
    "    estimator=None, linewidth=5, palette=\"Set2\" \n",
    "    ).set_title(f'Accuracy/Epoch for Train Val Sets EPOCHS={MAX_EPOCHS} \\\n",
    "                BATCH_SIZE_OF_CLIPS={BATCH_SIZE_OF_CLIPS} \\\n",
    "                NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP={NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} \\\n",
    "                LEARNING_RATE={LEARNING_RATE}')\n",
    "\n",
    "axes[0].set_ylim(0,105)\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.lineplot(\n",
    "    data=loss_df, \n",
    "    x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1],\n",
    "    estimator=None, linewidth=5, palette=\"Set2\" \n",
    "    ).set_title(f'Loss for  EPOCHS={MAX_EPOCHS} \\\n",
    "                BATCH_SIZE_OF_CLIPS={BATCH_SIZE_OF_CLIPS} \\\n",
    "                NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP={NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} \\\n",
    "                LEARNING_RATE={LEARNING_RATE}')\n",
    "axes[1].set_ylim(-0.4, 2)\n",
    "\n",
    "\n",
    "\n",
    "#### PREPARING AND SAVING PERFORMANCE CURVES\n",
    "WIDTH = 3\n",
    "PRECISION = 10\n",
    "TYPE = \"f\"\n",
    "STR_LR = f'{LEARNING_RATE:{WIDTH}.{PRECISION}{TYPE}}'\n",
    "STR_LR = STR_LR.replace(\".\", \"_\", 1)\n",
    "\n",
    "PARAMETERS_FILENAME = \"TRAINset_clips_\"+ \\\n",
    "                    str(len(train_set))+\"TESTset_clips_\"+ \\\n",
    "                    str(len(test_set))+\"VALset_clips\"+ \\\n",
    "                    str(len(validation_dataset))+\"___EPOCHS_\"+str(f'{MAX_EPOCHS:{WIDTH}}') + \\\n",
    "                    \"_BATCH_SIZE_OF_CLIPS_\"+str(f'{BATCH_SIZE_OF_CLIPS:02}')+ \\\n",
    "                    \"_NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP_\"+ \\\n",
    "                    str(NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP)+\"_LEARNING_RATE_\"+STR_LR\n",
    "\n",
    "## Saving training curves at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "os.chdir(TRAINING_CURVES_PATH)\n",
    "TEMP_DICT_TRAINING_CURVES_FOR = \"TEMP_DICT_TRAINING_CURVES_FOR____\"\n",
    "IMAGE_FILE_NAME = TEMP_DICT_TRAINING_CURVES_FOR+PARAMETERS_FILENAME\n",
    "#print(IMAGE_FILE_NAME)\n",
    "fig.savefig(IMAGE_FILE_NAME) \n",
    "\n",
    "## Saving training metrics in dictionaries at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "LOSS_ACC_DICTS_FILE_NAME = \"TEMP_DICT_TRAINING_CURVES_FOR____\"+\"_LOSS_ACC_\"+PARAMETERS_FILENAME+'.json'\n",
    "#print(LOSS_ACC_DICTS_FILE_NAME)\n",
    "#print(all_stats)\n",
    "\n",
    "with open(LOSS_ACC_DICTS_FILE_NAME, 'w') as file:\n",
    "   file.write(json.dumps(all_stats, indent=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aafb41",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791b35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T19:03:16.583772Z",
     "start_time": "2022-07-21T19:03:16.212159Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clip_batch_idx, sample_batched in enumerate(test_dataloader):\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(DEVICE), sample_batched[1].to(DEVICE)\n",
    "        print(f'==================================================')\n",
    "        print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        y_test_pred = model(X_train_batch)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)        \n",
    "        \n",
    "        for i in range(len(y_test_pred)):\n",
    "            y_true_list.append(y_train_batch[i].cpu().item())\n",
    "            y_pred_list.append(y_pred_tag[i].cpu().item())\n",
    "            \n",
    "        \n",
    "print(f'==================================================')        \n",
    "print(f'==================================================')        \n",
    "print(get_class_distribution(test_set, label_id))\n",
    "print(f'y_true_list{y_true_list}')\n",
    "print(f'y_pred_list{y_pred_list}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a2101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T19:03:21.054068Z",
     "start_time": "2022-07-21T19:03:20.942915Z"
    }
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_true_list, y_pred_list)\n",
    "print(report)\n",
    "# report_support = precision_recall_fscore_support(y_true_list, y_pred_list)\n",
    "# print(report_support)\n",
    "\n",
    "def metrics_report_to_df(ytrue, ypred):\n",
    "    classification_report_df = pd.DataFrame(data=list(precision_recall_fscore_support(y_true_list, y_pred_list)), \\\n",
    "                                         index=['Precision', 'Recall', 'F1-score', 'Support']).T    \n",
    "    classification_report_df.loc['weighted avg/Total', :] = precision_recall_fscore_support(ytrue, ypred, average='weighted')\n",
    "    classification_report_df.loc['Avg/Total', 'Support'] = classification_report_df['Support'].sum()\n",
    "    return(classification_report_df)\n",
    "\n",
    "classification_report_df = metrics_report_to_df(y_true_list, y_pred_list)\n",
    "print(classification_report_df)\n",
    "\n",
    "\n",
    "# print(confusion_matrix(y_true_list, y_pred_list))\n",
    "#################################\n",
    "### PLOTTING CONFUSION MATRIX\n",
    "cm=confusion_matrix(y_true_list, y_pred_list)\n",
    "print(cm)\n",
    "#cm=confusion_matrix(y_true_list, y_pred_list, normalize='all')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['BGR','4CV'])\n",
    "cmd.plot()\n",
    "cmd.ax_.set(xlabel='Predicted', ylabel='True')\n",
    "\n",
    "\n",
    "#### PREPARING AND SAVING TRAINING PERFORMANCE PARAMETERS\n",
    "train_values = {} # instantiate an empty train_values dict \n",
    "train_values['elapsed_time_in_secs'] = elapsed_time\n",
    "train_values['MAX_EPOCHS'] = MAX_EPOCHS\n",
    "train_values['LEARNING_RATE'] = LEARNING_RATE\n",
    "train_values['BATCH_SIZE_OF_CLIPS'] = BATCH_SIZE_OF_CLIPS\n",
    "train_values['PRETRANSFORM_IM_SIZE'] = PRETRANSFORM_IM_SIZE\n",
    "train_values['FRAMES_PER_CLIP'] = NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP\n",
    "train_values['Train Dataset Size'] = len(train_set)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP*BATCH_SIZE_OF_CLIPS\n",
    "train_values['Test Dataset Size'] = len(test_set)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP*BATCH_SIZE_OF_CLIPS\n",
    "train_values['Validation Dataset Size'] = len(validation_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP*BATCH_SIZE_OF_CLIPS\n",
    "train_values['Current date and time'] = datetime.now().strftime('%dD-%mM-%yY_%HH-%MM-%SS')\n",
    "#team['konqi'] = {'health': 18, 'level': 7}\n",
    "\n",
    "## Saving training curves at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "os.chdir(TRAINING_CURVES_PATH)\n",
    "TRAIN_FILE_NAME = TEMP_DICT_TRAINING_CURVES_FOR+\"TRAINING_PARAMETERS\"+PARAMETERS_FILENAME+\".json\"\n",
    "\n",
    "\n",
    "with open(TRAIN_FILE_NAME, 'w') as file:\n",
    "     file.write(json.dumps(train_values, indent=4))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060e041",
   "metadata": {},
   "source": [
    "## 12. [**!WARNING!**] Cleanup temporal data directory \n",
    "Remove directory if a temporary was used.\n",
    "\n",
    "```\n",
    "       Make sure you know which path you will remove as you do not like to remove important files.\n",
    "       shutil.rmtree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ae88d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:48:07.842055Z",
     "start_time": "2022-07-21T18:48:07.842045Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_files_path = config['temporal_data_path']\n",
    "\n",
    "shutil.rmtree(temporal_files_path)\n",
    "print(f' {temporal_files_path} is empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27027953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
