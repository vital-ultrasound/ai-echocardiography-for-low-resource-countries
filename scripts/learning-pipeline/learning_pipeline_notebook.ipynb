{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef885fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:05.695513Z",
     "start_time": "2021-12-17T09:18:03.125501Z"
    }
   },
   "source": [
    "# Machine Learning pipeline\n",
    "\n",
    "**Author**: Miguel Xochicale [@mxochicale](https://github.com/mxochicale)     \n",
    "**Contributors**: Nhat Phung Tran Huy [@huynhatd13](https://github.com/huynhatd13); Hamideh Kerdegari [@hamidehkerdegari](https://github.com/hamidehkerdegari);  Alberto Gomez [@gomezalberto](https://github.com/)  \n",
    "\n",
    "\n",
    "## History\n",
    "* Feb2022: Adding initial models with small dataset   \n",
    "* March2022: Improved datasets representation\n",
    "* April2022: Adds dataloader for clips and videos\n",
    "* May2022: Tidies VGG2D and VGG3D   \n",
    "* June2022: Tidies basicVGG model and adds heuristics for hyperarameters \n",
    "* Week1, July2022: Integreate modules in source path\n",
    "* Week2, July2022: Implements Tromp2022Net DOI: https://doi.org/10.1016/S2589-7500(21)00235-17\n",
    "* Week3, July2022: Adds LeNet, AlexNet and VGGNets, MobileNetV1, MobileNetV2\n",
    "* Week4, July2022: Adds ShuffleNetV1, ShuffleNetV2, SqueezeNet_sources 0,1,2 and merge all to main!\n",
    "\n",
    "\n",
    "## Summary\n",
    "This notebook presents a learning pipeline to classify 4 chamber view from echocardiography datasets.\n",
    "\n",
    "### How to run the notebook\n",
    "\n",
    "1. Go to echocardiography repository path: `$HOME/repositories/echocardiography/`\n",
    "2. Open echocardiography repo in pycharm and in the terminal type:\n",
    "    ```\n",
    "    git checkout master # or the branch\n",
    "    git pull # to bring a local branch up-to-date with its remote version\n",
    "    ```\n",
    "3. Launch Notebook server  \n",
    "    Go to you repository path: `cd $HOME/repositories/echocardiography/scripts/dataloaders` and type in the pycharm terminal:\n",
    "    ```\n",
    "    conda activate rt-ai-echo-VE \n",
    "    jupyter notebook\n",
    "    ```\n",
    "    which will open your web-browser.\n",
    "    \n",
    "    \n",
    "### References\n",
    "* \"Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback\". https://www.fda.gov/media/122535/download \n",
    "* https://nestedsoftware.com/2019/09/09/pytorch-image-recognition-with-convolutional-networks-4k17.159805.html \n",
    "* https://ai.stackexchange.com/questions/5769/in-a-cnn-does-each-new-filter-have-different-weights-for-each-input-channel-or\n",
    "* Gomez A. et al. 2021 https://github.com/vital-ultrasound/lung/blob/main/multiclass_pytorch/datasets/LUSVideoDataset.py \n",
    "* Kerdegari H. et al. 2021 https://github.com/vital-ultrasound/lung/tree/main/multiclass_tensorflow\n",
    "* https://learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/\n",
    "* https://github.com/shanglianlm0525/PyTorch-Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2aa0",
   "metadata": {},
   "source": [
    "# Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d3efa",
   "metadata": {},
   "source": [
    "## 1. Setting imports and datasets paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345efe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:33:27.822927Z",
     "start_time": "2022-08-12T17:33:26.994677Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io # io.StringIO(str(optimizer)\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "#import matplotlib.animation as animation\n",
    "from IPython.display import HTML #to be used with HTML(animation.ArtistAnimation().to_jshtml())\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, \\\n",
    "                            ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from source.dataloaders.EchocardiographicVideoDataset import EchoClassesDataset\n",
    "from source.helpers.various import concatenating_YAML_via_tags, \\\n",
    "                                    plot_dataset_classes, \\\n",
    "                                    split_train_validate_sets\n",
    "from source.helpers.learning_pipeline import get_class_distribution, \\\n",
    "                                            plot_from_dict, \\\n",
    "                                            creating_pair_of_clips, \\\n",
    "                                            pair_clips_labels, \\\n",
    "                                            animate_clips\n",
    "from source.models.learning_misc import train_loop, \\\n",
    "                                        test_loop\n",
    "\n",
    "from source.models.architectures import basicVGG, TrompNetV1, \\\n",
    "        LeNet5_source00, LeNet5_source01, LeNet5_source02, \\\n",
    "        AlexNet_source00, AlexNet_source01, AlexNet_source02, AlexNet_source03, \\\n",
    "        MobileNetV1, MobileNetV2, \\\n",
    "        SqueezeNet_source0, SqueezeNet_source1, SqueezeNet_source2\n",
    "        #ShuffleNetV1, ShuffleNetV2 (require 3 channels of input images)\n",
    "\n",
    "HOME_PATH = os.path.expanduser(f'~')\n",
    "USERNAME = os.path.split(HOME_PATH)[1]\n",
    "\n",
    "REPOSITORY_PATH='repositories/echocardiography'\n",
    "FULL_REPO_PATH = HOME_PATH+'/'+REPOSITORY_PATH\n",
    "FULL_REPO_MODEL_PATH = HOME_PATH +'/' + REPOSITORY_PATH + '/data/models'\n",
    "CONFIG_FILES_PATH= REPOSITORY_PATH + '/scripts/config_files/users_paths_files'\n",
    "YML_FILE =  'config_users_paths_files_username_' + USERNAME + '.yml'\n",
    "FULL_PATH_FOR_YML_FILE = os.path.join(HOME_PATH, CONFIG_FILES_PATH, YML_FILE)\n",
    "PATH_for_temporal_files = os.path.join(HOME_PATH, 'datasets/vital-us/echocardiography/temporal-files')\n",
    "\n",
    "## Setting TRAINING_CURVES_PATH\n",
    "#CURRENT_PATH=os.path.abspath(os.getcwd())\n",
    "RESULTS_PATH='scripts/learning-pipeline/results'\n",
    "TRAINING_CURVES_PATH = os.path.join(FULL_REPO_PATH, RESULTS_PATH)\n",
    "\n",
    "## Setting FULL_PATH_FOR_YML_FILE\n",
    "yaml.add_constructor('!join', concatenating_YAML_via_tags)  ## register the tag handler\n",
    "with open(FULL_PATH_FOR_YML_FILE, 'r') as yml:\n",
    "    config = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "\n",
    "## Printing Versions and paths\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')    \n",
    "print(f'FULL_PATH_FOR_YML_FILE: {FULL_PATH_FOR_YML_FILE}' )\n",
    "print(f'FULL_REPO_MODEL_PATH: {FULL_REPO_MODEL_PATH}' )\n",
    "print(f'TRAINING_CURVES_PATH: {TRAINING_CURVES_PATH}' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08abb29",
   "metadata": {},
   "source": [
    "## 2. Generate list text files for train and validate datasets\n",
    "\n",
    "Edit config_users_paths_files_username_$USER.yml at '../config_files/users_paths_files/config_users_paths_files_username_template.yml' with the right paths and percentage of `ntraining`:  \n",
    "```\n",
    "#ECHODATASET_PATH = config['echodataset_path'] # Default\n",
    "ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-GOOD'\n",
    "TRAINING_SPLITTING = 0.8 #config['ntraining'] #Default\n",
    "randomise_file_list: False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:08.264310Z",
     "start_time": "2021-12-17T09:18:08.250178Z"
    }
   },
   "source": [
    "## 2. Setting variables and loading datasets using pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a4a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:33:27.880418Z",
     "start_time": "2022-08-12T17:33:27.824408Z"
    }
   },
   "outputs": [],
   "source": [
    "START_TIME_OF_THE_NOTEBOOK = time.time()\n",
    "print(f'Starting time of the notebook {START_TIME_OF_THE_NOTEBOOK}')\n",
    "\n",
    "##############################\n",
    "##### Setting up device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #or \"cuda:NN\" can also be used e.g., \"cuda:0\"\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "# TRAIN_VERSION='train00'\n",
    "# TRAIN_VERSION='train01'\n",
    "TRAIN_VERSION='train02'\n",
    "\n",
    "TRAINING_SPLITTING = 0.70 #config['ntraining'] #Default\n",
    "TEST_FRACTION = 0.15\n",
    "VAL_FRACTION = 1-TRAINING_SPLITTING-TEST_FRACTION\n",
    "print(f'train_set_size  {TRAINING_SPLITTING}, test_set_size {TEST_FRACTION}, val_test_size {VAL_FRACTION}')\n",
    "FLAG_RANDOMISE_DATA=True #config['randomise_file_list'] #Default\n",
    "\n",
    "### Trump et al. 2022 \n",
    "##\"We trained the models on 55487 images from 1145 individual echocardiograms (appendix pp 2–3).\" \n",
    "##Appendix pp 2–3.\n",
    "##AC4: TRAINING: total videos 740 total frames 9615; T\n",
    "##      TESTING: total videos:64 total frames 1218                \n",
    "\n",
    "\n",
    "##############################\n",
    "## Setting ECHODATASET_PATH; \n",
    "#ECHODATASET_PATH = config['echodataset_path'] # Default\n",
    "\n",
    "ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-05-subjects'\n",
    "    #\"BATCH_SIZE_OF_CLIPS\": 20,\n",
    "    #\"FRAMES_PER_CLIP\": 1,\n",
    "    #\"Train Dataset Size\": 720,\n",
    "    #\"Test Dataset Size\": 320,\n",
    "    #\"Validation Dataset Size\": 280,\n",
    "\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects'\n",
    "    #\"BATCH_SIZE_OF_CLIPS\": 20,\n",
    "    #\"FRAMES_PER_CLIP\": 1,\n",
    "    #\"Train Dataset Size\": 1280,\n",
    "    #\"Test Dataset Size\": 560,\n",
    "    #\"Validation Dataset Size\": 440,\n",
    "\n",
    "# ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-20-subjects'\n",
    "    #\"BATCH_SIZE_OF_CLIPS\": 20,\n",
    "    #\"FRAMES_PER_CLIP\": 1,\n",
    "    #\"Train Dataset Size\": 2240,\n",
    "    #\"Test Dataset Size\": 960,\n",
    "    #\"Validation Dataset Size\": 720,\n",
    "\n",
    "# ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-31-subjects'\n",
    "    #\"BATCH_SIZE_OF_CLIPS\": 20,\n",
    "    #\"FRAMES_PER_CLIP\": 1,\n",
    "    #\"Train Dataset Size\": 3620,\n",
    "    #\"Test Dataset Size\": 1540,\n",
    "    #\"Validation Dataset Size\": 880,\n",
    "\n",
    "# ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-33-subjects'\n",
    "    #\"BATCH_SIZE_OF_CLIPS\": ?,\n",
    "    #\"FRAMES_PER_CLIP\": ?,\n",
    "    #\"Train Dataset Size\": ?,\n",
    "    #\"Test Dataset Size\": ?,\n",
    "    #\"Validation Dataset Size\": ?,\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "split_train_validate_sets(  \n",
    "                        ECHODATASET_PATH, #config['echodataset_path']\n",
    "                        config['data_list_output_path'], \n",
    "                        TRAINING_SPLITTING,\n",
    "                        TEST_FRACTION,\n",
    "                        FLAG_RANDOMISE_DATA\n",
    "                        )\n",
    "\n",
    "# PRETRANSFORM_IM_SIZE = [64, 64] #[650, 690] original pixel size for VenueGO\n",
    "PRETRANSFORM_IM_SIZE = [128, 128] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = [256, 256] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = [512, 512] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = config['pretransform_im_size'] ##DEFAULT\n",
    "\n",
    "### >> CHANGE DENSE LAYER FEATURES IN VGG3D\n",
    "### >> `self.fc0 = nn.Linear(in_features=4194304, out_features=500) #128x128`\n",
    "\n",
    "##############################\n",
    "##### Experiments for Basic HYPERPARAMETER Heuristics \n",
    "\n",
    "#### TESTS\n",
    "# NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 1\n",
    "NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 2 \n",
    "# NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 5; \n",
    "# NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; \n",
    "# NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 20; \n",
    "\n",
    "\n",
    "# RuntimeError: Given groups=1, weight of size [96, 1, 3, 3], expected input[30, 2, 128, 128] to have 1 channels, but got 2 channels instead\n",
    "\n",
    "\n",
    "# BATCH_SIZE_OF_CLIPS = 5; \n",
    "# BATCH_SIZE_OF_CLIPS = 10; \n",
    "BATCH_SIZE_OF_CLIPS = 30; \n",
    "\n",
    "# BATCH_SIZE_OF_CLIPS = 20; \n",
    "\n",
    "#LEARNING_RATE= 0.00005; \n",
    "\n",
    "\n",
    "#################  LEARNING_RATE \n",
    "### EXPERIMENT 01,02,03,04\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 1; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.00005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.0000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.00000005; \n",
    "\n",
    "#################  BATCH_SIZE_OF_CLIPS with LEARNING_RATE= 0.000005 as it is the best peformance of prevous LRs \n",
    "### EXPERIMENT 04,06,07,08\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 2; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 5; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 15; LEARNING_RATE= 0.000005; \n",
    "\n",
    "#################  NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP with LEARNING_RATE= 0.000005 and BATCH_SIZE_OF_CLIPS=10\n",
    "### EXPERIMENT 09,10,11,12\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 2; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 7; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 13; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 20; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "\n",
    "##TOADD\n",
    "### * NUMBER_OF_FRAMES_AND PARTICPANTS\n",
    "### * OPTIMISERS \n",
    "\n",
    "# LEARNING_RATE =Trial and Error with diffent values  0.00005; 0.0005; 0.005 and 0.000001; 0.00001; 0.0001; 0.001  \n",
    "\n",
    "\n",
    "MAX_EPOCHS = 500 #Alternatvely, make use of: config['max_epochs']\n",
    "\n",
    "\n",
    "##############################\n",
    "##### Setting up animation\n",
    "interval_between_frames_in_milliseconds=33.3 ## 1/30=0.033333\n",
    "frame_per_seconds_for_animated_frames=30\n",
    "\n",
    "\n",
    "#SUBJECT_ID = '073'\n",
    "#print(SUBJECT_ID)\n",
    "\n",
    "### CUDA out of memory \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10\n",
    "#PRETRANSFORM_IM_SIZE = [128, 128] \n",
    "#RuntimeError: CUDA out of memory. Tried to allocate 7.81 GiB (GPU 0; 15.74 GiB total capacity; 8.51 GiB already allocated; 5.05 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
    "## REBOOT MACHINE\n",
    "#RuntimeError: CUDA out of memory. Tried to allocate 7.81 GiB (GPU 0; 15.74 GiB total capacity; 8.51 GiB already allocated; 5.13 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "## Setting labels\n",
    "label_id = ('BKGR', '4CV')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:33:28.000707Z",
     "start_time": "2022-08-12T17:33:27.882821Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining transforms that apply to the entire dataset.\n",
    "# These transforms are not augmentation.\n",
    "if config['use_pretransform_image_size']:\n",
    "    pretransform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=PRETRANSFORM_IM_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    pretransform = None\n",
    "    \n",
    "    #config['use_train_augmentation']#Default\n",
    "    \n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_train_augmentation']:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),  # in degrees\n",
    "        transforms.RandomEqualize(p=0.5),\n",
    "        transforms.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "    \n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_validation_augmentation']:\n",
    "    val_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=5),  # in degrees\n",
    "    #transforms.RandomEqualize(p=0.5),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "\n",
    "\n",
    "train_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_train'],\n",
    "    participant_path_json_list=config['participant_path_json_list_train'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=train_transform, #None,#transform=train_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "test_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_test'],\n",
    "    participant_path_json_list=config['participant_path_json_list_test'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,#transform=test_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "val_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_validation'],\n",
    "    participant_path_json_list=config['participant_path_json_list_validation'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,#transform=val_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760906",
   "metadata": {},
   "source": [
    "## 3. Plotting Class Distribution (creates temp clips and it takes few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6472a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:34:24.764726Z",
     "start_time": "2022-08-12T17:33:28.001651Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set_class_dict = get_class_distribution(train_dataset,label_id)\n",
    "test_set_class_dict = get_class_distribution(test_dataset,label_id)\n",
    "val_set_class_dict = get_class_distribution(val_dataset,label_id)\n",
    "\n",
    "\n",
    "print(f'class_distribution(train_dataset): {train_set_class_dict}')\n",
    "print(f'class_distribution(test_dataset): {test_set_class_dict}')\n",
    "print(f'class_distribution(val_dataset): {val_set_class_dict}' )\n",
    "    \n",
    "#number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip']    \n",
    "print(f'Number of frames for training datasets {len(train_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "print(f'Number of frames for testing datasets {len(test_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "print(f'Number of frames for Validation datasets {len(val_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "\n",
    "plot_title_train_label= f'TRAIN dataset of {len(train_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_test_label= f'TEST dataset of {len(test_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_val_label= f'VALIDATION dataset of {len(val_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,7))\n",
    "plot_from_dict(train_set_class_dict, plot_title=plot_title_train_label, ax=axes[0])\n",
    "plot_from_dict(val_set_class_dict, plot_title=plot_title_test_label, ax=axes[1])\n",
    "plot_from_dict(test_set_class_dict, plot_title=plot_title_val_label, ax=axes[2])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ea2a6",
   "metadata": {},
   "source": [
    "## 4. Animating frames of one clip of the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b57ab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:34:24.903890Z",
     "start_time": "2022-08-12T17:34:24.765530Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'-----------------[val_dataset]-----------------')\n",
    "clips=creating_pair_of_clips(val_dataset, label_id)\n",
    "pair_clips_and_labels = pair_clips_labels(clips)\n",
    "\n",
    "print(f'-----------------[test_dataset]-----------------')\n",
    "clips=creating_pair_of_clips(test_dataset, label_id)\n",
    "pair_clips_and_labels = pair_clips_labels(clips)\n",
    "\n",
    "print(f'-----------------[train_dataset]-----------------')\n",
    "clips=creating_pair_of_clips(train_dataset, label_id)\n",
    "pair_clips_and_labels = pair_clips_labels(clips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49ba14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:34:25.314043Z",
     "start_time": "2022-08-12T17:34:24.904928Z"
    }
   },
   "outputs": [],
   "source": [
    "#average_HR =\n",
    "#fps = 30\n",
    "# 60 # beats per minute\n",
    "#Beats-per-minute: 60 BPM\n",
    "#Beats-per-second: 1 Hz\n",
    "#Cycle-per-second: 1 (Cycle/s)\n",
    "\n",
    "PAIR_OF_CLIPS = pair_clips_and_labels[0]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS, label_id, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP,\n",
    "                  interval_between_frames_in_milliseconds)\n",
    "HTML(animated_frames.to_jshtml())      \n",
    "\n",
    "\n",
    "# ##SAVE ANIMATIONS\n",
    "# for idx in range(0,len(pair_clips_labels)):\n",
    "#     PAIR_OF_CLIPS = pair_clips_labels[idx]\n",
    "#     print( f' pair_clips_labels {str(PAIR_OF_CLIPS[2])} {str(PAIR_OF_CLIPS[6])}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d225",
   "metadata": {},
   "source": [
    "## 5. Displayting frames in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8805e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:34:25.446322Z",
     "start_time": "2022-08-12T17:34:25.315030Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'====================================================')\n",
    "print(f'train_dataset.__len__() = {train_dataset.__len__()}')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f'len(train_dataloader): {len(train_dataloader)} BATCHES of BATCH_SIZE_OF_CLIPS {BATCH_SIZE_OF_CLIPS}')\n",
    "for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "    print(f'  ====================================================')\n",
    "    sample_batched_images=sample_batched[0]\n",
    "    sample_batched_labels=sample_batched[1]\n",
    "    print(f'    BATCH_OF_CLIPS_INDEX : {clip_batch_idx} / {len(train_dataloader) - 1}')\n",
    "    print(f'    sample_batched_labels.size(): {  sample_batched_labels.size()  }')\n",
    "    print(f'    sample_batched_labels.squeeze().size(): {  sample_batched_labels.squeeze().size()  }')\n",
    "    print(f'    sample_batched_images.size(): {sample_batched_images.size()}')\n",
    "\n",
    "    for BATCH_SIZE_IDX, label in enumerate(sample_batched_labels):\n",
    "        print(f'        BATCH_SIZE_IDX {BATCH_SIZE_IDX} ')\n",
    "        print(f'          label: {label}')\n",
    "        sample_batched_idx_image = sample_batched_images[BATCH_SIZE_IDX,...]\n",
    "        print(f'          Sample_batched_idx_image.size()  {sample_batched_idx_image.size() }'  )\n",
    "\n",
    "        grid = utils.make_grid(sample_batched_idx_image)\n",
    "        print(f'          Grid size {grid.size()}' )\n",
    "#         plt.figure(figsize =(20,20) )\n",
    "#         plt.imshow( grid.cpu().detach().numpy().transpose(1, 2, 0) )\n",
    "#         plt.title(f'BATCH_SIZE_IDX {BATCH_SIZE_IDX}; Label: {label_id[label]}')\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f' test_dataset.__len__() = {test_dataset.__len__()}')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f' validation_dataset.__len__() = {val_dataset.__len__()}')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e92336",
   "metadata": {},
   "source": [
    "## 7. Define networks\n",
    "**This cell is useful to prototype other newtorks**\n",
    "See `$HOMErepositories/echocardiography/source/models/arquitectures.py` to amend or implement other Neural Network Arquitectures!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8249058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:31:13.073706Z",
     "start_time": "2022-08-12T17:31:13.071726Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40bb1dd6",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Sanity checks for the model and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5dd8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:34:27.201221Z",
     "start_time": "2022-08-12T17:34:25.447286Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "##### Tensor Shape\n",
    "#tensor_shape_size = [BATCH_SIZE_OF_CLIPS, config['number_of_frames_per_segment_in_a_clip'], 1, 128, 128]\n",
    "#model = basicVGGNet(tensor_shape_size)\n",
    "\n",
    "#n_frames_per_clip \n",
    "#model = VGG3D(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "#model = basicVGG(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #torch.Size([BATCH_SIZE_OF_CLIPS, CLASS_NUMBER])\n",
    "#model = TrompNet2022(PRETRANSFORM_IM_SIZE, BATCH_SIZE_OF_CLIPS, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "\n",
    "\n",
    "# model=MobileNetV1(ch_in=1, n_classes=2)#Total params: 3,208,450\n",
    "\n",
    "\n",
    "# model = TrompNetV1(PRETRANSFORM_IM_SIZE, BATCH_SIZE_OF_CLIPS, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "#Total params: 402,284\n",
    "\n",
    "# model=LeNet5_source00()#Total params: 6,350,946\n",
    "# model=LeNet5_source01()#Total params: 1,627,790\n",
    "# model=LeNet5_source02()#Total params: 1,741,026\n",
    "\n",
    "# model = AlexNet_source00()#Total params: 56,996,546\n",
    "# model = AlexNet_source01()#Total params: 24,714,626 \n",
    "# model = AlexNet_source02()#Total params: 28,841,314\n",
    "# model = AlexNet_source03(num_classes=2, in_channels=1)#Total params: 56,996,546 \n",
    "\n",
    "\n",
    "# model=MobileNetV2(ch_in=1, n_classes=2)#Total params: 2,225,858\n",
    "\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP\n",
    "\n",
    "model=SqueezeNet_source0(num_classes=2, in_channels=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #Total params: 733,580\n",
    "# model=SqueezeNet_source1(num_classes=2, in_channels=1) #Total params: 729,154\n",
    "# model=SqueezeNet_source2(channels=1, class_count=2) #Total params: 722,370\n",
    "    \n",
    "    \n",
    "##TRIALS        \n",
    "# model=ShuffleNetV1_G3()##RuntimeError: expected to have 3 channels\n",
    "# model = shufflenetv2()##RuntimeError: expected to have 3 channels\n",
    "# https://github.com/ntoussaint/fetalnav/blob/master/fetalnav/models/vgg.py\n",
    "# model = VGG(cfg=VGGlayers, batch_norm=batch_norm, in_channels=1, num_classes=2)      \n",
    "# model = ResNet(RNblock, RNlayers, num_classes=2, in_channels=1) #Total params: 21,279,426\n",
    "\n",
    "\n",
    "model.to(DEVICE) # Place model on GPU\n",
    "\n",
    "print(model) \n",
    "print(summary(model))\n",
    "#print(str(summary(model, (1, 32, 32), depth=1)))\n",
    "\n",
    "### Sanity check\n",
    "#print(len(train_dataloader)) #6 BATCHES of 10=BATCH_SIZE_OF_CLIPS\n",
    "sample_batched = next(iter(train_dataloader))\n",
    "#print(sample_batched[0].shape) #torch.Size([10, 60, 1, 128, 128])\n",
    "#print(sample_batched[1])#tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1])\n",
    "#print(sample_batched[2]) #tensor([47, 42,  0, 51, 49, 75, 67, 67, 62, 84])\n",
    "#print(sample_batched[3]) #tensor([105, 102,  43, 106,  94, 161, 151, 183, 150, 151])\n",
    "\n",
    "clip_batch = sample_batched[0]\n",
    "#print(f'clip_batch.size() {clip_batch.size()}') ##torch.Size([4, 60, 1, 128, 128])\n",
    "#                                                ##[batch_size, channels, depth, height, width]\n",
    "\n",
    "# # frames = image.to(device)\n",
    "print(f'Shape of vector: {model(clip_batch).shape}' ) #torch.Size([4, 2])\n",
    "print(f'Type of variable  {type(model(clip_batch))}')  #torch.Size([4, 2]) #<class 'torch.Tensor'>\n",
    "\n",
    "# #https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
    "del sample_batched\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22976ae2",
   "metadata": {},
   "source": [
    "## 8. Define Optimizer and setting up of other hyperameters\n",
    "1. Set learning rate for how much the model is updated per batch.\n",
    "2. Set total epoch number, as we have shuffle and random transforms, so the training data of every epoch is different.\n",
    "3. Set the number of clips per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295fe94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:34:27.226121Z",
     "start_time": "2022-08-12T17:34:27.203118Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model = VGG3D(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "#model = basicVGG(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #torch.Size([BATCH_SIZE_OF_CLIPS, CLASS_NUMBER])\n",
    "\n",
    "\n",
    "\n",
    "# ###########################\n",
    "# #### MobileNetV1\n",
    "# #https://docs.netspresso.ai/docs/classification-mobilenet-v1-on-cifar100\n",
    "# model=MobileNetV1(ch_in=1, n_classes=2)#Total params: 3,208,450 ## Training curves look good\n",
    "# gamma=0.1\n",
    "# rho=0.9\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum = rho) \n",
    "# LEARNING_RATE = gamma\n",
    "\n",
    "\n",
    "# ############################\n",
    "# #### TrompNetV1 #Total params: 402,284\n",
    "# model = TrompNetV1(PRETRANSFORM_IM_SIZE, BATCH_SIZE_OF_CLIPS, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "\n",
    "# ### Training curves not converging ACC~0.50; Loss~0.75\n",
    "# # gamma=0.00005\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=gamma)\n",
    "# # model.to(DEVICE) # Place model on GPU\n",
    "# # LEARNING_RATE = gamma\n",
    "\n",
    "# # ## Training curves not converging ACC~0.50; Loss~0.75\n",
    "# # gamma=0.00001\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=gamma)\n",
    "# # model.to(DEVICE) # Place model on GPU\n",
    "# # LEARNING_RATE = gamma\n",
    "\n",
    "# # ## Training curves not converging ACC~0.50; Loss~0.75\n",
    "# # gamma=0.0001\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=gamma)\n",
    "# # model.to(DEVICE) # Place model on GPU\n",
    "# # LEARNING_RATE = gamma\n",
    "\n",
    "# # ## Training curves not converging ACC~0.50; Loss~0.75\n",
    "# # gamma=0.001\n",
    "# # criterion = nn.CrossEntropyLoss()\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=gamma)\n",
    "# # model.to(DEVICE) # Place model on GPU\n",
    "# # LEARNING_RATE = gamma\n",
    "\n",
    "# ### Training curves not converging ACC~0.50; Loss~0.75\n",
    "# gamma=0.001\n",
    "# rho=.9 \n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum=rho) \n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# LEARNING_RATE = gamma\n",
    "\n",
    "\n",
    "# ###########################\n",
    "# #### MobileNetV2  ----  Training curves look good\n",
    "# model=MobileNetV2(ch_in=1, n_classes=2)#Total params: 2,225,858 \n",
    "# gamma=0.01\n",
    "# #pshi=0.005\n",
    "# rho=0.9\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum = rho) \n",
    "# LEARNING_RATE = gamma\n",
    "# # A Flower Classification Approach with MobileNetV2 and Transfer Learning (2020)\n",
    "# #   https://isciia2020.bit.edu.cn/docs/20201114083020836285.pdf                  \n",
    "\n",
    "\n",
    "# ###########################\n",
    "# #### MobileNetV2\n",
    "# model=MobileNetV2(ch_in=1, n_classes=2)#Total params: 2,225,858 ## Training curves goes to ~ACC0.5; LOSS~>2 \n",
    "# gamma=0.045 #Learning rate\n",
    "# rho=0.9 # momentum\n",
    "# lambda_ = 0.00004 # weight decay\n",
    "# #learning rate decay rate of 0.98 per epoch?\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=gamma, momentum = rho, weight_decay=lambda_) \n",
    "# LEARNING_RATE = gamma\n",
    "# # https://arxiv.org/pdf/1801.04381.pdf\n",
    "# # https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html \n",
    "\n",
    "## OTHER REFERENCES ON MobileNetV2\n",
    "# #https://github.com/d-li14/mobilenetv2.pytorch/issues/2#issuecomment-454282418\n",
    "#torch.optim.SGD                    \n",
    "# rho=0.001 #Learning rate #https://www.kaggle.com/code/gpiosenka/mobilenet-v2-transfer-learning-99-accuracy/notebook\n",
    "# optimizer = torch.optim.Adam(lr=rho)\n",
    "\n",
    "###########################\n",
    "#### SqueezeNet_source0\n",
    "model=SqueezeNet_source0(num_classes=2, in_channels=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #Total params: 733,580 ## Training curves look good \n",
    "# gamma=0.01\n",
    "gamma=0.001 #default \n",
    "# gamma=0.0001\n",
    "# gamma=0.00001\n",
    "rho=0.9\n",
    "pshi=5e-4\n",
    "model.to(DEVICE) # Place model on GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum=rho, weight_decay=pshi)\n",
    "# https://github.com/gsp-27/pytorch_Squeezenet/blob/master/main.py\n",
    "LEARNING_RATE = gamma\n",
    "\n",
    "# ###########################\n",
    "# #### SqueezeNet_source1\n",
    "# model=SqueezeNet_source1(num_classes=2, in_channels=1) #Total params: 729,154 \n",
    "#     ## Training curves look good; AAC~0.75[1000epochs;05subjects]\n",
    "#     ## Runned it like 5 times and keeps gving a flat curves\n",
    "# gamma=0.01\n",
    "# rho=0.9\n",
    "# pshi=5e-4\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# # criterion = nn.BCEWithLogitsLoss()#if self.classes ==2: ValueError: Target size (torch.Size([13])) must be the same as input size (torch.Size([13, 2]))\n",
    "# criterion = nn.CrossEntropyLoss()#else \n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum=rho, weight_decay=pshi)\n",
    "# # https://github.com/gsp-27/pytorch_Squeezenet/blob/master/main.py\n",
    "# LEARNING_RATE = gamma\n",
    "\n",
    "\n",
    "# ###########################\n",
    "# #### SqueezeNet_source2\n",
    "# model=SqueezeNet_source2(channels=1, class_count=2) #Total params: 722,370 \n",
    "#         ## Runned like 5ish times and Train/loss curves look flat with gamma=0.01;rho=0.9;pshi=5e-4  \n",
    "#         ## https://github.com/arvention/SqueezeNet-PyTorch/blob/master/main.py\n",
    "        \n",
    "#         ##  Train/loss curves look okay with gamma=0.01;rho=0.5;pshi=5e-4\n",
    "#         ##https://github.com/akashsunilgaikwad/Pytorch-Squeeznet/blob/master/main.py\n",
    "# gamma=0.01  #0.001flat curves  #0.04 flat curves arvention/SqueezeNet-PyTorch/blob/master/main.py flat curves\n",
    "# rho=0.5  #0.9\n",
    "# pshi=5e-4   # 0.0002arvention/SqueezeNet-PyTorch/blob/master/main.py flat curves\n",
    "\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum=rho, weight_decay=pshi)\n",
    "# LEARNING_RATE = gamma\n",
    "\n",
    "\n",
    "# ###########################\n",
    "# #### SqueezeNet_source2\n",
    "# model=SqueezeNet_source2(channels=1, class_count=2) #Total params: 722,370  \n",
    "#     ## Train/loss curves look flat\n",
    "# gamma=0.001\n",
    "# model.to(DEVICE) # Place model on GPU\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=gamma)\n",
    "# ###https://github.com/michaelyhuang23/SqueezeNet/blob/master/squeeze_net_trainer.py\n",
    "# LEARNING_RATE = gamma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ############################\n",
    "# #### LeNet5_sourceNN\n",
    "# # model=LeNet5_source00() #Total params: 6,350,946 ## Training curves look good: Reaching ACC~0.81; Loss~0.5\n",
    "# model=LeNet5_source01() #Total params: 1,627,790 ## Training curves look good: Reaching ACC~0.93; Loss~0.5\n",
    "# # model=LeNet5_source02() #Total params: 1,741,026  ## Training curves look good: Reaching ACC~0.80; Loss~0.5\n",
    "# gamma=.001 #or0.002\n",
    "# rho=.9 #LetNet_5()\n",
    "# model.to(DEVICE) \n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, momentum=rho) \n",
    "# #optimizer = torch.optim.RMSprop(model.parameters(), lr=gamma, momentum=rho) \n",
    "# LEARNING_RATE = gamma\n",
    "\n",
    "\n",
    "# ###########################\n",
    "# #### AlexNet\n",
    "# # model = AlexNet_source00()#Total params: 56,996,546\n",
    "# # model = AlexNet_source01()#Total params: 24,714,626 ## Training curves look good\n",
    "# # model = AlexNet_source02()#Total params: 28,841,314 ## Training curves not converging\n",
    "# # model = AlexNet_source03(num_classes=2, in_channels=1)#Total params: 56,996,546 [Training keeps 50%ACC in 1000epochs]\n",
    "# #gamma=0.005#AlexNet\n",
    "# gamma=0.001#AlexNet\n",
    "# pshi=0.005#AlexNet\n",
    "# rho=0.9#AlexNet\n",
    "# model.to(DEVICE) \n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=gamma, weight_decay = pshi, momentum = rho)  #AlexNet\n",
    "# LEARNING_RATE = gamma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## PRINT MODEL\n",
    "print(f'====================================================')\n",
    "print(model)\n",
    "\n",
    "# ### PRINT model.named_parameters\n",
    "# print(f'====================================================')\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98807711",
   "metadata": {},
   "source": [
    "## 9. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee627c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:36:19.275886Z",
     "start_time": "2022-08-12T17:34:27.227098Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TRAINING\n",
    "#clip_batch_size = tuple(train_dataloader.dataset.__getitem__(0)[0].shape) ##(60, 1, 128, 128) frames, chs, [width, height]\n",
    "#print(clip_batch_size)\n",
    "\n",
    "startt = time.time()\n",
    "print(f'Starting training loop {startt}')\n",
    "\n",
    "############################\n",
    "####### BINARY ACCURACY MODULE\n",
    "def binary_accuracy(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    binary_accuracy to calculate accuracy per epoch.\n",
    "    \"\"\"\n",
    "    y_pred_tag = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_tag, dim = 1)\n",
    "    correct_results_sum = (y_pred_tags == y_test).sum().float()\n",
    "    accuracy = correct_results_sum/y_test.shape[0]\n",
    "    accuracy = torch.round(accuracy * 100)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "############################\n",
    "####### TRAIN LOOP MODULE\n",
    "def train_loop(train_dataloader, model, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    train_loop\n",
    "    Arguments:\n",
    "        dataloader, model, criterion, optimizer, device\n",
    "\n",
    "    Return:\n",
    "        train_epoch_loss\n",
    "    \"\"\"\n",
    "    train_epoch_loss = 0\n",
    "    train_acc_loss_epoch = 0\n",
    "    step_train = 0\n",
    "    #size = len(train_dataloader.dataset)\n",
    "    for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "        step_train += 1\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(device), sample_batched[1].to(device)\n",
    "\n",
    "        #print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        # print(f'----------------------------------------------------------')\n",
    "        # print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        # print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        y_train_pred = model(X_train_batch) #torch.Size([9, 2])\n",
    "        #y_train_pred = model(X_train_batch).squeeze()  # torch.Size([9, 2])\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = binary_accuracy(y_train_pred, y_train_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if clip_batch_idx % 10 == 0: ## Print loss values every 10 clip batches\n",
    "        #     train_loss, current = train_loss.item(), clip_batch_idx * len(X_train_batch)\n",
    "        #     print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        train_epoch_loss += train_loss.detach().item()\n",
    "        train_acc_loss_epoch += train_acc.detach().item()\n",
    "\n",
    "    train_epoch_loss /= step_train\n",
    "    train_acc_loss_epoch /= step_train\n",
    "\n",
    "    return train_epoch_loss, train_acc_loss_epoch\n",
    "\n",
    "\n",
    "############################\n",
    "####### TEST LOOP MODULE\n",
    "def test_loop(dataloader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Test loop \n",
    "    \n",
    "    Arguments:\n",
    "        dataloader, model, criterion, optimizer, device\n",
    "\n",
    "    Return:\n",
    "        test_epoch_loss, correct\n",
    "    \"\"\"\n",
    "\n",
    "    train_epoch_acc = 0\n",
    "    step_test = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_epoch_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #model.eval()\n",
    "        #val_epoch_loss = 0\n",
    "        #val_epoch_acc = 0\n",
    "        for clip_batch_idx, sample_val_batched in enumerate(dataloader):\n",
    "            step_test += 1\n",
    "            X_val_batch, y_val_batch = sample_val_batched[0].to(device), sample_val_batched[1].to(device)\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            test_epoch_loss += criterion(y_val_pred, y_val_batch).detach().item()\n",
    "            correct += (y_val_pred.argmax(1) == y_val_batch).type(torch.float).sum().detach().item()\n",
    "\n",
    "    test_epoch_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    return test_epoch_loss, correct\n",
    "\n",
    "\n",
    "#Dictionaries to store the accuracy/epoch and loss/epoch for both train and validation sets.\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    #\"val\": []\n",
    "}\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    #\"val\": []\n",
    "}\n",
    "\n",
    "\n",
    "###################################################\n",
    "#for epoch in tqdm(range(1, MAX_EPOCHS)):   \n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"EPOCH {epoch + 1}/{MAX_EPOCHS}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    \n",
    "    train_epoch_loss, train_acc_loss_epoch = train_loop(train_dataloader, model, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    test_epoch_loss, correct = test_loop(test_dataloader, model, criterion, DEVICE)\n",
    "\n",
    "    #print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} | Average Train Acc: {train_epoch_acc:.5f} | Average Validation Loss: {val_epoch_loss:.3f} | Average Validation Acc: {val_epoch_acc:.5f} ')\n",
    "    #print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} |Average Train Acc: {train_epoch_acc:.5f}  ')\n",
    "    \n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_epoch_loss:>8f} \\n\")\n",
    "\n",
    "    print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} Average Train Accuracy Loss: {(train_acc_loss_epoch):>0.1f}% ')\n",
    "    \n",
    "    print(f\"Test Error: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg Test loss: {test_epoch_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "    loss_stats['train'].append(train_epoch_loss)\n",
    "    loss_stats['test'].append(test_epoch_loss)\n",
    "    accuracy_stats['train'].append(train_acc_loss_epoch)\n",
    "    accuracy_stats['test'].append(100*correct)\n",
    "            \n",
    "print(\"DONE TRAINING LOOP!\")\n",
    "\n",
    "\n",
    "\n",
    "endt = time.time()\n",
    "elapsed_time = endt - startt\n",
    "print(f'Finishing training loop {endt}')\n",
    "print(f'Elapsed time for the training loop: {elapsed_time} (s)')\n",
    "\n",
    "#model_path = ' /home/mx19/repositories/echocardiography/models' \n",
    "print(f' {FULL_REPO_MODEL_PATH}')\n",
    "torch.save(model.state_dict(), os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\"))\n",
    "print(\"Saved metric model\")\n",
    "\n",
    "print(loss_stats)\n",
    "print(accuracy_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a7882",
   "metadata": {},
   "source": [
    "## 10. Visualize accuracy and loss performance and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd312af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:36:19.672945Z",
     "start_time": "2022-08-12T17:36:19.276949Z"
    }
   },
   "outputs": [],
   "source": [
    "## Convert stats as dataframes\n",
    "loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "## Concatenate dataframes and dictionaries\n",
    "loss_df.insert(0, 'curve', 'Loss', True)\n",
    "acc_df.insert(0, 'curve', 'Acc', True)\n",
    "valuesall = [loss_df, acc_df]\n",
    "values_all = pd.concat(valuesall)\n",
    "\n",
    "all_stats = {\n",
    "    'ACC': accuracy_stats,\n",
    "    'LOS': loss_stats,\n",
    "}\n",
    "\n",
    "\n",
    "## Saving training curves at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "os.chdir(TRAINING_CURVES_PATH)\n",
    "\n",
    "\n",
    "#################################\n",
    "#### PLOTING\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(45,10))\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.lineplot(\n",
    "    data=acc_df, \n",
    "    x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0],    \n",
    "    estimator=None, linewidth=5, palette=\"Set2\" \n",
    "    ).set_title(f'Accuracy/Epoch for Train Val Sets EPOCHS={MAX_EPOCHS} \\\n",
    "                BATCH_SIZE_OF_CLIPS={BATCH_SIZE_OF_CLIPS} \\\n",
    "                NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP={NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} \\\n",
    "                LEARNING_RATE={LEARNING_RATE}')\n",
    "\n",
    "axes[0].set_ylim(0,105)\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.lineplot(\n",
    "    data=loss_df, \n",
    "    x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1],\n",
    "    estimator=None, linewidth=5, palette=\"Set2\" \n",
    "    ).set_title(f'Loss for  EPOCHS={MAX_EPOCHS} \\\n",
    "                BATCH_SIZE_OF_CLIPS={BATCH_SIZE_OF_CLIPS} \\\n",
    "                NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP={NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} \\\n",
    "                LEARNING_RATE={LEARNING_RATE}')\n",
    "axes[1].set_ylim(-0.4, 2)\n",
    "\n",
    "\n",
    "\n",
    "#### PREPARING AND SAVING PERFORMANCE CURVES\n",
    "WIDTH = 3\n",
    "PRECISION = 10\n",
    "TYPE = \"f\"\n",
    "STR_LR = f'{LEARNING_RATE:{WIDTH}.{PRECISION}{TYPE}}'\n",
    "STR_LR = STR_LR.replace(\".\", \"_\", 1)\n",
    "\n",
    "PARAMETERS_FILENAME = \"TRAINset_clips_\"+ \\\n",
    "                    str(len(train_dataset))+\"TESTset_clips_\"+ \\\n",
    "                    str(len(test_dataset))+\"VALset_clips\"+ \\\n",
    "                    str(len(val_dataset))+\"___EPOCHS_\"+str(f'{MAX_EPOCHS:{WIDTH}}') + \\\n",
    "                    \"_BATCH_SIZE_OF_CLIPS_\"+str(f'{BATCH_SIZE_OF_CLIPS:02}')+ \\\n",
    "                    \"_NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP_\"+ \\\n",
    "                    str(NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP)+\"_LEARNING_RATE_\"+ STR_LR + \\\n",
    "                    \"_TRAINVERSION_\"+TRAIN_VERSION\n",
    "\n",
    "## Saving training curves at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "os.chdir(TRAINING_CURVES_PATH)\n",
    "TEMP_DICT_TRAINING_CURVES_FOR = \"TEMP_DICT_TRAINING_CURVES_FOR____\"\n",
    "IMAGE_FILE_NAME = TEMP_DICT_TRAINING_CURVES_FOR+PARAMETERS_FILENAME\n",
    "#print(IMAGE_FILE_NAME)\n",
    "fig.savefig(IMAGE_FILE_NAME) \n",
    "\n",
    "## Saving training metrics in dictionaries at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "LOSS_ACC_DICTS_FILE_NAME = \"TEMP_DICT_TRAINING_CURVES_FOR____\"+\"_LOSS_ACC_\"+PARAMETERS_FILENAME+'.json'\n",
    "#print(LOSS_ACC_DICTS_FILE_NAME)\n",
    "#print(all_stats)\n",
    "\n",
    "with open(LOSS_ACC_DICTS_FILE_NAME, 'w') as file:\n",
    "   file.write(json.dumps(all_stats, indent=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aafb41",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791b35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:36:19.725741Z",
     "start_time": "2022-08-12T17:36:19.673929Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clip_batch_idx, sample_batched in enumerate(val_dataloader):\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(DEVICE), sample_batched[1].to(DEVICE)\n",
    "        print(f'==================================================')\n",
    "        print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        y_test_pred = model(X_train_batch)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)        \n",
    "        \n",
    "        for i in range(len(y_test_pred)):\n",
    "            y_true_list.append(y_train_batch[i].cpu().item())\n",
    "            y_pred_list.append(y_pred_tag[i].cpu().item())\n",
    "            \n",
    "        \n",
    "print(f'==================================================')        \n",
    "print(f'==================================================')        \n",
    "print(get_class_distribution(val_dataset, label_id))\n",
    "print(f'y_true_list{y_true_list}')\n",
    "print(f'y_pred_list{y_pred_list}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a2101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:36:19.836272Z",
     "start_time": "2022-08-12T17:36:19.726693Z"
    }
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_true_list, y_pred_list)\n",
    "print(report)\n",
    "# report_support = precision_recall_fscore_support(y_true_list, y_pred_list)\n",
    "# print(report_support)\n",
    "\n",
    "def metrics_report_to_df(ytrue, ypred):\n",
    "    classification_report_df = pd.DataFrame(data=list(precision_recall_fscore_support(y_true_list, y_pred_list)), \\\n",
    "                                         index=['Precision', 'Recall', 'F1-score', 'Support']).T    \n",
    "    classification_report_df.loc['weighted avg/Total', :] = precision_recall_fscore_support(ytrue, ypred, average='weighted')\n",
    "    classification_report_df.loc['Avg/Total', 'Support'] = classification_report_df['Support'].sum()\n",
    "    return(classification_report_df)\n",
    "\n",
    "classification_report_df = metrics_report_to_df(y_true_list, y_pred_list)\n",
    "print(classification_report_df)\n",
    "\n",
    "#################################\n",
    "### PLOTTING CONFUSION MATRIX\n",
    "cm=confusion_matrix(y_true_list, y_pred_list)\n",
    "print(cm)\n",
    "#cm=confusion_matrix(y_true_list, y_pred_list, normalize='all')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['BGR','4CV'])\n",
    "cmd.plot()\n",
    "cmd.ax_.set(xlabel='Predicted', ylabel='True')\n",
    "\n",
    "\n",
    "END_TIME_OF_THE_NOTEBOOK = time.time()\n",
    "NOTEBOOK_ELAPSE_TIME = END_TIME_OF_THE_NOTEBOOK - START_TIME_OF_THE_NOTEBOOK\n",
    "print(f'Elapsed time for the notebook loop: {NOTEBOOK_ELAPSE_TIME} (s)')\n",
    "\n",
    "\n",
    "\n",
    "#### PREPARING AND SAVING TRAINING PERFORMANCE PARAMETERS\n",
    "train_values = {} # instantiate an empty train_values dict \n",
    "train_values['elapsed_time_for_the_NOTEBOOK_in_secs'] = NOTEBOOK_ELAPSE_TIME\n",
    "train_values['elapsed_time_for_the_training_loop_in_secs'] = elapsed_time\n",
    "train_values['MAX_EPOCHS'] = MAX_EPOCHS\n",
    "train_values['LEARNING_RATE'] = LEARNING_RATE\n",
    "train_values['BATCH_SIZE_OF_CLIPS'] = BATCH_SIZE_OF_CLIPS\n",
    "train_values['PRETRANSFORM_IM_SIZE'] = PRETRANSFORM_IM_SIZE\n",
    "train_values['FRAMES_PER_CLIP'] = NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP\n",
    "train_values['Train Dataset Size in Number of Frames'] = len(train_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP*BATCH_SIZE_OF_CLIPS\n",
    "train_values['Test Dataset Size in Number of Frames'] = len(test_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP*BATCH_SIZE_OF_CLIPS\n",
    "train_values['Validation Dataset Size in Number of Frames'] = len(val_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP*BATCH_SIZE_OF_CLIPS\n",
    "train_values['Current date and time'] = datetime.now().strftime('%dD-%mM-%yY_%HH-%MM-%SS')\n",
    "train_values['Classification Report']=classification_report_df.head().to_dict()\n",
    "train_values['Training version']= TRAIN_VERSION\n",
    "# train_values['Optimizer']=pd.read_csv(io.StringIO(str(optimizer))).to_dict() #ERRORS\n",
    "train_values['Optimizer']=str(optimizer)\n",
    "train_values['Arquitecture']=(str(model)[:])#.head().to_dict()\n",
    "\n",
    "\n",
    "## Saving training curves at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "os.chdir(TRAINING_CURVES_PATH)\n",
    "TRAIN_FILE_NAME = TEMP_DICT_TRAINING_CURVES_FOR+\"TRAINING_PARAMETERS\"+PARAMETERS_FILENAME+\".json\"\n",
    "\n",
    "\n",
    "with open(TRAIN_FILE_NAME, 'w') as file:\n",
    "     file.write(json.dumps(train_values, indent=4))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060e041",
   "metadata": {},
   "source": [
    "## 12. [**!WARNING!**] Cleanup temporal data directory \n",
    "Remove directory if a temporary was used.\n",
    "\n",
    "```\n",
    "       Make sure you know which path you will remove as you do not like to remove important files.\n",
    "       shutil.rmtree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ae88d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:36:19.883117Z",
     "start_time": "2022-08-12T17:36:19.837256Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_files_path = config['temporal_data_path']\n",
    "\n",
    "shutil.rmtree(temporal_files_path)\n",
    "print(f' {temporal_files_path} is empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31953577",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
