{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef885fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:05.695513Z",
     "start_time": "2021-12-17T09:18:03.125501Z"
    }
   },
   "source": [
    "# Learning pipeline\n",
    "\n",
    "**Author**: Miguel Xochicale [@mxochicale](https://github.com/mxochicale)     \n",
    "**Contributors**: Nhat Phung Tran Huy [@huynhatd13](https://github.com/huynhatd13); Hamideh Kerdegari [@hamidehkerdegari](https://github.com/hamidehkerdegari);  Alberto Gomez [@gomezalberto](https://github.com/)  \n",
    "\n",
    "\n",
    "## History\n",
    "* Feb2022: Adding initial models with small dataset   \n",
    "* March2022: Improved datasets representation\n",
    "* April2022: Adds dataloader for clips and videos\n",
    "* May2022: Tidies VGG2D and VGG3D   \n",
    "* June2022: Tidies basicVGG model and adds heuristics for hyperarameters \n",
    "\n",
    "## Summary\n",
    "This notebook presents a learning pipeline to classify 4 chamber view from echocardiography datasets.\n",
    "\n",
    "### How to run the notebook\n",
    "\n",
    "1. Go to echocardiography repository path: `$HOME/repositories/echocardiography/`\n",
    "2. Open echocardiography repo in pycharm and in the terminal type:\n",
    "    ```\n",
    "    git checkout master # or the branch\n",
    "    git pull # to bring a local branch up-to-date with its remote version\n",
    "    ```\n",
    "3. Launch Notebook server  \n",
    "    Go to you repository path: `cd $HOME/repositories/echocardiography/scripts/dataloaders` and type in the pycharm terminal:\n",
    "    ```\n",
    "    conda activate rt-ai-echo-VE \n",
    "    jupyter notebook\n",
    "    ```\n",
    "    which will open your web-browser.\n",
    "    \n",
    "    \n",
    "### References\n",
    "* \"Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) - Discussion Paper and Request for Feedback\". https://www.fda.gov/media/122535/download \n",
    "* Gomez A. et al. 2021 https://github.com/vital-ultrasound/lung/blob/main/multiclass_pytorch/datasets/LUSVideoDataset.py \n",
    "* Kerdegari H. et al. 2021 https://github.com/vital-ultrasound/lung/tree/main/multiclass_tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f2aa0",
   "metadata": {},
   "source": [
    "# Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d3efa",
   "metadata": {},
   "source": [
    "## 1. Setting imports and datasets paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345efe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:44:44.536011Z",
     "start_time": "2022-07-05T15:44:44.501893Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML #to be used with HTML(animation.ArtistAnimation().to_jshtml())\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from source.dataloaders.EchocardiographicVideoDataset import EchoClassesDataset\n",
    "from source.models.learning_misc import train_loop, test_loop#, basicVGGNet\n",
    "from source.helpers.various import concatenating_YAML_via_tags, plot_dataset_classes, split_train_validate_sets\n",
    "\n",
    "HOME_PATH = os.path.expanduser(f'~')\n",
    "USERNAME = os.path.split(HOME_PATH)[1]\n",
    "\n",
    "REPOSITORY_PATH='repositories/echocardiography'\n",
    "FULL_REPO_PATH = HOME_PATH+'/'+REPOSITORY_PATH\n",
    "FULL_REPO_MODEL_PATH = HOME_PATH +'/' + REPOSITORY_PATH + '/data/models'\n",
    "CONFIG_FILES_PATH= REPOSITORY_PATH + '/scripts/config_files/users_paths_files'\n",
    "YML_FILE =  'config_users_paths_files_username_' + USERNAME + '.yml'\n",
    "FULL_PATH_FOR_YML_FILE = os.path.join(HOME_PATH, CONFIG_FILES_PATH, YML_FILE)\n",
    "PATH_for_temporal_files = os.path.join(HOME_PATH, 'datasets/vital-us/echocardiography/temporal-files')\n",
    "\n",
    "## Setting TRAINING_CURVES_PATH\n",
    "#CURRENT_PATH=os.path.abspath(os.getcwd())\n",
    "RESULTS_PATH='scripts/learning-pipeline/results'\n",
    "TRAINING_CURVES_PATH = os.path.join(FULL_REPO_PATH, RESULTS_PATH)\n",
    "\n",
    "## Setting FULL_PATH_FOR_YML_FILE\n",
    "yaml.add_constructor('!join', concatenating_YAML_via_tags)  ## register the tag handler\n",
    "with open(FULL_PATH_FOR_YML_FILE, 'r') as yml:\n",
    "    config = yaml.load(yml, Loader=yaml.FullLoader)\n",
    "\n",
    "## Printing Versions and paths\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "print(f'Torchvision Version: {torchvision.__version__}')    \n",
    "print(f'FULL_PATH_FOR_YML_FILE: {FULL_PATH_FOR_YML_FILE}' )\n",
    "print(f'FULL_REPO_MODEL_PATH: {FULL_REPO_MODEL_PATH}' )\n",
    "print(f'TRAINING_CURVES_PATH: {TRAINING_CURVES_PATH}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08abb29",
   "metadata": {},
   "source": [
    "## 2. Generate list text files for train and validate datasets\n",
    "\n",
    "Edit config_users_paths_files_username_$USER.yml at '../config_files/users_paths_files/config_users_paths_files_username_template.yml' with the right paths and percentage of `ntraining`:  \n",
    "```\n",
    "#ECHODATASET_PATH = config['echodataset_path'] # Default\n",
    "ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-GOOD'\n",
    "TRAINING_SPLITTING = 0.8 #config['ntraining'] #Default\n",
    "randomise_file_list: False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52167b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:45:50.355626Z",
     "start_time": "2022-07-05T15:45:50.329196Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "##### Setting up device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #or \"cuda:NN\" can also be used e.g., \"cuda:0\"\n",
    "print(f'Device: {DEVICE}')\n",
    "\n",
    "##############################\n",
    "## Setting ECHODATASET_PATH; \n",
    "#ECHODATASET_PATH = config['echodataset_path'] # Default\n",
    "ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-05-subjects'\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-10-subjects'\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-20-subjects'\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-31-subjects'\n",
    "#ECHODATASET_PATH = '/media/mx19/vitaluskcl/datasets/echocardiography/videos-echo-annotated-in-verification40-49'\n",
    "\n",
    "TRAINING_SPLITTING = 0.8 #config['ntraining'] #Default\n",
    "\n",
    "split_train_validate_sets(  \n",
    "                        ECHODATASET_PATH, #config['echodataset_path']\n",
    "                        config['data_list_output_path'], \n",
    "                        TRAINING_SPLITTING,\n",
    "                        config['randomise_file_list']\n",
    "                        )\n",
    "\n",
    "# PRETRANSFORM_IM_SIZE = [64, 64] #[650, 690] original pixel size for VenueGO\n",
    "PRETRANSFORM_IM_SIZE = [128, 128] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = [256, 256] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = [512, 512] #[650, 690] original pixel size for VenueGO\n",
    "# PRETRANSFORM_IM_SIZE = config['pretransform_im_size'] ##DEFAULT\n",
    "\n",
    "### >> CHANGE DENSE LAYER FEATURES IN VGG3D\n",
    "### >> `self.fc0 = nn.Linear(in_features=4194304, out_features=500) #128x128`\n",
    "\n",
    "##############################\n",
    "##### Experiments for Basic HYPERPARAMETER Heuristics \n",
    "\n",
    "#################  LEARNING_RATE \n",
    "### EXPERIMENT 01,02,03,04\n",
    "NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.00005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.0000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 20; LEARNING_RATE= 0.00000005; \n",
    "\n",
    "#################  BATCH_SIZE_OF_CLIPS with LEARNING_RATE= 0.000005 as it is the best peformance of prevous LRs \n",
    "### EXPERIMENT 04,06,07,08\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 2; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 5; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10; BATCH_SIZE_OF_CLIPS = 15; LEARNING_RATE= 0.000005; \n",
    "\n",
    "#################  NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP with LEARNING_RATE= 0.000005 and BATCH_SIZE_OF_CLIPS=10\n",
    "### EXPERIMENT 09,10,11,12\n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 2; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 7; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 13; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 20; BATCH_SIZE_OF_CLIPS = 10; LEARNING_RATE= 0.000005; \n",
    "\n",
    "##TOADD\n",
    "### * NUMBER_OF_FRAMES_AND PARTICPANTS\n",
    "### * OPTIMISERS \n",
    "\n",
    "# LEARNING_RATE =Trial and Error with diffent values  0.00005; 0.0005; 0.005 and 0.000001; 0.00001; 0.0001; 0.001  \n",
    "\n",
    "\n",
    "MAX_EPOCHS = 50 #Alternatvely, make use of: config['max_epochs']\n",
    "\n",
    "\n",
    "##############################\n",
    "##### Setting up animation\n",
    "interval_between_frames_in_milliseconds=33.3 ## 1/30=0.033333\n",
    "frame_per_seconds_for_animated_frames=30\n",
    "\n",
    "\n",
    "#SUBJECT_ID = '073'\n",
    "#print(SUBJECT_ID)\n",
    "\n",
    "### CUDA out of memory \n",
    "#NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP = 10\n",
    "#PRETRANSFORM_IM_SIZE = [128, 128] \n",
    "#RuntimeError: CUDA out of memory. Tried to allocate 7.81 GiB (GPU 0; 15.74 GiB total capacity; 8.51 GiB already allocated; 5.05 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
    "## REBOOT MACHINE\n",
    "#RuntimeError: CUDA out of memory. Tried to allocate 7.81 GiB (GPU 0; 15.74 GiB total capacity; 8.51 GiB already allocated; 5.13 GiB free; 8.53 GiB reserved in total by PyTorch)\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "## Setting labels\n",
    "label_id = ('BKGR', '4CV')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b3729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-17T09:18:08.264310Z",
     "start_time": "2021-12-17T09:18:08.250178Z"
    }
   },
   "source": [
    "## 2. Setting variables and loading datasets using pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109aecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:45:51.400889Z",
     "start_time": "2022-07-05T15:45:51.294566Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining transforms that apply to the entire dataset.\n",
    "# These transforms are not augmentation.\n",
    "if config['use_pretransform_image_size']:\n",
    "    pretransform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size=PRETRANSFORM_IM_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    pretransform = None\n",
    "\n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_train_augmentation']:\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=5),  # in degrees\n",
    "        transforms.RandomEqualize(p=0.5),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "    \n",
    "# These transforms have random parameters changing at each epoch.\n",
    "if config['use_validation_augmentation']:\n",
    "    val_transform = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=5),  # in degrees\n",
    "    #transforms.RandomEqualize(p=0.5),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #transforms.ToTensor(), \n",
    "    ])\n",
    "else:\n",
    "    transform = None\n",
    "\n",
    "\n",
    "train_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_train'],\n",
    "    participant_path_json_list=config['participant_path_json_list_train'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,#transform=train_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "validation_dataset = EchoClassesDataset(\n",
    "    echodataset_path=ECHODATASET_PATH,\n",
    "    temporal_data_path=config['temporal_data_path'],\n",
    "    participant_videos_list=config['participant_videos_list_validation'],\n",
    "    participant_path_json_list=config['participant_path_json_list_validation'],\n",
    "    crop_bounds_for_us_image=config['crop_bounds_for_us_image'],\n",
    "    pretransform_im_size=PRETRANSFORM_IM_SIZE,\n",
    "    pretransform=pretransform,\n",
    "    number_of_frames_per_segment_in_a_clip=NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP, #config['number_of_frames_per_segment_in_a_clip'],\n",
    "    sliding_window_length_in_percentage_of_frames_per_segment=config['sliding_window_length_in_percentage_of_frames_per_segment'],\n",
    "    device=DEVICE,\n",
    "    max_background_duration_in_secs=config['max_background_duration_in_secs'],\n",
    "    transform=None,#transform=train_transform,\n",
    "    use_tmp_storage=config['use_tmp_storage']\n",
    "    )\n",
    "\n",
    "\n",
    "## Spliting train_dataset into train_set and test_set\n",
    "Ntdt = train_dataset.__len__()\n",
    "ntraining = 0.7\n",
    "\n",
    "Ntrain=round(Ntdt*ntraining)\n",
    "Ntest = round(Ntdt - (Ntdt*ntraining))\n",
    "print(f'Ntrain size: {Ntrain}, Ntest size: {Ntest}, \"Ntrain+Ntest\" size={Ntrain+Ntest}')\n",
    "train_set, test_set = torch.utils.data.random_split(train_dataset, [Ntrain, Ntest])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15737900",
   "metadata": {},
   "source": [
    "## 3. Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d07f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:45:54.451811Z",
     "start_time": "2022-07-05T15:45:54.443484Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_class_distribution(dataset_obj):\n",
    "    count_class_dict = {\n",
    "   'BKGR': 0 ,\n",
    "   \"4CV\": 0\n",
    "    }\n",
    "    \n",
    "    for clip_index_i in range(len(dataset_obj)):\n",
    "        data_idx = dataset_obj[clip_index_i]\n",
    "        label_id_idx = data_idx[1]\n",
    "        label = label_id[label_id_idx]\n",
    "        count_class_dict[label]+= 1\n",
    "        #count_class_dict[label]+= 1* number_of_frames_per_segment_in_a_clip\n",
    "\n",
    "    return count_class_dict\n",
    "        \n",
    "        \n",
    "def plot_from_dict(dict_obj, plot_title, **kwargs):\n",
    "    return sns.barplot(data = pd.DataFrame.from_dict([dict_obj]).melt(), \n",
    "                       x = \"variable\", y=\"value\", hue=\"variable\", **kwargs).set_title(plot_title)\n",
    "\n",
    "\n",
    "def creating_pair_of_clips(dataset):\n",
    "    number_of_clips = len(dataset)\n",
    "    clips=[]\n",
    "    for clip_index in range( int(number_of_clips)  ):\n",
    "        data_idx = dataset[clip_index]\n",
    "        data_clip_idx = data_idx[0]\n",
    "        label_clip_idx = data_idx[1]\n",
    "        clip_frame_clip_idx = data_idx[2]\n",
    "        n_available_frames_clip_idx = data_idx[3]\n",
    "        print(f' CLIP:{clip_index:02d} of {label_id[label_clip_idx]} label for {data_clip_idx.size()} TOTAL_FRAMES: {n_available_frames_clip_idx} from clip_frame_clip_idx {clip_frame_clip_idx}')    \n",
    "        clips.append([data_clip_idx, label_clip_idx, clip_index, clip_frame_clip_idx, n_available_frames_clip_idx ]) \n",
    "\n",
    "    return(clips)\n",
    "\n",
    "\n",
    "def pair_clips_labels(clips):\n",
    "    pair_clips_labels_=[]    \n",
    "    number_of_clips=len(clips)\n",
    "    for clip_index_i_A in range( int(number_of_clips/2)  ):\n",
    "        clip_index_i_B=int(number_of_clips/2) + clip_index_i_A \n",
    "        print(f' pair_clips_labels[{clip_index_i_A}]-- BKRG:{clip_index_i_A}, 4CV:{clip_index_i_B}')\n",
    "        data_clip_i_A=clips[clip_index_i_A][0]\n",
    "        label_i_A=clips[clip_index_i_A][1]\n",
    "        clip_i_A=clips[clip_index_i_A][2]\n",
    "        number_of_frames_A=clips[clip_index_i_A][4]\n",
    "        data_clip_i_B=clips[clip_index_i_B][0]\n",
    "        label_i_B=clips[clip_index_i_B][1]\n",
    "        clip_i_B=clips[clip_index_i_B][2]\n",
    "        number_of_frames_B=clips[clip_index_i_B][4]\n",
    "        pair_clips_labels_.append([data_clip_i_A, label_i_A, clip_i_A, number_of_frames_A, data_clip_i_B, label_i_B, clip_i_B, number_of_frames_B])\n",
    "    \n",
    "    return(pair_clips_labels_)\n",
    "\n",
    "\n",
    "def animate_clips(pair_clips_labels):\n",
    "    #print(f' CLIP: for {label_id[pair_clips_labels[1]]} ')\n",
    "    fig = plt.figure()    \n",
    "    pair_of_clip_index_i_frames=[]   \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "    data_clip_tensor_A=pair_clips_labels[0]\n",
    "    label_clip_A=pair_clips_labels[1]\n",
    "    clip_i_A=pair_clips_labels[2]\n",
    "    number_of_frames_A=pair_clips_labels[3]\n",
    "    data_clip_tensor_B=pair_clips_labels[4]\n",
    "    label_clip_B=pair_clips_labels[5]\n",
    "    clip_i_B=pair_clips_labels[6]\n",
    "    number_of_frames_B=pair_clips_labels[7]\n",
    "    \n",
    "    \n",
    "    ax1.title.set_text(f' CLIP: {clip_i_A:02d}--\\\n",
    "    {label_id[label_clip_A]} with {number_of_frames_A} of \\\n",
    "    {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} frames [for Subject ]')\n",
    "    ax2.title.set_text(f' CLIP: {clip_i_B:02d}--\\\n",
    "    {label_id[label_clip_B]} with {number_of_frames_B} of \\\n",
    "    {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} frames [for Subject ]')\n",
    "    for frames_idx in range(data_clip_tensor_A[:,:,...].size()[1]):\n",
    "        imA = ax1.imshow(data_clip_tensor_A[:,frames_idx,...].cpu().detach().numpy().transpose(1,2,0) , cmap=plt.get_cmap('gray') )  \n",
    "        imB = ax2.imshow(data_clip_tensor_B[:,frames_idx,...].cpu().detach().numpy().transpose(1,2,0) , cmap=plt.get_cmap('gray') )  \n",
    "        pair_of_clip_index_i_frames.append( [imA, imB] )\n",
    "    fig.tight_layout()    \n",
    "    #return fig, pair_of_clip_index_i_frames\n",
    "\n",
    "    anim = animation.ArtistAnimation(fig, pair_of_clip_index_i_frames, interval=interval_between_frames_in_milliseconds, blit=True, repeat_delay=1000)\n",
    "    return anim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4760906",
   "metadata": {},
   "source": [
    "## 4. Plotting Class Distribution (creates temp clips and it takes few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6472a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:46:52.717384Z",
     "start_time": "2022-07-05T15:45:57.661450Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'class_distribution(train_set): {get_class_distribution(train_set)}')\n",
    "print(f'class_distribution(validation_dataset): {get_class_distribution(validation_dataset)}' )\n",
    "print(f'class_distribution(test_set): {get_class_distribution(test_set)}')\n",
    "    \n",
    "#number_of_frames_per_segment_in_a_clip = config['number_of_frames_per_segment_in_a_clip']    \n",
    "print(f'Number of frames for training datasets {len(train_set)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "print(f'Number of frames for testing datasets {len(test_set)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "print(f'Number of frames for Validation datasets {len(validation_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP}')\n",
    "\n",
    "plot_title_train_label= f'TRAIN dataset of {len(train_set)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_test_label= f'TEST dataset of {len(test_set)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "plot_title_val_label= f'VALIDATION dataset of {len(validation_dataset)} clips with {NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} n_frames_per_clip'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,7))\n",
    "plot_from_dict(get_class_distribution(train_set), plot_title=plot_title_train_label, ax=axes[0])\n",
    "plot_from_dict(get_class_distribution(test_set), plot_title=plot_title_test_label, ax=axes[1])\n",
    "plot_from_dict(get_class_distribution(validation_dataset), plot_title=plot_title_val_label, ax=axes[2])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ea2a6",
   "metadata": {},
   "source": [
    "## 5. Animating frames of one clip of the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b57ab1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:46:56.229284Z",
     "start_time": "2022-07-05T15:46:56.132512Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'---------------------------------------')\n",
    "clips=creating_pair_of_clips(train_dataset)\n",
    "pair_clips_and_labels = pair_clips_labels(clips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49ba14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:00.622961Z",
     "start_time": "2022-07-05T15:46:59.489045Z"
    }
   },
   "outputs": [],
   "source": [
    "#average_HR =\n",
    "#fps = 30\n",
    "# 60 # beats per minute\n",
    "#Beats-per-minute: 60 BPM\n",
    "#Beats-per-second: 1 Hz\n",
    "#Cycle-per-second: 1 (Cycle/s)\n",
    "\n",
    "PAIR_OF_CLIPS = pair_clips_and_labels[0]\n",
    "\n",
    "animated_frames=animate_clips(PAIR_OF_CLIPS)\n",
    "HTML(animated_frames.to_jshtml())      \n",
    "\n",
    "# ##SAVE ANIMATIONS\n",
    "# for idx in range(0,len(pair_clips_labels)):\n",
    "#     PAIR_OF_CLIPS = pair_clips_labels[idx]\n",
    "#     print( f' pair_clips_labels {str(PAIR_OF_CLIPS[2])} {str(PAIR_OF_CLIPS[6])}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6d225",
   "metadata": {},
   "source": [
    "## 6. Displayting frames in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8805e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:03.967597Z",
     "start_time": "2022-07-05T15:47:03.911517Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f'====================================================')\n",
    "print(f'train_dataset.__len__() = {train_set.__len__()}')\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f'len(train_dataloader): {len(train_dataloader)} BATCHES of BATCH_SIZE_OF_CLIPS {BATCH_SIZE_OF_CLIPS}')\n",
    "for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "    print(f'  ====================================================')\n",
    "    sample_batched_images=sample_batched[0]\n",
    "    sample_batched_labels=sample_batched[1]\n",
    "    print(f'    BATCH_OF_CLIPS_INDEX : {clip_batch_idx} / {len(train_dataloader) - 1}')\n",
    "    print(f'    sample_batched_labels.size(): {  sample_batched_labels.size()  }')\n",
    "    print(f'    sample_batched_labels.squeeze().size(): {  sample_batched_labels.squeeze().size()  }')\n",
    "    print(f'    sample_batched_images.size(): {sample_batched_images.size()}')\n",
    "\n",
    "    for BATCH_SIZE_IDX, label in enumerate(sample_batched_labels):\n",
    "        print(f'        BATCH_SIZE_IDX {BATCH_SIZE_IDX} ')\n",
    "        print(f'          label: {label}')\n",
    "        sample_batched_idx_image = sample_batched_images[BATCH_SIZE_IDX,...]\n",
    "        print(f'          Sample_batched_idx_image.size()  {sample_batched_idx_image.size() }'  )\n",
    "\n",
    "        grid = utils.make_grid(sample_batched_idx_image)\n",
    "        print(f'          Grid size {grid.size()}' )\n",
    "#         plt.figure(figsize =(20,20) )\n",
    "#         plt.imshow( grid.cpu().detach().numpy().transpose(1, 2, 0) )\n",
    "#         plt.title(f'BATCH_SIZE_IDX {BATCH_SIZE_IDX}; Label: {label_id[label]}')\n",
    "#         plt.axis('off')\n",
    "#         plt.ioff()\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f' test_dataset.__len__() = {test_set.__len__()}')\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "print(f'====================================================')\n",
    "print(f' validation_dataset.__len__() = {validation_dataset.__len__()}')\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE_OF_CLIPS, \n",
    "    shuffle=True, \n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e92336",
   "metadata": {},
   "source": [
    "## 7. Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369a416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:07.510622Z",
     "start_time": "2022-07-05T15:47:07.493331Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "################################\n",
    "##### Define VGG3D architecture\n",
    "class VGG3D(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, n_frames_per_clip, n_classes=2):\n",
    "        \"\"\"\n",
    "        Simple Video classifier to classify into two classes:\n",
    "        Args:\n",
    "            input_size:  shape of the input image. Should be a 2 element vector for a 2D video (width, height) [e.g. 128, 128].\n",
    "            n_classes: number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        super(VGG3D, self).__init__()\n",
    "        self.name = 'VGG00'\n",
    "        self.input_size = input_size\n",
    "        self.n_classes = n_classes\n",
    "        self.n_frames_per_clip = n_frames_per_clip\n",
    "        self.n_features = np.prod(self.input_size)*self.n_frames_per_clip\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        \n",
    "        #NOTES\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html\n",
    "        #IN: [N,Cin,D,H,W]; OUT: (N,Cout,Dout,Hout,Wout)\n",
    "        #[batch_size, channels, depth, height, width].\n",
    "        \n",
    "\n",
    "        self.conv0 = nn.Sequential(\n",
    "                                nn.Conv3d(in_channels=1, out_channels=32,\n",
    "                                    kernel_size = (3, 1, 1),  ## (-depth, -height, -width)\n",
    "                                    stride =      (1, 1, 1), ##(depth/val0, height/val1, width/val2)\n",
    "                                    padding =     (0, 0, 0),\n",
    "                                    bias=False),\n",
    "                                nn.BatchNorm3d(32),\n",
    "                                nn.ReLU(True)\n",
    "                                )\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "                                nn.Conv3d(in_channels=32, out_channels=32,\n",
    "                                    kernel_size = (3, 1, 1),  ## (-depth, -height, -width)\n",
    "                                    stride =      (1, 1, 1), ##(depth/val0, height/val1, width/val2)\n",
    "                                    padding =     (0, 0, 0),\n",
    "                                    bias=False),\n",
    "                                nn.BatchNorm3d(32),\n",
    "                                nn.ReLU(True)\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "                                nn.Conv3d(in_channels=32, out_channels=64,\n",
    "                                    kernel_size = (3, 1, 1),  ## (-depth, -height, -width)\n",
    "                                    stride =      (1, 1, 1), ##(depth/val0, height/val1, width/val2)\n",
    "                                    padding =     (0, 0, 0),\n",
    "                                    bias=False),\n",
    "                                nn.BatchNorm3d(64),\n",
    "                                nn.ReLU(True)\n",
    "                                )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "                                nn.Conv3d(in_channels=64, out_channels=128,\n",
    "                                    kernel_size = (3, 1, 1),  ## (-depth, -height, -width)\n",
    "                                    stride =      (1, 1, 1), ##(depth/val0, height/val1, width/val2)\n",
    "                                    padding =     (0, 0, 0),\n",
    "                                    bias=False),\n",
    "                                nn.BatchNorm3d(128),\n",
    "                                nn.ReLU(True)\n",
    "                                )        \n",
    "\n",
    "        \n",
    "#         self.conv4 = nn.Sequential(\n",
    "#                                 nn.Conv3d(in_channels=128, out_channels=256,\n",
    "#                                     kernel_size = (3, 1, 1),  ## (-depth, -height, -width)\n",
    "#                                     stride =      (1, 1, 1), ##(depth/val0, height/val1, width/val2)\n",
    "#                                     padding =     (0, 0, 0),\n",
    "#                                     bias=False),\n",
    "#                                 nn.BatchNorm3d(256),\n",
    "#                                 nn.ReLU(True)\n",
    "#                                 )  \n",
    "        \n",
    "#         self.conv0 = nn.Conv3d(in_channels=1, out_channels=64,\n",
    "#                                kernel_size = (3, 3, 3),  ## (-depth, -height, -width)\n",
    "#                                stride =      (3, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding =     (0, 0, 0)\n",
    "#                                )        \n",
    "\n",
    "#         self.conv1 = nn.Conv3d(in_channels=64, out_channels=128,\n",
    "#                                kernel_size = (3, 3, 3),  # (-depth, -height, -width)\n",
    "#                                stride =      (3, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding =     (0, 0, 0)\n",
    "#                                )\n",
    "        \n",
    "#         self.conv2 = nn.Conv3d(in_channels=128, out_channels=256,\n",
    "#                                kernel_size =  (1, 3, 3),  # (-depth, -height, -width)\n",
    "#                                stride =       (3, 3, 3), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding =      (0, 0, 0)\n",
    "#                                )\n",
    "        \n",
    "        \n",
    "#         self.conv3 = nn.Conv3d(in_channels=256, out_channels=512,\n",
    "#                                kernel_size=   (2, 2, 2),  # (-depth, -height, -width)\n",
    "#                                stride=        (2, 2, 2), ##(depth/val0, height/val1, width/val2)\n",
    "#                                padding =      (0, 0, 0)\n",
    "#                                )\n",
    "        \n",
    "        \n",
    "#         self.pool0 = nn.MaxPool3d(\n",
    "#                                 kernel_size = (1, 3, 3),  # (-depth, -height, -width)\n",
    "#                                 stride =      (1, 1, 1), \n",
    "#                                 padding =     (0, 0, 0), \n",
    "#                                 dilation =    (1, 1, 1)\n",
    "#                                 )\n",
    "\n",
    "\n",
    "        #self.fc0 = nn.Linear(in_features=1048576, out_features=500) # \n",
    "        self.fc0 = nn.Linear(in_features=2097152, out_features=500) #128x128\n",
    "        #self.fc0 = nn.Linear(in_features=4194304, out_features=500) #128x128\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=self.n_classes)\n",
    "        #self.fc1 = nn.Linear(in_features=2048, out_features=self.n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(f'x.shape(): {x.size()}') ##[batch_size, channels, depth, height, width]\n",
    "        #x = x.permute(0,2,1,3,4)##[batch_size, depth,channels, height, width]\n",
    "        print(f'x.shape(): {x.size()}')\n",
    "        \n",
    "        x = self.conv0(x)\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape(): x.shape(): torch.Size([2, 64, 60, 128, 128]) with kernel_size=(1, 1, 1)\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape():torch.Size([2, 64, 51, 29, 29]) with kernel_size=(10, 100, 100)\n",
    "        print(f'conv0.size(): {x.size()}')\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        #print(f'x.shape(): {x.size()}') with kernel_size=(1, 10, 10) #x.shape(): torch.Size([2, 32, 60, 20, 20])\n",
    "        print(f'conv1.size(): {x.size()}')\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        #print(f'x.shape(): {x.size()}') with kernel_size=(1, 10, 10) #x.shape(): torch.Size([2, 32, 60, 20, 20])\n",
    "        print(f'conv2.size(): {x.size()}')\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        #print(f'x.shape(): {x.size()}') with kernel_size=(1, 10, 10) #x.shape(): torch.Size([2, 32, 60, 20, 20])\n",
    "        print(f'conv3.size(): {x.size()}')\n",
    "        \n",
    "#         x = self.conv4(x)\n",
    "#         #print(f'x.shape(): {x.size()}') with kernel_size=(1, 10, 10) #x.shape(): torch.Size([2, 32, 60, 20, 20])\n",
    "#         print(f'conv4.size(): {x.size()}')\n",
    "        \n",
    "        #x = self.pool0(x)\n",
    "        #print(f'x.pool0..shape(): {x.size()}') \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        print(f'self.flatten(x) size() {x.size()}') #x.shape(): torch.Size([4, 983040])\n",
    "        x = self.fc0(x)\n",
    "        #print(f'x.shape(): {x.size()}') #x.shape(): torch.Size([4, 32])\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=0.5) #dropout was included to combat overfitting\n",
    "        \n",
    "        #print(f'x.shape(): {x.size()}') # x.shape(): torch.Size([4, 2])\n",
    "        #x = self.sigmoid(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        #print(f'x.shape(): {x.size()}')  #x.shape(): torch.Size([4, 2])\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620d458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:18.418387Z",
     "start_time": "2022-07-05T15:47:18.412339Z"
    }
   },
   "outputs": [],
   "source": [
    "################################\n",
    "##### Define basicVGG architecture\n",
    "class basicVGG(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, n_frames_per_clip, n_classes=2):\n",
    "        \"\"\"\n",
    "        Simple Video classifier to classify into two classes:\n",
    "        Args:\n",
    "            input_size:  shape of the input image. Should be a 2 element vector for a 2D video (width, height) [e.g. 128, 128].\n",
    "            n_classes: number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        super(basicVGG, self).__init__()\n",
    "        self.name = 'basicVGG'\n",
    "        self.input_size = input_size\n",
    "        self.n_classes = n_classes\n",
    "        self.n_frames_per_clip = n_frames_per_clip\n",
    "        self.n_features = np.prod(self.input_size)*self.n_frames_per_clip\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=self.n_features, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=n_classes),\n",
    "            #nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(f'x.shape(): {x.size()}') ##[batch_size, channels, depth, height, width]\n",
    "        #x = x.permute(0,2,1,3,4)##[batch_size, depth,channels, height, width]\n",
    "        #print(f'x.shape(): {x.size()}')\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb1dd6",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Sanity checks for the model and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5dd8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:22.623192Z",
     "start_time": "2022-07-05T15:47:21.286487Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "##################################################################\n",
    "##### Tensor Shape\n",
    "#tensor_shape_size = [BATCH_SIZE_OF_CLIPS, config['number_of_frames_per_segment_in_a_clip'], 1, 128, 128]\n",
    "#model = basicVGGNet(tensor_shape_size)\n",
    "\n",
    "#n_frames_per_clip \n",
    "#model = VGG3D(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "model = basicVGG(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "model.to(DEVICE) # Place model on GPU-\n",
    "\n",
    "### Sanity check\n",
    "#print(len(train_dataloader)) #6 BATCHES of 10=BATCH_SIZE_OF_CLIPS\n",
    "sample_batched = next(iter(train_dataloader))\n",
    "#print(sample_batched[0].shape) #torch.Size([10, 60, 1, 128, 128])\n",
    "#print(sample_batched[1])#tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1])\n",
    "#print(sample_batched[2]) #tensor([47, 42,  0, 51, 49, 75, 67, 67, 62, 84])\n",
    "#print(sample_batched[3]) #tensor([105, 102,  43, 106,  94, 161, 151, 183, 150, 151])\n",
    "\n",
    "clip_batch = sample_batched[0]\n",
    "#print(f'clip_batch.size() {clip_batch.size()}') #torch.Size([4, 60, 1, 128, 128])\n",
    "##[batch_size, channels, depth, height, width]\n",
    "\n",
    "# # frames = image.to(device)\n",
    "print(model(clip_batch).shape) #torch.Size([4, 2])\n",
    "\n",
    "# #https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n",
    "del sample_batched\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22976ae2",
   "metadata": {},
   "source": [
    "## 8. Define Optimizer\n",
    "1. Set learning rate for how much the model is updated per batch.\n",
    "2. Set total epoch number, as we have shuffle and random transforms, so the training data of every epoch is different.\n",
    "3. Set the number of clips per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295fe94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:29.779811Z",
     "start_time": "2022-07-05T15:47:29.597318Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#model = VGG3D(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "model = basicVGG(PRETRANSFORM_IM_SIZE, NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP) #print(config['pretransform_im_size']) #(128, 128)\n",
    "model.to(DEVICE) # Place model on GPU\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "## PRINT MODEL\n",
    "print(f'====================================================')\n",
    "print(model)\n",
    "\n",
    "# ### PRINT model.named_parameters\n",
    "# print(f'====================================================')\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98807711",
   "metadata": {},
   "source": [
    "## 9. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee627c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:36.782158Z",
     "start_time": "2022-07-05T15:47:32.873114Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TRAINING\n",
    "#clip_batch_size = tuple(train_dataloader.dataset.__getitem__(0)[0].shape) ##(60, 1, 128, 128) frames, chs, [width, height]\n",
    "#print(clip_batch_size)\n",
    "\n",
    "\n",
    "startt = time.time()\n",
    "print(f'Starting training loop {startt}')\n",
    "\n",
    "# endt = time.time()\n",
    "# print(f'Elapsed time: {endt - startt}s')\n",
    "\n",
    "############################\n",
    "####### BINARY ACCURACY MODULE\n",
    "def binary_accuracy(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    binary_accuracy to calculate accuracy per epoch.\n",
    "    \"\"\"\n",
    "    y_pred_tag = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_tag, dim = 1)\n",
    "    correct_results_sum = (y_pred_tags == y_test).sum().float()\n",
    "    accuracy = correct_results_sum/y_test.shape[0]\n",
    "    accuracy = torch.round(accuracy * 100)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "############################\n",
    "####### TRAIN LOOP MODULE\n",
    "def train_loop(train_dataloader, model, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    train_loop\n",
    "    Arguments:\n",
    "        dataloader, model, criterion, optimizer, device\n",
    "\n",
    "    Return:\n",
    "        train_epoch_loss\n",
    "    \"\"\"\n",
    "    train_epoch_loss = 0\n",
    "    train_acc_loss_epoch = 0\n",
    "    step_train = 0\n",
    "    #size = len(train_dataloader.dataset)\n",
    "    for clip_batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "        step_train += 1\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(device), sample_batched[1].to(device)\n",
    "\n",
    "        #print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        # print(f'----------------------------------------------------------')\n",
    "        # print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        # print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        y_train_pred = model(X_train_batch) #torch.Size([9, 2])\n",
    "        #y_train_pred = model(X_train_batch).squeeze()  # torch.Size([9, 2])\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = binary_accuracy(y_train_pred, y_train_batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if clip_batch_idx % 10 == 0: ## Print loss values every 10 clip batches\n",
    "        #     train_loss, current = train_loss.item(), clip_batch_idx * len(X_train_batch)\n",
    "        #     print(f\"loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        train_epoch_loss += train_loss.detach().item()\n",
    "        train_acc_loss_epoch += train_acc.detach().item()\n",
    "\n",
    "    train_epoch_loss /= step_train\n",
    "    train_acc_loss_epoch /= step_train\n",
    "\n",
    "    return train_epoch_loss, train_acc_loss_epoch\n",
    "\n",
    "\n",
    "############################\n",
    "####### TEST LOOP MODULE\n",
    "def test_loop(dataloader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Test loop \n",
    "    \n",
    "    Arguments:\n",
    "        dataloader, model, criterion, optimizer, device\n",
    "\n",
    "    Return:\n",
    "        test_epoch_loss, correct\n",
    "    \"\"\"\n",
    "\n",
    "    train_epoch_acc = 0\n",
    "    step_test = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_epoch_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #model.eval()\n",
    "        #val_epoch_loss = 0\n",
    "        #val_epoch_acc = 0\n",
    "        for clip_batch_idx, sample_val_batched in enumerate(dataloader):\n",
    "            step_test += 1\n",
    "            X_val_batch, y_val_batch = sample_val_batched[0].to(device), sample_val_batched[1].to(device)\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "            y_val_pred = model(X_val_batch)\n",
    "            test_epoch_loss += criterion(y_val_pred, y_val_batch).detach().item()\n",
    "            correct += (y_val_pred.argmax(1) == y_val_batch).type(torch.float).sum().detach().item()\n",
    "\n",
    "    test_epoch_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    return test_epoch_loss, correct\n",
    "\n",
    "\n",
    "#Dictionaries to store the accuracy/epoch and loss/epoch for both train and validation sets.\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    #\"val\": []\n",
    "}\n",
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    'test': [],\n",
    "    #\"val\": []\n",
    "}\n",
    "\n",
    "#for epoch in tqdm(range(1, MAX_EPOCHS)):   \n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"EPOCH {epoch + 1}/{MAX_EPOCHS}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    \n",
    "    train_epoch_loss, train_acc_loss_epoch = train_loop(train_dataloader, model, criterion, optimizer, DEVICE)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    test_epoch_loss, correct = test_loop(val_dataloader, model, criterion, DEVICE)\n",
    "\n",
    "    #print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} | Average Train Acc: {train_epoch_acc:.5f} | Average Validation Loss: {val_epoch_loss:.3f} | Average Validation Acc: {val_epoch_acc:.5f} ')\n",
    "    #print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} |Average Train Acc: {train_epoch_acc:.5f}  ')\n",
    "    \n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_epoch_loss:>8f} \\n\")\n",
    "\n",
    "    print(f'Epoch {epoch+0:02}: | Average Train Loss: {train_epoch_loss:.3f} Average Train Accuracy Loss: {(train_acc_loss_epoch):>0.1f}% ')\n",
    "    \n",
    "    print(f\"Test Error: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg Test loss: {test_epoch_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "    loss_stats['train'].append(train_epoch_loss)\n",
    "    loss_stats['test'].append(test_epoch_loss)\n",
    "    accuracy_stats['train'].append(train_acc_loss_epoch)\n",
    "    accuracy_stats['test'].append(100*correct)\n",
    "            \n",
    "print(\"DONE TRAINING LOOP!\")\n",
    "\n",
    "\n",
    "\n",
    "endt = time.time()\n",
    "elapsed_time = endt - startt\n",
    "print(f'Finishing training loop {endt}')\n",
    "print(f'Elapsed time for the training loop: {elapsed_time} (s)')\n",
    "\n",
    "#model_path = ' /home/mx19/repositories/echocardiography/models' \n",
    "print(f' {FULL_REPO_MODEL_PATH}')\n",
    "torch.save(model.state_dict(), os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\"))\n",
    "print(\"Saved metric model\")\n",
    "\n",
    "print(loss_stats)\n",
    "print(accuracy_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a7882",
   "metadata": {},
   "source": [
    "## 10. Visualize accuracy and loss performance and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd312af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:45.134582Z",
     "start_time": "2022-07-05T15:47:44.760166Z"
    }
   },
   "outputs": [],
   "source": [
    "## Convert stats as dataframes\n",
    "loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
    "## Concatenate dataframes and dictionaries\n",
    "loss_df.insert(0, 'curve', 'Loss', True)\n",
    "acc_df.insert(0, 'curve', 'Acc', True)\n",
    "valuesall = [loss_df, acc_df]\n",
    "values_all = pd.concat(valuesall)\n",
    "\n",
    "all_stats = {\n",
    "    'ACC': accuracy_stats,\n",
    "    'LOS': loss_stats,\n",
    "}\n",
    "\n",
    "\n",
    "## Saving training curves at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "os.chdir(TRAINING_CURVES_PATH)\n",
    "\n",
    "\n",
    "#################################\n",
    "#### PLOTING\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(45,10))\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.lineplot(\n",
    "    data=acc_df, \n",
    "    x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0],    \n",
    "    estimator=None, linewidth=5, palette=\"Set2\" \n",
    "    ).set_title(f'Accuracy/Epoch for Train Val Sets EPOCHS={MAX_EPOCHS} \\\n",
    "                BATCH_SIZE_OF_CLIPS={BATCH_SIZE_OF_CLIPS} \\\n",
    "                NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP={NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} \\\n",
    "                LEARNING_RATE={LEARNING_RATE}')\n",
    "\n",
    "axes[0].set_ylim(0,105)\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.lineplot(\n",
    "    data=loss_df, \n",
    "    x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1],\n",
    "    estimator=None, linewidth=5, palette=\"Set2\" \n",
    "    ).set_title(f'Loss for  EPOCHS={MAX_EPOCHS} \\\n",
    "                BATCH_SIZE_OF_CLIPS={BATCH_SIZE_OF_CLIPS} \\\n",
    "                NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP={NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP} \\\n",
    "                LEARNING_RATE={LEARNING_RATE}')\n",
    "axes[1].set_ylim(-0.4, 2)\n",
    "\n",
    "\n",
    "\n",
    "#### PREPARING AND SAVING PERFORMANCE CURVES\n",
    "WIDTH = 3\n",
    "PRECISION = 10\n",
    "TYPE = \"f\"\n",
    "STR_LR = f'{LEARNING_RATE:{WIDTH}.{PRECISION}{TYPE}}'\n",
    "STR_LR = STR_LR.replace(\".\", \"_\", 1)\n",
    "\n",
    "PARAMETERS_FILENAME = \"TRAINset_clips_\"+ \\\n",
    "                    str(len(train_set))+\"TESTset_clips_\"+ \\\n",
    "                    str(len(test_set))+\"VALset_clips\"+ \\\n",
    "                    str(len(validation_dataset))+\"___EPOCHS_\"+str(f'{MAX_EPOCHS:{WIDTH}}') + \\\n",
    "                    \"_BATCH_SIZE_OF_CLIPS_\"+str(f'{BATCH_SIZE_OF_CLIPS:02}')+ \\\n",
    "                    \"_NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP_\"+ \\\n",
    "                    str(NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP)+\"_LEARNING_RATE_\"+STR_LR\n",
    "\n",
    "## Saving training curves at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "os.chdir(TRAINING_CURVES_PATH)\n",
    "TEMP_DICT_TRAINING_CURVES_FOR = \"TEMP_DICT_TRAINING_CURVES_FOR____\"\n",
    "IMAGE_FILE_NAME = TEMP_DICT_TRAINING_CURVES_FOR+PARAMETERS_FILENAME\n",
    "#print(IMAGE_FILE_NAME)\n",
    "fig.savefig(IMAGE_FILE_NAME) \n",
    "\n",
    "## Saving training metrics in dictionaries at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "LOSS_ACC_DICTS_FILE_NAME = \"TEMP_DICT_TRAINING_CURVES_FOR____\"+\"_LOSS_ACC_\"+PARAMETERS_FILENAME+'.json'\n",
    "#print(LOSS_ACC_DICTS_FILE_NAME)\n",
    "#print(all_stats)\n",
    "\n",
    "with open(LOSS_ACC_DICTS_FILE_NAME, 'w') as file:\n",
    "   file.write(json.dumps(all_stats, indent=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aafb41",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Evaluate the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791b35a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:49.757436Z",
     "start_time": "2022-07-05T15:47:49.605239Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\n",
    "    os.path.join(FULL_REPO_MODEL_PATH, \"metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clip_batch_idx, sample_batched in enumerate(test_dataloader):\n",
    "        X_train_batch, y_train_batch = sample_batched[0].to(DEVICE), sample_batched[1].to(DEVICE)\n",
    "        print(f'==================================================')\n",
    "        print(f' BATCH_OF_CLIPS_INDEX: {clip_batch_idx} ')\n",
    "        print(f'   X_train_batch.size(): {X_train_batch.size()}') # torch.Size([9, 60, 1, 128, 128]) clips, frames, channels, [width, height]\n",
    "        print(f'   y_train_batch.size(): {y_train_batch.size()}') # torch.Size([9])\n",
    "\n",
    "        y_test_pred = model(X_train_batch)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)        \n",
    "        \n",
    "        for i in range(len(y_test_pred)):\n",
    "            y_true_list.append(y_train_batch[i].cpu().item())\n",
    "            y_pred_list.append(y_pred_tag[i].cpu().item())\n",
    "            \n",
    "        \n",
    "print(f'==================================================')        \n",
    "print(f'==================================================')        \n",
    "print(get_class_distribution(test_set))\n",
    "print(f'y_true_list{y_true_list}')\n",
    "print(f'y_pred_list{y_pred_list}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a2101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:47:54.319187Z",
     "start_time": "2022-07-05T15:47:54.221763Z"
    }
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_true_list, y_pred_list)\n",
    "print(report)\n",
    "# report_support = precision_recall_fscore_support(y_true_list, y_pred_list)\n",
    "# print(report_support)\n",
    "\n",
    "def metrics_report_to_df(ytrue, ypred):\n",
    "    classification_report_df = pd.DataFrame(data=list(precision_recall_fscore_support(y_true_list, y_pred_list)), \\\n",
    "                                         index=['Precision', 'Recall', 'F1-score', 'Support']).T    \n",
    "    classification_report_df.loc['weighted avg/Total', :] = precision_recall_fscore_support(ytrue, ypred, average='weighted')\n",
    "    classification_report_df.loc['Avg/Total', 'Support'] = classification_report_df['Support'].sum()\n",
    "    return(classification_report_df)\n",
    "\n",
    "classification_report_df = metrics_report_to_df(y_true_list, y_pred_list)\n",
    "print(classification_report_df)\n",
    "\n",
    "\n",
    "# print(confusion_matrix(y_true_list, y_pred_list))\n",
    "#################################\n",
    "### PLOTTING CONFUSION MATRIX\n",
    "cm=confusion_matrix(y_true_list, y_pred_list)\n",
    "print(cm)\n",
    "#cm=confusion_matrix(y_true_list, y_pred_list, normalize='all')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['BGR','4CV'])\n",
    "cmd.plot()\n",
    "cmd.ax_.set(xlabel='Predicted', ylabel='True')\n",
    "\n",
    "\n",
    "#### PREPARING AND SAVING TRAINING PERFORMANCE PARAMETERS\n",
    "train_values = {} # instantiate an empty train_values dict \n",
    "train_values['elapsed_time_in_secs'] = elapsed_time\n",
    "train_values['MAX_EPOCHS'] = MAX_EPOCHS\n",
    "train_values['LEARNING_RATE'] = LEARNING_RATE\n",
    "train_values['BATCH_SIZE_OF_CLIPS'] = BATCH_SIZE_OF_CLIPS\n",
    "train_values['PRETRANSFORM_IM_SIZE'] = PRETRANSFORM_IM_SIZE\n",
    "train_values['FRAMES_PER_CLIP'] = NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP\n",
    "train_values['Train Dataset Size'] = len(train_set)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP*BATCH_SIZE_OF_CLIPS\n",
    "train_values['Test Dataset Size'] = len(test_set)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP*BATCH_SIZE_OF_CLIPS\n",
    "train_values['Validation Dataset Size'] = len(validation_dataset)*NUMBER_OF_FRAMES_PER_SEGMENT_IN_A_CLIP*BATCH_SIZE_OF_CLIPS\n",
    "train_values['Current date and time'] = datetime.now().strftime('%dD-%mM-%yY_%HH-%MM-%SS')\n",
    "#team['konqi'] = {'health': 18, 'level': 7}\n",
    "\n",
    "## Saving training curves at $HOME/repositories/echocardiography/scripts/learning-pipeline/results\n",
    "os.chdir(TRAINING_CURVES_PATH)\n",
    "TRAIN_FILE_NAME = TEMP_DICT_TRAINING_CURVES_FOR+\"TRAINING_PARAMETERS\"+PARAMETERS_FILENAME+\".json\"\n",
    "\n",
    "\n",
    "with open(TRAIN_FILE_NAME, 'w') as file:\n",
    "     file.write(json.dumps(train_values, indent=4))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060e041",
   "metadata": {},
   "source": [
    "## 12. [**!WARNING!**] Cleanup temporal data directory \n",
    "Remove directory if a temporary was used.\n",
    "\n",
    "```\n",
    "       Make sure you know which path you will remove as you do not like to remove important files.\n",
    "       shutil.rmtree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ae88d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T15:48:12.466176Z",
     "start_time": "2022-07-05T15:48:12.420444Z"
    }
   },
   "outputs": [],
   "source": [
    "temporal_files_path = config['temporal_data_path']\n",
    "\n",
    "shutil.rmtree(temporal_files_path)\n",
    "print(f' {temporal_files_path} is empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eea63f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-28T15:53:10.760424Z",
     "start_time": "2022-03-28T15:53:10.754191Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f0279",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
